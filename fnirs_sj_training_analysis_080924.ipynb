{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment and define functions\n",
    "\n",
    "# Import packages\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as op\n",
    "import time\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import mne\n",
    "from mne.preprocessing.nirs import tddr\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix  \n",
    "from mne_nirs.channels import get_long_channels\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.stats import ttest_rel, zscore\n",
    "import mne_nirs\n",
    "\n",
    "mne.viz.set_browser_backend('matplotlib')\n",
    "\n",
    "######### Set these variables as appropriate:\n",
    "raw_path = '../../data'\n",
    "proc_path = '../../processed'\n",
    "results_path = '../../results'\n",
    "subjects_dir = '../../subjects'\n",
    "subject_group_mapping = pd.read_csv('../../subject_group_mapping.csv')\n",
    "behavior_results_path = '../../fnirs-behavior-results'\n",
    "behavior_file = '../../behavior_diff_data.csv'\n",
    "output_suffix = \"final\" # used for all file names that are created\n",
    "\n",
    "# Create the subject to group mapping dictionary\n",
    "subject_group_mapping = subject_group_mapping.dropna(subset=['Subject'])  \n",
    "subject_group_mapping['Subject'] = subject_group_mapping['Subject'].astype(int) \n",
    "subject_to_group = dict(zip(subject_group_mapping['Subject'], subject_group_mapping['Group']))\n",
    "subjects = subject_group_mapping['Subject'].astype(str).tolist()\n",
    "\n",
    "sfreq = 4.807692\n",
    "conditions = ('A', 'V', 'AV', 'W')\n",
    "groups = ('trained', 'control')\n",
    "days = ('1', '3')\n",
    "runs = (1, 2)\n",
    "duration = 1.8\n",
    "design = 'event'\n",
    "filt_kwargs = dict(l_freq=0.01, h_freq=0.2) \n",
    "n_jobs = 4  # for GLM\n",
    "\n",
    "os.makedirs(proc_path, exist_ok=True)\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "os.makedirs(subjects_dir, exist_ok=True)\n",
    "# mne.datasets.fetch_fsaverage(subjects_dir=subjects_dir, verbose=True)  # Only need to run once\n",
    "\n",
    "use = None\n",
    "all_sci = list()\n",
    "plt.rcParams['axes.titlesize'] = 8\n",
    "plt.rcParams['axes.labelsize'] = 8\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "\n",
    "# Prep making bad channels report\n",
    "bad_channels_filename = op.join(results_path, f'bad_channels_report_{output_suffix}.csv')\n",
    "\n",
    "with open(bad_channels_filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Subject', 'Day', 'Run', 'Percent Bad'])\n",
    "\n",
    "def add_bad_channel_entry(subject, day, run, percentage_bad):\n",
    "    with open(bad_channels_filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([subject, day, run, f'{percentage_bad:.2f}%'])\n",
    "\n",
    "def normalize_channel_names(channels_set):\n",
    "    return {name.split()[0] for name in channels_set}\n",
    "\n",
    "# Sanity check for subjects\n",
    "subjects_check = {int(subject) for subject in subjects}\n",
    "subject_to_group_check = set(subject_to_group.keys())\n",
    "if subjects_check == subject_to_group_check:\n",
    "    print(\"N=\" + str(len(subjects)))\n",
    "    del subjects_check, subject_to_group_check\n",
    "else:\n",
    "    print(\"Error loading subject info\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for preprocessing\n",
    "\n",
    "def preprocess_fnirs_data(raw_intensity, proc_path, base):\n",
    "    # 1. Convert to optical density:\n",
    "    print(f'    Analyzing {base}')\n",
    "    raw_od = mne.preprocessing.nirs.optical_density(raw_intensity, verbose='error')\n",
    "\n",
    "    # 2. Identify bad channels based on flat signal and scalp coupling index:\n",
    "    peaks = np.ptp(raw_od.get_data('fnirs'), axis=-1)\n",
    "    flat_names = [raw_od.ch_names[f].split(' ')[0] for f in np.where(peaks < 0.001)[0]]\n",
    "    sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "    sci_mask = (sci < 0.25)\n",
    "    got = np.where(sci_mask)[0]\n",
    "    percentage_bad = (len(got) / len(raw_od.ch_names)) * 100\n",
    "    assert raw_od.info['bads'] == []\n",
    "    bads = set(raw_od.ch_names[pick] for pick in got)\n",
    "    bads = bads | set(ch_name for ch_name in raw_od.ch_names if ch_name.split(' ')[0] in flat_names)\n",
    "    bads = sorted(bads)\n",
    "\n",
    "    # 3. Apply temporal derivative distribution repair (TDDR), bandpass filter, apply bad channels:\n",
    "    raw_tddr = tddr(raw_od)\n",
    "    raw_tddr_bp = raw_tddr.copy().filter(**filt_kwargs)\n",
    "    raw_tddr_bp.info['bads'] = bads\n",
    "\n",
    "    # 5. Short channel regression (if present): \n",
    "    try:\n",
    "        raw_tddr_bp = mne_nirs.signal_enhancement.short_channel_regression(raw_tddr_bp)\n",
    "    except:\n",
    "        print(f\"No short channels found for {base}.\")\n",
    "\n",
    "    # 6. Convert to hemoglobin concentration:\n",
    "    raw_h = mne.preprocessing.nirs.beer_lambert_law(raw_tddr_bp, 6.)\n",
    "\n",
    "    # 7. Normalize channel names and verify bad channels\n",
    "    h_bads = [ch_name for ch_name in raw_h.ch_names if ch_name.split(' ')[0] in set(bad.split(' ')[0] for bad in bads)]\n",
    "    raw_h.info['bads'] = h_bads\n",
    "    raw_h.info._check_consistency()\n",
    "\n",
    "    # 8. Select long channels and verify that the signal is not flat:\n",
    "    raw_h = get_long_channels(raw_h)\n",
    "    picks = mne.pick_types(raw_h.info, fnirs=True)\n",
    "    peaks = np.ptp(raw_h.get_data(picks), axis=-1)\n",
    "    assert (peaks > 1e-9).all()\n",
    "\n",
    "    # 9. Interpolate bad channels\n",
    "    raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n",
    "    raw_h_interp.save(op.join(proc_path, f'{subject}_{day}_{run:03d}_long_hbo_{output_suffix}_raw.fif'), overwrite=True)\n",
    "    assert len(raw_h.ch_names) == len(raw_h_interp.ch_names)\n",
    "\n",
    "    return raw_h_interp, percentage_bad, bads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load participant data\n",
    "\n",
    "#subjects = ['223'] #testing\n",
    "\n",
    "for subject in subjects:\n",
    "    for day in days:\n",
    "        for run in runs:\n",
    "            group = subject_to_group.get(int(subject), \"unknown\")\n",
    "            root1 = f'Day{day}'\n",
    "            root2 = f'{subject}_{day}'\n",
    "            root3 = f'*-*-*_{run:03d}'\n",
    "            fname_base = op.join(raw_path, root1, root2, root3)\n",
    "            fname = glob.glob(fname_base)\n",
    "            base = f'{subject}_{day}_{run:03d}'\n",
    "            base_pr = base.ljust(20)\n",
    "            raw_intensity = mne.io.read_raw_nirx(fname[0])\n",
    "            raw_intensity, percentage_bad_long, bads_long = preprocess_fnirs_data(raw_intensity, proc_path, base + '_long')\n",
    "            add_bad_channel_entry(subject, day, run, percentage_bad_long)\n",
    "            del raw_intensity, percentage_bad_long, bads_long\n",
    "            gc.collect()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove subjects with >30% bad channels \n",
    "\n",
    "bad_channels_df = pd.read_csv(bad_channels_filename)\n",
    "bad_channels_df['Percent Bad'] = bad_channels_df['Percent Bad'].str.rstrip('%').astype(float)\n",
    "average_bad_channels = bad_channels_df.groupby('Subject')['Percent Bad'].mean()\n",
    "\n",
    "# Find subjects with more than 30% bad channels\n",
    "bad_subjects = average_bad_channels[average_bad_channels > 30].index.tolist()\n",
    "print(\"Subjects with more than 30% bad channels:\", bad_subjects)\n",
    "\n",
    "# Initialize counters for each group\n",
    "removed_trained = 0\n",
    "removed_control = 0\n",
    "remaining_trained = 0\n",
    "remaining_control = 0\n",
    "\n",
    "# Count and remove the subjects\n",
    "for subject in bad_subjects:\n",
    "    subject_int = int(subject) \n",
    "    if subject_int in subject_to_group:\n",
    "        if subject_to_group[subject_int] == \"trained\":\n",
    "            removed_trained += 1\n",
    "        elif subject_to_group[subject_int] == \"control\":\n",
    "            removed_control += 1\n",
    "        subject_to_group.pop(subject_int, None)\n",
    "\n",
    "# Update the subjects list after counting the removed subjects\n",
    "subjects = [subject for subject in subjects if subject not in bad_subjects]\n",
    "for group in subject_to_group.values():\n",
    "    if group == \"trained\":\n",
    "        remaining_trained += 1\n",
    "    elif group == \"control\":\n",
    "        remaining_control += 1\n",
    "\n",
    "# Output the results\n",
    "print(\" \")\n",
    "print(f'Removed {removed_trained} trained subjects.')\n",
    "print(f'Removed {removed_control} control subjects.')\n",
    "print(\" \")\n",
    "print(f'Remaining trained subjects: {remaining_trained}')\n",
    "print(f'Remaining control subjects: {remaining_control}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean events and make design matrix\n",
    "\n",
    "def make_design(raw_h_long, design, subject=None, run=None, day=None, group=None):\n",
    "    annotations_to_remove = raw_h_long.annotations.description == '255.0'\n",
    "    raw_h_long.annotations.delete(annotations_to_remove)\n",
    "    events, _ = mne.events_from_annotations(raw_h_long)\n",
    "    \n",
    "    # Fix mis-codings\n",
    "    rows_to_remove = events[:, -1] == 1\n",
    "    events = events[~rows_to_remove]\n",
    "    if len(events) == 101:\n",
    "        events = events[1:]\n",
    "\n",
    "    n_times = len(raw_h_long.times)\n",
    "    stim = np.zeros((n_times, 4))\n",
    "    events[:, 2] -= 1\n",
    "    assert len(events) == 100, len(events)\n",
    "    want = [0] + [25] * 4\n",
    "    count = np.bincount(events[:, 2])\n",
    "    assert np.array_equal(count, want), count\n",
    "    assert events.shape == (100, 3), events.shape\n",
    "\n",
    "    if design == 'block':\n",
    "        events = events[0::5]\n",
    "        duration = 20.\n",
    "        assert np.array_equal(np.bincount(events[:, 2]), [0] + [5] * 4)\n",
    "    else:\n",
    "        assert design == 'event'\n",
    "        assert len(events) == 100\n",
    "        duration = 1.8\n",
    "        assert events.shape == (100, 3)\n",
    "        events_r = events[:, 2].reshape(20, 5)\n",
    "        assert (events_r == events_r[:, :1]).all()\n",
    "        del events_r\n",
    "        \n",
    "    idx = (events[:, [0, 2]] - [0, 1]).T\n",
    "    assert np.in1d(idx[1], np.arange(len(conditions))).all()\n",
    "    stim[tuple(idx)] = 1\n",
    "    \n",
    "    n_block = int(np.ceil(duration * sfreq))\n",
    "    stim = signal.fftconvolve(stim, np.ones((n_block, 1)), axes=0)[:n_times]\n",
    "    dm_events = pd.DataFrame({\n",
    "        'trial_type': [conditions[ii] for ii in idx[1]],\n",
    "        'onset': idx[0] / raw_h_long.info['sfreq'],\n",
    "        'duration': n_block / raw_h_long.info['sfreq']})\n",
    "    dm = make_first_level_design_matrix(\n",
    "        raw_h_long.times, dm_events, hrf_model='glover',\n",
    "        drift_model='polynomial', drift_order=0)\n",
    "        \n",
    "    return stim, dm, events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the subject, day, and run to plot different waveforms\n",
    "\n",
    "plot_subject = '223'\n",
    "plot_day = 1\n",
    "plot_run = 1\n",
    "\n",
    "fname2 = op.join(proc_path, f'{plot_subject}_{plot_day}_{plot_run:03d}_long_hbo_{output_suffix}_raw.fif')\n",
    "use = mne.io.read_raw_fif(fname2, preload=True)\n",
    "events, _ = mne.events_from_annotations(use)\n",
    "ch_names = [ch_name.rstrip(' hbo') for ch_name in use.ch_names]\n",
    "info = use.info\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(6., 3), constrained_layout=True)\n",
    "ax = axes[0]\n",
    "raw_h = use\n",
    "stim, dm, _ = make_design(raw_h, design)\n",
    "\n",
    "colors = dict(\n",
    "    A='#4477AA',  # blue\n",
    "    AV='#CCBB44',  # yellow\n",
    "    V='#EE7733',  # orange\n",
    "    W='#AA3377',  # purple\n",
    ")\n",
    "\n",
    "for ci, condition in enumerate(conditions):\n",
    "    color = colors[condition]\n",
    "    ax.fill_between(\n",
    "        raw_h.times, stim[:, ci], 0, edgecolor='none', facecolor='k',\n",
    "        alpha=0.5)\n",
    "    model = dm[conditions[ci]].to_numpy()\n",
    "    ax.plot(raw_h.times, model, ls='-', lw=1, color=color)\n",
    "    x = raw_h.times[np.where(model > 0)[0][0]]\n",
    "    ax.text(\n",
    "        x + 10, 1.1, condition, color=color, fontweight='bold', ha='center')\n",
    "ax.set(ylabel='Modeled\\noxyHb', xlabel='', xlim=raw_h.times[[0, -1]])\n",
    "\n",
    "# HbO/HbR\n",
    "ax = axes[1]\n",
    "picks = [pi for pi, ch_name in enumerate(raw_h.ch_names)\n",
    "         if 'S7_D19' in ch_name]\n",
    "colors = dict(hbo='r', hbr='b')\n",
    "ylim = np.array([-1, 1])\n",
    "for pi, pick in enumerate(picks):\n",
    "    color = colors[raw_h.ch_names[pick][-3:]]\n",
    "    data = raw_h.get_data(pick)[0] * 1e6\n",
    "    val = np.ptp(data)\n",
    "    assert val > 0.01\n",
    "    ax.plot(raw_h.times, data, color=color, lw=1.)\n",
    "ax.set(ylim=ylim, xlabel='Time (s)', ylabel='μM',\n",
    "       xlim=raw_h.times[[0, -1]])\n",
    "for ax in axes:\n",
    "    for key in ('top', 'right'):\n",
    "        ax.spines[key].set_visible(False)\n",
    "plt.savefig(op.join(results_path, f'figure_1_{output_suffix}.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GLM analysis and epoching\n",
    "\n",
    "subj_cha_list = []\n",
    "for subject in subjects:\n",
    "    group = subject_to_group.get(int(subject), \"unknown\")\n",
    "    for day in days:\n",
    "        for run in runs:\n",
    "            fname_long = op.join(proc_path, f'{subject}_{day}_{run:03d}_long_hbo_{output_suffix}_raw.fif')\n",
    "            raw_h_long = mne.io.read_raw_fif(fname_long)\n",
    "            _, dm, _ = make_design(raw_h_long, design, subject, run, day, group)\n",
    "            glm_est = mne_nirs.statistics.run_glm(\n",
    "                raw_h_long, dm, noise_model='ols', n_jobs=n_jobs)\n",
    "            cha = glm_est.to_dataframe()\n",
    "            cha['subject'] = subject\n",
    "            cha['run'] = run\n",
    "            cha['day'] = day\n",
    "            cha['group'] = group\n",
    "            subj_cha_list.append(cha)\n",
    "            del raw_h_long, dm, glm_est, cha\n",
    "            gc.collect()  #\n",
    "        print(f'***Finished processing subject {subject} day {day}.')\n",
    "\n",
    "df_cha = pd.concat(subj_cha_list, ignore_index=True)\n",
    "df_cha.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block averages\n",
    "\n",
    "event_id = {condition: ci for ci, condition in enumerate(conditions, 1)}\n",
    "evokeds = {condition: dict() for condition in conditions}\n",
    "for day in days:\n",
    "    for subject in subjects:\n",
    "        fname = op.join(proc_path, f'{subject}_{day}_{output_suffix}-ave.fif')\n",
    "        tmin, tmax = -2, 38\n",
    "        baseline = (None, 0)\n",
    "        t0 = time.time()\n",
    "        print(f'Creating block average for {subject} day {day}... ', end='')\n",
    "        raws = list()\n",
    "        events = list()\n",
    "        for run in runs:\n",
    "            fname2 = op.join(proc_path, f'{subject}_{day}_{run:03d}_long_hbo_{output_suffix}_raw.fif')\n",
    "            raw_h = mne.io.read_raw_fif(fname2)\n",
    "            events.append(make_design(raw_h, None, 'block', subject, run)[2])\n",
    "            raws.append(raw_h)\n",
    "        bads = sorted(set(sum((r.info['bads'] for r in raws), [])))\n",
    "        for r in raws:\n",
    "            r.info['bads'] = bads\n",
    "        raw_h, events = mne.concatenate_raws(raws, events_list=events)\n",
    "        epochs = mne.Epochs(raw_h, events, event_id, tmin=tmin, tmax=tmax,\n",
    "                            baseline=baseline)\n",
    "        this_ev = [epochs[condition].average() for condition in conditions]\n",
    "        assert all(ev.nave > 0 for ev in this_ev)\n",
    "        mne.write_evokeds(fname, this_ev, overwrite=True)\n",
    "        print(f'{time.time() - t0:0.1f} sec')\n",
    "        for condition in conditions:\n",
    "            evokeds[condition][subject] = mne.read_evokeds(fname, condition)\n",
    "        print(f'Done for {group} {subject} day {day} run {run:03d}... ', end='')\n",
    "        del raws, events, raw_h, epochs, this_ev\n",
    "        gc.collect()  #\n",
    "\n",
    "# Mark bad channels\n",
    "bad = dict()\n",
    "bb = dict()\n",
    "\n",
    "for day in days:\n",
    "    for subject in subjects:\n",
    "        for run in runs:\n",
    "            fname2 = op.join(proc_path, f'{subject}_{day}_{run:03d}_long_hbo_{output_suffix}_raw.fif')\n",
    "            this_info = mne.io.read_info(fname2)\n",
    "            bad_channels = [idx - 1 for idx in sorted(\n",
    "                this_info['ch_names'].index(bad) + 1 for bad in this_info['bads'])]\n",
    "            valid_indices = np.arange(len(use.ch_names))\n",
    "            bb = [b for b in bad_channels if b in valid_indices]\n",
    "            bad[(subject, run, day)] = bb\n",
    "        assert np.in1d(bad[(subject, run, day)], np.arange(len(use.ch_names))).all()\n",
    "\n",
    "bad_combo = dict()\n",
    "for day in days:\n",
    "    for (subject, run, day), bb in bad.items():\n",
    "        bad_combo[subject] = sorted(set(bad_combo.get(subject, [])) | set(bb))\n",
    "bad = bad_combo\n",
    "\n",
    "start = len(df_cha)\n",
    "n_drop = 0\n",
    "for day in days:\n",
    "    for (subject, run, day), bb in bad.items():\n",
    "        if not len(bb):\n",
    "            continue\n",
    "        drop_names = [use.ch_names[b] for b in bb]\n",
    "        is_subject = (df_cha['subject'] == subject)\n",
    "        is_day = (df_cha['day'] == day)\n",
    "        drop = df_cha.index[\n",
    "            is_subject &\n",
    "            is_day &\n",
    "            np.in1d(df_cha['ch_name'], drop_names)]\n",
    "        n_drop += len(drop)\n",
    "        if len(drop):\n",
    "            print(f'Dropping {len(drop)} for {subject} day {day}')\n",
    "            df_cha.drop(drop, inplace=True)\n",
    "end = len(df_cha)\n",
    "assert n_drop == start - end, (n_drop, start - end)\n",
    "\n",
    "# Combine runs by averaging\n",
    "sorts = ['subject', 'ch_name', 'Chroma', 'Condition', 'group', 'day', 'run']\n",
    "df_cha.sort_values(sorts, inplace=True)\n",
    "theta = np.array(df_cha['theta']).reshape(-1, len(runs)).mean(-1)\n",
    "df_cha.drop(\n",
    "    [col for col in df_cha.columns if col not in sorts[:-1]], axis='columns',\n",
    "    inplace=True)\n",
    "df_cha.reset_index(drop=True, inplace=True)\n",
    "df_cha = df_cha[::len(runs)]\n",
    "df_cha.reset_index(drop=True, inplace=True)\n",
    "df_cha['theta'] = theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate HbDiff\n",
    "\n",
    "# Load the data\n",
    "df_cha_nolabels = df_cha.copy()\n",
    "df_cha_nolabels['ch_name'] = df_cha_nolabels['ch_name'].str[:-4]\n",
    "\n",
    "# Separate HbO and HbR\n",
    "df_hbo = df_cha_nolabels[df_cha_nolabels['Chroma'].str.endswith('hbo')].set_index(['subject', 'Condition', 'group', 'day', 'ch_name']).sort_index()\n",
    "df_hbr = df_cha_nolabels[df_cha_nolabels['Chroma'].str.endswith('hbr')].set_index(['subject', 'Condition', 'group', 'day', 'ch_name']).sort_index()\n",
    "\n",
    "# Compute the difference\n",
    "df_cha_diff_list = []\n",
    "for ch_name in df_hbo.index.get_level_values('ch_name').unique():\n",
    "    # Get aligned indices\n",
    "    df_hbo_ch = df_hbo.loc[(slice(None), slice(None), slice(None), slice(None), ch_name), :].sort_index()\n",
    "    df_hbr_ch = df_hbr.loc[(slice(None), slice(None), slice(None), slice(None), ch_name), :].sort_index()\n",
    "    \n",
    "    # Ensure df_hbo_ch and df_hbr_ch have the same length\n",
    "    common_index = df_hbo_ch.index.intersection(df_hbr_ch.index)\n",
    "    df_hbo_ch = df_hbo_ch.loc[common_index]\n",
    "    df_hbr_ch = df_hbr_ch.loc[common_index]\n",
    "    \n",
    "    # Calculate the difference\n",
    "    df_diff = df_hbo_ch[['theta']].sub(df_hbr_ch[['theta']])\n",
    "    \n",
    "    # Align df_cha_ch with df_diff\n",
    "    df_cha_ch = df_hbo_ch.reset_index()\n",
    "    df_cha_ch['theta'] = df_diff.values\n",
    "    df_cha_ch['Chroma'] = 'hbdiff'\n",
    "    df_cha_ch['ch_name'] = df_cha_ch['ch_name'] + ' hbdiff'\n",
    "    \n",
    "    if not df_cha_ch.empty:\n",
    "        df_cha_diff_list.append(df_cha_ch)\n",
    "\n",
    "df_cha_diff_concat = pd.concat(df_cha_diff_list, ignore_index=True)\n",
    "\n",
    "# Concatenate original df_cha with df_cha_diff_concat\n",
    "df_final = pd.concat([df_cha, df_cha_diff_concat], ignore_index=True)\n",
    "df_final.to_csv(op.join(results_path, f'df_combined_final_cha_{output_suffix}.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run correlational analyses below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ValueWarning) # type: ignore\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # type: ignore\n",
    "\n",
    "# Load the datasets\n",
    "behavior_file = '../../behavior_diff_data.csv'\n",
    "behavior_df = pd.read_csv(behavior_file)\n",
    "behavior_df['subject'] = behavior_df['subject'].astype(str)\n",
    "df_final = pd.read_csv(op.join('../../results/df_combined_final_cha_final.csv'))\n",
    "theta_df = df_final.copy()\n",
    "theta_df['subject'] = theta_df['subject'].astype(str)\n",
    "results_path = '../../results/correlations'\n",
    "\n",
    "# Get the unique conditions\n",
    "conditions = ['A', 'AV', 'V']\n",
    "response_vars = ['AO_WR', 'AV_WR', 'TBW']\n",
    "chromas = ['hbo', 'hbr', 'hbdiff']\n",
    "days = [1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline theta vs. WR changes\n",
    "\n",
    "def perform_analysis(group, output_suffix):\n",
    "    # Initialize a list to store significant models\n",
    "    significant_models = []\n",
    "    \n",
    "    # Initialize a dictionary to store p-values and model data by condition, response variable, chroma, and day\n",
    "    all_p_values = {condition: {response_var: {chroma: {day: [] for day in days} for chroma in chromas} for response_var in response_vars} for condition in conditions}\n",
    "    all_model_data = {condition: {response_var: {chroma: {day: [] for day in days} for chroma in chromas} for response_var in response_vars} for condition in conditions}\n",
    "\n",
    "    # Track maximum R-squared value\n",
    "    max_r_squared = 0\n",
    "    max_r_squared_model = None\n",
    "\n",
    "    for day in days:\n",
    "        theta_df_filtered = theta_df[theta_df['day'] == day].copy()\n",
    "        theta_df_filtered['ch_name'] = theta_df_filtered['ch_name'].str.split(' ').str[0]\n",
    "\n",
    "        for chroma in chromas:\n",
    "            # Filter theta_df for the specific chroma\n",
    "            theta_dataset = theta_df_filtered[theta_df_filtered['Chroma'] == chroma].copy()\n",
    "            theta_dataset['ch_name'] = theta_dataset['ch_name'].str.split(' ').str[0]\n",
    "\n",
    "            # Collect all p-values and model data for each response variable and condition\n",
    "            for condition in conditions:\n",
    "                for response_var in response_vars:\n",
    "                    # Filter the dataset for the current condition\n",
    "                    theta_df_condition = theta_dataset[theta_dataset['Condition'] == condition]\n",
    "                    \n",
    "                    # Pivot the theta_df to have channel names as columns\n",
    "                    theta_pivot = theta_df_condition.pivot_table(index=['subject', 'group', 'Condition'], columns='ch_name', values='theta').reset_index()\n",
    "                    theta_pivot['subject'] = theta_pivot['subject'].astype(str)  # Ensure subject is string\n",
    "\n",
    "                    # Merge the datasets based on 'subject' and 'group'\n",
    "                    merged_df = pd.merge(theta_pivot, behavior_df[['subject', 'group', 'TBW', 'AO_WR', 'AV_WR', 'VO_WR', 'age', 'AO_WR_1', 'AV_WR_1', 'VO_WR_1', 'TBW_1']], on=['subject', 'group'])\n",
    "                    channels = theta_df_condition['ch_name'].unique()  # list of all channel names\n",
    "\n",
    "                    for channel in channels:\n",
    "                        df = merged_df[[channel, response_var, 'group']].dropna()  # drop rows with missing values\n",
    "                        if df.empty:\n",
    "                            continue  # Skip this channel if there is no data\n",
    "                        model = smf.ols(f\"{response_var} ~ {channel}\", df[df['group'] == group]).fit()\n",
    "                        r_sq = model.rsquared\n",
    "                        p_value_channel = model.pvalues[channel]  # p-value for the channel\n",
    "                        all_p_values[condition][response_var][chroma][day].append(p_value_channel)\n",
    "                        all_model_data[condition][response_var][chroma][day].append((condition, channel, response_var, model, r_sq, p_value_channel, df, chroma, day))\n",
    "\n",
    "                    p_values = all_p_values[condition][response_var][chroma][day]\n",
    "                    model_data = all_model_data[condition][response_var][chroma][day]\n",
    "                    \n",
    "                    if p_values:\n",
    "                        # Apply FDR correction\n",
    "                        rejected, p_values_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "                        \n",
    "                        # Filter results based on FDR corrected p-values\n",
    "                        for (condition, channel, response_var, model, r_sq, p_value, df, chroma, day), p_val_corr, reject in zip(model_data, p_values_corrected, rejected):\n",
    "                            if p_val_corr < 0.05:\n",
    "                                print(f\"Group: {group}, Day: {day}, Chroma: {chroma}, Condition: {condition}, Channel: {channel}, Outcome: {response_var}\\n   R-squared: {r_sq}, corrected p-value: {p_val_corr}\\n\")\n",
    "                                significant_models.append({\n",
    "                                    'Condition': condition,\n",
    "                                    'Channel': channel,\n",
    "                                    'Response Variable': response_var,\n",
    "                                    'R-squared': r_sq,\n",
    "                                    'P-value': p_value,\n",
    "                                    'P-value Corrected': p_val_corr,\n",
    "                                    'Model Summary': model.summary().as_text(),\n",
    "                                    'Chroma': chroma,\n",
    "                                    'Day': day\n",
    "                                })\n",
    "\n",
    "                                # Plot the significant results\n",
    "                                plt.figure(figsize=(8, 6))\n",
    "\n",
    "                                # Plot trained data\n",
    "                                trained_df = df[df['group'] == 'trained']\n",
    "                                if not trained_df.empty:\n",
    "                                    trained_model = smf.ols(f\"{response_var} ~ {channel}\", trained_df).fit()\n",
    "                                    sns.scatterplot(x=trained_df[channel], y=trained_df[response_var], label='Trained', color='#92b6f0', s=100)\n",
    "                                    sns.lineplot(x=trained_df[channel], y=trained_model.predict(trained_df), color='#92b6f0', linewidth=2)\n",
    "\n",
    "                                # Plot control data\n",
    "                                control_df = df[df['group'] == 'control']\n",
    "                                if not control_df.empty:\n",
    "                                    control_model = smf.ols(f\"{response_var} ~ {channel}\", control_df).fit()\n",
    "                                    sns.scatterplot(x=control_df[channel], y=control_df[response_var], label='Control', color='gray', s=100)\n",
    "                                    sns.lineplot(x=control_df[channel], y=control_model.predict(control_df), color='gray', linewidth=2)\n",
    "                                \n",
    "                                xlabel = ('[HbO] on Day ' + str(day) if chroma == 'hbo' else\n",
    "                                        '[HbR] on Day ' + str(day) if chroma == 'hbr' else\n",
    "                                        '[HbDiff] on Day ' + str(day) if chroma == 'hbdiff' else\n",
    "                                        f'{chroma.upper()} on Day ' + str(day))\n",
    "                                ylabel = ('Change in Auditory Word Recognition' if response_var == 'AO_WR' else\n",
    "                                        'Change in Audiovisual Word Recognition' if response_var == 'AV_WR' else\n",
    "                                        'Change in Visual Word Recognition' if response_var == 'VO_WR' else\n",
    "                                        f'Change in {response_var}')\n",
    "                                plt.legend(loc='upper right') \n",
    "                                plt.xlabel(xlabel, fontsize=16)\n",
    "                                plt.ylabel(ylabel, fontsize=16)\n",
    "                                plt.title(f'{ylabel} vs.\\nCortical Response to {condition} Speech ({channel})', fontsize=16)\n",
    "                                plt.savefig(op.join(results_path, f'{output_suffix}__{group}_day{day}_{chroma}_{condition}_{response_var}_{channel}_plot.png'))\n",
    "                                plt.close()\n",
    "                                \n",
    "    if significant_models:\n",
    "        significant_models_df = pd.DataFrame(significant_models).sort_values(by='R-squared', ascending=False)\n",
    "        significant_models_df.to_csv(op.join(results_path, f'{output_suffix}_{group}_models.csv'), index=False)\n",
    "    else:\n",
    "        print(f'No significant models found for {group} group')\n",
    "\n",
    "# Perform analysis for the trained group\n",
    "perform_analysis('trained', 'theta_vs_behavior')\n",
    "\n",
    "# Perform analysis for the control group\n",
    "perform_analysis('control', 'theta_vs_behavior')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes in theta vs. WR changes\n",
    "\n",
    "# Load the datasets\n",
    "theta_df_filtered = theta_df.pivot_table(index=['subject', 'group', 'Condition', 'Chroma', 'ch_name'], columns='day', values='theta').reset_index()\n",
    "theta_df_filtered['theta_diff'] = theta_df_filtered[3] - theta_df_filtered[1]\n",
    "output_suffix = \"theta_diff_final\"\n",
    "\n",
    "def perform_analysis(group, output_suffix):\n",
    "    # Initialize a list to store significant models\n",
    "    theta_df_filtered['ch_name'] = theta_df_filtered['ch_name'].str.split(' ').str[0]\n",
    "    significant_models = []\n",
    "    all_p_values = {condition: {response_var: {chroma: [] for chroma in chromas} for response_var in response_vars} for condition in conditions}\n",
    "    all_model_data = {condition: {response_var: {chroma: [] for chroma in chromas} for response_var in response_vars} for condition in conditions}\n",
    "\n",
    "    for chroma in chromas:\n",
    "        # Filter theta_df for the specific chroma\n",
    "        theta_dataset = theta_df_filtered[theta_df_filtered['Chroma'] == chroma].copy()\n",
    "        theta_dataset['ch_name'] = theta_dataset['ch_name'].str.split(' ').str[0]\n",
    "\n",
    "        # Collect all p-values and model data for each response variable and condition\n",
    "        for condition in conditions:\n",
    "            for response_var in response_vars:\n",
    "                # Filter the dataset for the current condition\n",
    "                theta_df_condition = theta_dataset[theta_dataset['Condition'] == condition]\n",
    "                \n",
    "                # Pivot the theta_df to have channel names as columns\n",
    "                theta_pivot = theta_df_condition.pivot_table(index=['subject', 'group', 'Condition'], columns='ch_name', values='theta_diff').reset_index()\n",
    "                theta_pivot['subject'] = theta_pivot['subject'].astype(str)  # Ensure subject is string\n",
    "\n",
    "                # Merge the datasets based on 'subject' and 'group'\n",
    "                merged_df = pd.merge(theta_pivot, behavior_df[['subject', 'group', 'TBW', 'AO_WR', 'AV_WR', 'VO_WR', 'age', 'AO_WR_1', 'AV_WR_1', 'VO_WR_1', 'TBW_1']], on=['subject', 'group'])\n",
    "                channels = theta_df_condition['ch_name'].unique()  # list of all channel names\n",
    "\n",
    "                for channel in channels:\n",
    "                    df = merged_df[[channel, response_var, 'group']].dropna()  # drop rows with missing values\n",
    "                    if df.empty:\n",
    "                        continue  # Skip this channel if there is no data\n",
    "                    model = smf.ols(f\"{response_var} ~ {channel}\", df[df['group'] == group]).fit()\n",
    "                    r_sq = model.rsquared\n",
    "                    p_value_channel = model.pvalues[channel]  # p-value for the channel\n",
    "                    all_p_values[condition][response_var][chroma].append(p_value_channel)\n",
    "                    all_model_data[condition][response_var][chroma].append((condition, channel, response_var, model, r_sq, p_value_channel, df, chroma))\n",
    "\n",
    "                p_values = all_p_values[condition][response_var][chroma]\n",
    "                model_data = all_model_data[condition][response_var][chroma]\n",
    "                \n",
    "                if p_values:\n",
    "                    # Apply FDR correction\n",
    "                    rejected, p_values_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "                    \n",
    "                    # Filter results based on FDR corrected p-values\n",
    "                    for (condition, channel, response_var, model, r_sq, p_value, df, chroma), p_val_corr, reject in zip(model_data, p_values_corrected, rejected):\n",
    "                        if p_val_corr < 0.05:\n",
    "                            print(f\"Group: {group}, Chroma: {chroma}, Condition: {condition}, Channel: {channel}, Outcome: {response_var}\\n   R-squared: {r_sq}, p-value: {p_value}, corrected p-value: {p_val_corr}\\n\")\n",
    "                            significant_models.append({\n",
    "                                'Condition': condition,\n",
    "                                'Channel': channel,\n",
    "                                'Response Variable': response_var,\n",
    "                                'R-squared': r_sq,\n",
    "                                'P-value': p_value,\n",
    "                                'P-value Corrected': p_val_corr,\n",
    "                                'Model Summary': model.summary().as_text(),\n",
    "                                'Chroma': chroma,\n",
    "                            })\n",
    "                            # Plot the significant results\n",
    "                            plt.figure(figsize=(8, 6))\n",
    "\n",
    "                            # Plot trained data\n",
    "                            trained_df = df[df['group'] == 'trained']\n",
    "                            if not trained_df.empty:\n",
    "                                trained_model = smf.ols(f\"{response_var} ~ {channel}\", trained_df).fit()\n",
    "                                sns.scatterplot(x=trained_df[channel], y=trained_df[response_var], label='Trained', color='#92b6f0', s=100)\n",
    "                                sns.lineplot(x=trained_df[channel], y=trained_model.predict(trained_df), color='#92b6f0', linewidth=2)\n",
    "\n",
    "                            # Plot control data\n",
    "                            control_df = df[df['group'] == 'control']\n",
    "                            if not control_df.empty:\n",
    "                                control_model = smf.ols(f\"{response_var} ~ {channel}\", control_df).fit()\n",
    "                                sns.scatterplot(x=control_df[channel], y=control_df[response_var], label='Control', color='gray', s=100)\n",
    "                                sns.lineplot(x=control_df[channel], y=control_model.predict(control_df), color='gray', linewidth=2)\n",
    "                            \n",
    "                            xlabel = f'{chroma.upper()} Change (Day 3 - Day 1) on {channel}'\n",
    "                            ylabel = ('Change in Auditory Word Recognition' if response_var == 'AO_WR' else\n",
    "                                    'Change in Audiovisual Word Recognition' if response_var == 'AV_WR' else\n",
    "                                    'Change in Visual Word Recognition' if response_var == 'VO_WR' else\n",
    "                                    f'Change in {response_var}')\n",
    "                            plt.legend(loc='upper right') \n",
    "                            plt.xlabel(xlabel, fontsize=16)\n",
    "                            plt.ylabel(ylabel, fontsize=16)\n",
    "                            plt.title(f'{ylabel} vs.\\nCortical Response to {condition} Speech ({channel})', fontsize=16)\n",
    "                            plt.savefig(op.join(results_path, f'{output_suffix}__{group}_{chroma}_{condition}_{response_var}_{channel}_plot.png'))\n",
    "                            plt.close()\n",
    "                                \n",
    "    if significant_models:\n",
    "        significant_models_df = pd.DataFrame(significant_models).sort_values(by='R-squared', ascending=False)\n",
    "        csv_file = op.join(results_path, f'{output_suffix}_{group}_models.csv')\n",
    "        if not op.isfile(csv_file):\n",
    "            significant_models_df.to_csv(csv_file, index=False, mode='w')\n",
    "        else:\n",
    "            significant_models_df.to_csv(csv_file, index=False, mode='a', header=False)\n",
    "    else:\n",
    "        print(f'No significant models found for {group} group')\n",
    "\n",
    "\n",
    "# Perform analysis for the trained group\n",
    "perform_analysis('trained', 'theta_diff_vs_behavior')\n",
    "\n",
    "# Perform analysis for the control group\n",
    "perform_analysis('control', 'theta_diff_vs_behavior')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run same analysis for baseline theta vs. change in theta values\n",
    "\n",
    "# Pivot table to get both Day 1 theta and theta_diff (Day 3 - Day 1)\n",
    "theta_df_filtered = theta_df.pivot_table(index=['subject', 'group', 'Condition', 'Chroma', 'ch_name'], columns='day', values='theta').reset_index()\n",
    "theta_df_filtered['theta_diff'] = theta_df_filtered[3] - theta_df_filtered[1]  # Calculate theta_diff\n",
    "theta_df_filtered['theta_baseline'] = theta_df_filtered[1]  # Baseline theta (Day 1)\n",
    "\n",
    "# Specify the channels you want to analyze\n",
    "channels_to_analyze = ['S19_D4', 'S19_D6', 'S25_D14', 'S15_D14', 'S21_D10', 'S3_D4', 'S4_D3']  # Example channels\n",
    "\n",
    "# Get the unique conditions\n",
    "conditions = ['A', 'AV', 'V']\n",
    "response_vars = ['theta_diff']\n",
    "chromas = ['hbo', 'hbr', 'hbdiff']\n",
    "\n",
    "def perform_analysis(group, output_suffix, channels):\n",
    "    # Initialize a list to store significant models\n",
    "    theta_df_filtered['ch_name'] = theta_df_filtered['ch_name'].str.split(' ').str[0]\n",
    "    significant_models = []\n",
    "    \n",
    "    # Reset counters here\n",
    "    total_channels_analyzed = 0\n",
    "    significant_channels_count = 0\n",
    "    \n",
    "    for chroma in chromas:\n",
    "        all_p_values = []\n",
    "        model_data = []\n",
    "\n",
    "        for condition in conditions:\n",
    "            print(f\"\\nStarting analysis for condition: {condition}, chroma: {chroma}, group: {group}\\n\")\n",
    "            # Filter the dataset for the current condition and chroma\n",
    "            theta_df_condition = theta_df_filtered[(theta_df_filtered['Chroma'] == chroma) & (theta_df_filtered['Condition'] == condition)]\n",
    "            \n",
    "            # Filter the dataset to include only the specified channels\n",
    "            theta_df_condition = theta_df_condition[theta_df_condition['ch_name'].isin(channels)]\n",
    "\n",
    "            # Pivot the theta_df to have channel names as columns, keeping 'group' in the index\n",
    "            theta_pivot = theta_df_condition.pivot_table(index=['subject', 'group'], columns='ch_name', values=['theta_diff', 'theta_baseline'])\n",
    "            theta_pivot.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in theta_pivot.columns.values]  # Flatten the MultiIndex\n",
    "            \n",
    "            # Reset the index to make 'group' a column again\n",
    "            theta_pivot.reset_index(inplace=True)\n",
    "            \n",
    "            # Iterate over each channel\n",
    "            for channel in channels:                \n",
    "                # Ensure both baseline and diff values are available\n",
    "                baseline_column = f'theta_baseline_{channel}'\n",
    "                diff_column = f'theta_diff_{channel}'\n",
    "                \n",
    "                if baseline_column in theta_pivot.columns and diff_column in theta_pivot.columns:\n",
    "                    df = theta_pivot[[diff_column, baseline_column, 'group']].dropna()  # drop rows with missing values\n",
    "                    if df.empty:\n",
    "                        print(f\"  No data available for channel: {channel}, skipping.\")\n",
    "                        continue  # Skip this channel if there is no data\n",
    "\n",
    "                    total_channels_analyzed += 1  # Increment the total channels analyzed counter\n",
    "\n",
    "                    # Regression model: theta_diff ~ theta_baseline\n",
    "                    model = smf.ols(f\"{diff_column} ~ {baseline_column}\", df[df['group'] == group]).fit()\n",
    "                    r_sq = model.rsquared\n",
    "                    p_value_channel = model.pvalues[baseline_column]  # p-value for the channel\n",
    "\n",
    "                    # Collect p-values and models for FDR correction\n",
    "                    all_p_values.append(p_value_channel)\n",
    "                    model_data.append((condition, channel, r_sq, p_value_channel, model, chroma, baseline_column, diff_column, df))\n",
    "        \n",
    "        # Apply FDR correction\n",
    "        rejected, p_values_corrected, _, _ = multipletests(all_p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "        # Filter results based on FDR corrected p-values and plot only if significant\n",
    "        for i, (condition, channel, r_sq, p_value, model, chroma, baseline_column, diff_column, df) in enumerate(model_data):\n",
    "            if p_values_corrected[i] < 0.05:\n",
    "            #if p_values_corrected[i] < 0.05:\n",
    "                print(f\"Channel: {channel}, {condition}, chroma: {chroma}, group: {group}\")\n",
    "                print(f\"   p-value: {p_value}, corrected p-value: {p_values_corrected[i]}, R-squared: {r_sq}\\n\")\n",
    "                significant_channels_count += 1  # Increment the significant channels counter\n",
    "                significant_models.append({\n",
    "                    'Condition': condition,\n",
    "                    'Channel': channel,\n",
    "                    'R-squared': r_sq,\n",
    "                    'P-value': p_value,\n",
    "                    'P-value Corrected': p_values_corrected[i],\n",
    "                    'Model Summary': model.summary().as_text(),\n",
    "                    'Chroma': chroma,\n",
    "                    'Group': group\n",
    "                })\n",
    "\n",
    "                # Plot the significant results\n",
    "                plt.figure(figsize=(8, 6))\n",
    "\n",
    "                # Plot trained data\n",
    "                trained_df = df[df['group'] == 'trained']\n",
    "                if not trained_df.empty:\n",
    "                    trained_model = smf.ols(f\"{diff_column} ~ {baseline_column}\", trained_df).fit()\n",
    "                    sns.scatterplot(x=trained_df[baseline_column], y=trained_df[diff_column], label='Trained', color='#92b6f0', s=100)\n",
    "                    sns.lineplot(x=trained_df[baseline_column], y=trained_model.predict(trained_df), color='#92b6f0', linewidth=2)\n",
    "\n",
    "                # Plot control data\n",
    "                control_df = df[df['group'] == 'control']\n",
    "                if not control_df.empty:\n",
    "                    control_model = smf.ols(f\"{diff_column} ~ {baseline_column}\", control_df).fit()\n",
    "                    sns.scatterplot(x=control_df[baseline_column], y=control_df[diff_column], label='Control', color='gray', s=100)\n",
    "                    sns.lineplot(x=control_df[baseline_column], y=control_model.predict(control_df), color='gray', linewidth=2)\n",
    "                \n",
    "                xlabel = f'Baseline {chroma.upper()} (Day 1) on {channel}'\n",
    "                ylabel = f'{chroma.upper()} Change (Day 3 - Day 1)'\n",
    "                plt.legend(loc='upper right') \n",
    "                plt.xlabel(xlabel, fontsize=16)\n",
    "                plt.ylabel(ylabel, fontsize=16)\n",
    "                plt.title(f'{ylabel} vs.\\nBaseline Cortical Response to {condition} Speech ({channel})', fontsize=16)\n",
    "                plt.savefig(op.join(results_path, f'{output_suffix}__{group}_{chroma}_{condition}_{channel}_plot.png'))\n",
    "                plt.close()\n",
    "\n",
    "    # Save significant models to CSV\n",
    "    if significant_models:\n",
    "        significant_models_df = pd.DataFrame(significant_models).sort_values(by='R-squared', ascending=False)\n",
    "        csv_file = op.join(results_path, f'{output_suffix}_{group}_models.csv')\n",
    "        if not op.isfile(csv_file):\n",
    "            significant_models_df.to_csv(csv_file, index=False, mode='w')\n",
    "        else:\n",
    "            significant_models_df.to_csv(csv_file, index=False, mode='a', header=False)\n",
    "    else:\n",
    "        print(f'No significant models found for group {group}')\n",
    "    \n",
    "    # Print the counter\n",
    "    print(f\"\\n{significant_channels_count} out of {total_channels_analyzed} channels were significantly correlated for group {group}.\")\n",
    "\n",
    "# Perform analysis for the trained group\n",
    "perform_analysis('trained', 'theta_diff_vs_baseline', channels_to_analyze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run same analysis for day 1 vs day 3 theta values\n",
    "\n",
    "# Load the datasets\n",
    "behavior_df = pd.read_csv(behavior_file)\n",
    "behavior_df['subject'] = behavior_df['subject'].astype(str)\n",
    "\n",
    "theta_df_filtered = df_final.copy()\n",
    "theta_df_filtered['subject'] = theta_df_filtered['subject'].astype(str)\n",
    "\n",
    "# Pivot table to get both Day 1 theta and theta_diff (Day 3 - Day 1)\n",
    "theta_df_filtered = theta_df_filtered.pivot_table(index=['subject', 'group', 'Condition', 'Chroma', 'ch_name'], columns='day', values='theta').reset_index()\n",
    "theta_df_filtered['theta_diff'] = theta_df_filtered['3'] - theta_df_filtered['1']  # Calculate theta_diff\n",
    "theta_df_filtered['theta_baseline'] = theta_df_filtered['1']  # Baseline theta (Day 1)\n",
    "\n",
    "# Specify the channels you want to analyze\n",
    "channels_to_analyze = ['S19_D4', 'S19_D6', 'S25_D14', 'S15_D14', 'S21_D10', 'S3_D4', 'S4_D3', 'S8_D18']  # Example channels\n",
    "\n",
    "# Get the unique conditions\n",
    "conditions = ['A', 'AV', 'V']\n",
    "chromas = ['hbo', 'hbr', 'hbdiff']\n",
    "\n",
    "def perform_analysis(group, output_suffix, channels):\n",
    "    # Initialize a list to store significant models\n",
    "    theta_df_filtered['ch_name'] = theta_df_filtered['ch_name'].str.split(' ').str[0]\n",
    "    significant_models = []\n",
    "    total_channels_analyzed = 0\n",
    "    significant_channels_count = 0\n",
    "\n",
    "    for chroma in chromas:\n",
    "        all_p_values = []\n",
    "        model_data = []\n",
    "\n",
    "        # Filter theta_df for the specific chroma\n",
    "        theta_dataset = theta_df_filtered[theta_df_filtered['Chroma'] == chroma].copy()\n",
    "        theta_dataset['ch_name'] = theta_dataset['ch_name'].str.split(' ').str[0]\n",
    "\n",
    "        # Collect all p-values and model data for each condition\n",
    "        for condition in conditions:\n",
    "            print(f\"\\nStarting analysis for condition: {condition}, chroma: {chroma}, group: {group}\\n\")\n",
    "            # Filter the dataset for the current condition\n",
    "            theta_df_condition = theta_dataset[theta_dataset['Condition'] == condition].copy()\n",
    "            \n",
    "            # Filter the dataset to include only the specified channels\n",
    "            theta_df_condition = theta_df_condition[theta_df_condition['ch_name'].isin(channels)]\n",
    "\n",
    "            # Pivot the theta_df to have channel names as columns, keeping 'group' in the index\n",
    "            theta_pivot = theta_df_condition.pivot_table(index=['subject', 'group'], columns='ch_name', values=['1', '3'])  # 1 for Day 1, 3 for Day 3\n",
    "            theta_pivot.columns = [f'{day}_{ch_name}' for day, ch_name in theta_pivot.columns]  # Simplify the column names\n",
    "            \n",
    "            # Reset the index to make 'group' a column again\n",
    "            theta_pivot.reset_index(inplace=True)\n",
    "            \n",
    "            # Iterate over each channel\n",
    "            for channel in channels:                \n",
    "                # Ensure both Day 1 and Day 3 values are available\n",
    "                day1_column = f'1_{channel}'\n",
    "                day3_column = f'3_{channel}'\n",
    "                \n",
    "                if day1_column in theta_pivot.columns and day3_column in theta_pivot.columns:\n",
    "                    df = theta_pivot[[day1_column, day3_column, 'group']].dropna()  # drop rows with missing values\n",
    "                    if df.empty:\n",
    "                        print(f\"  No data available for channel: {channel}, skipping.\")\n",
    "                        continue  # Skip this channel if there is no data\n",
    "\n",
    "                    total_channels_analyzed += 1  # Increment the total channels analyzed counter\n",
    "\n",
    "                    # Regression model: Day 3 ~ Day 1\n",
    "                    formula = f'Q(\"{day3_column}\") ~ Q(\"{day1_column}\")'\n",
    "                    model = smf.ols(formula, df[df['group'] == group]).fit()\n",
    "                    r_sq = model.rsquared\n",
    "                    p_value_channel = model.pvalues[f'Q(\"{day1_column}\")']  # p-value for the channel\n",
    "\n",
    "                    # Collecting all p-values and models\n",
    "                    all_p_values.append(p_value_channel)\n",
    "                    model_data.append((condition, channel, r_sq, p_value_channel, model, chroma))\n",
    "\n",
    "        if all_p_values:\n",
    "            # Apply FDR correction\n",
    "            rejected, p_values_corrected, _, _ = multipletests(all_p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "            # Filter results based on FDR corrected p-values and plot only if significant\n",
    "            for i, (condition, channel, r_sq, p_value, model, chroma) in enumerate(model_data):\n",
    "                if p_values_corrected[i] < 0.05:\n",
    "                    print(f\"Channel: {channel}, {condition}, chroma: {chroma}, group: {group}\\n   R-squared: {r_sq}, corrected p-value: {p_values_corrected[i]}\\n\")\n",
    "                    significant_channels_count += 1  # Increment the significant channels counter\n",
    "                    significant_models.append({\n",
    "                        'Condition': condition,\n",
    "                        'Channel': channel,\n",
    "                        'R-squared': r_sq,\n",
    "                        'P-value': p_value,\n",
    "                        'P-value Corrected': p_values_corrected[i],\n",
    "                        'Model Summary': model.summary().as_text(),\n",
    "                        'Chroma': chroma,\n",
    "                        'Group': group\n",
    "                    })\n",
    "\n",
    "                    # Plot the significant results\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "\n",
    "                    # Plot trained data\n",
    "                    trained_df = df[df['group'] == 'trained']\n",
    "                    if not trained_df.empty:\n",
    "                        trained_model = smf.ols(formula, trained_df).fit()\n",
    "                        sns.scatterplot(x=trained_df[day1_column], y=trained_df[day3_column], label='Trained', color='#92b6f0', s=100)\n",
    "                        sns.lineplot(x=trained_df[day1_column], y=trained_model.predict(trained_df), color='#92b6f0', linewidth=2)\n",
    "\n",
    "                    # Plot control data\n",
    "                    control_df = df[df['group'] == 'control']\n",
    "                    if not control_df.empty:\n",
    "                        control_model = smf.ols(formula, control_df).fit()\n",
    "                        sns.scatterplot(x=control_df[day1_column], y=control_df[day3_column], label='Control', color='gray', s=100)\n",
    "                        sns.lineplot(x=control_df[day1_column], y=control_model.predict(control_df), color='gray', linewidth=2)\n",
    "\n",
    "                    xlabel = f'{chroma.upper()} (Day 1) on {channel}'\n",
    "                    ylabel = f'{chroma.upper()} (Day 3)'\n",
    "                    plt.legend(loc='upper right') \n",
    "                    plt.xlabel(xlabel, fontsize=16)\n",
    "                    plt.ylabel(ylabel, fontsize=16)\n",
    "                    plt.title(f'{ylabel} vs.\\nCortical Response to {condition} Speech ({channel})', fontsize=16)\n",
    "                    plt.savefig(op.join(results_path, f'{output_suffix}__{group}_{chroma}_{condition}_{channel}_plot.png'))\n",
    "                    plt.close()\n",
    "\n",
    "    # Save significant models to CSV\n",
    "    if significant_models:\n",
    "        significant_models_df = pd.DataFrame(significant_models).sort_values(by='R-squared', ascending=False)\n",
    "        csv_file = op.join(results_path, f'{output_suffix}_{group}_models.csv')\n",
    "        if not op.isfile(csv_file):\n",
    "            significant_models_df.to_csv(csv_file, index=False, mode='w')\n",
    "        else:\n",
    "            significant_models_df.to_csv(csv_file, index=False, mode='a', header=False)\n",
    "    else:\n",
    "        print(f'No significant models found for group {group}')\n",
    "    \n",
    "    # Print the counter\n",
    "    print(f\"\\n{significant_channels_count} out of {total_channels_analyzed} channels were significantly correlated for group {group}.\")\n",
    "\n",
    "# Perform analysis for the trained group\n",
    "perform_analysis('trained', 'theta_day1_vs_day3', channels_to_analyze)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below is still in progress..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sensors of interest\n",
    "\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as op\n",
    "\n",
    "# Use the info from your existing data\n",
    "proc_path = '../../processed'\n",
    "fname = op.join(proc_path, f'205_1_001_long_hbo_final_raw.fif')\n",
    "use = mne.io.read_raw_fif(fname, preload=True)\n",
    "use.load_data()\n",
    "info = use.copy().info\n",
    "\n",
    "# Specify the names of the sensors you want to plot\n",
    "selected_sensors = ['S25_D14 hbo']\n",
    "valid_sensors = [sensor for sensor in selected_sensors if sensor in info['ch_names']]\n",
    "\n",
    "if not valid_sensors:\n",
    "    print(\"No valid sensors selected for plotting.\")\n",
    "else:\n",
    "    # Increase the font size for sensor names\n",
    "    plt.rcParams.update({'font.size': 20})  # Set the desired font size\n",
    "\n",
    "    # Pick only the selected channels\n",
    "    info_picked = mne.pick_info(info, mne.pick_channels(info['ch_names'], valid_sensors))\n",
    "    info_picked.rename_channels({ch: '  ' + ch for ch in info_picked['ch_names']})\n",
    "\n",
    "    # Plot the sensors manually with blue circles\n",
    "    fig = mne.viz.plot_sensors(info_picked, kind='topomap', show_names=False, pointsize=800, linewidth=0)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some waveforms\n",
    "\n",
    "import mne\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load subject group mapping\n",
    "subject_group_mapping = pd.read_csv('../../subject_group_mapping.csv')\n",
    "subject_group_mapping['Subject'] = subject_group_mapping['Subject'].astype(str).str.strip().str.split('.').str[0]\n",
    "subject_to_group = dict(zip(subject_group_mapping['Subject'], subject_group_mapping['Group']))\n",
    "\n",
    "# Ensure all subjects are in string format without decimals\n",
    "subjects = subject_group_mapping['Subject'].tolist()\n",
    "subjects = [subject for subject in subjects if subject not in ['202', '203', '204', '206', '214', '221', '223', '226', '233']]\n",
    "\n",
    "# Define the subjects in the 'trained' group\n",
    "trained_subjects = [subject for subject in subjects if subject_to_group[subject] == 'control']\n",
    "\n",
    "# Specify the channel of interest\n",
    "channel_of_interest = 'S25_D14'\n",
    "condition = 'AV'\n",
    "\n",
    "# Initialize the dictionaries to store evoked data for averaging and plotting\n",
    "evoked_dict_day1 = {'HbO': [], 'HbR': []}\n",
    "evoked_dict_day3 = {'HbO': [], 'HbR': []}\n",
    "\n",
    "# Loop over subjects and load their evoked data\n",
    "for subject in trained_subjects:\n",
    "    for day in ['1', '3']:\n",
    "        # Define the filename\n",
    "        fname = op.join(proc_path, f'{subject}_{day}_final-ave.fif')\n",
    "        \n",
    "        # Load the evoked data\n",
    "        evokeds = mne.read_evokeds(fname, condition=condition, baseline=(None, 0), verbose=False)\n",
    "                \n",
    "        # Pick the HbO and HbR data for the channel of interest\n",
    "        evoked_hbo = evokeds.copy().pick([f'{channel_of_interest} hbo'])\n",
    "        evoked_hbr = evokeds.copy().pick([f'{channel_of_interest} hbr'])\n",
    "        \n",
    "        evoked_hbo.rename_channels(lambda x: x[:-4])\n",
    "        evoked_hbr.rename_channels(lambda x: x[:-4])\n",
    "        \n",
    "        # Check if both HbO and HbR data exist for this subject and day\n",
    "        if len(evoked_hbo.ch_names) == 1 and len(evoked_hbr.ch_names) == 1:\n",
    "            # Store in the appropriate evoked dictionary\n",
    "            if day == '1':\n",
    "                evoked_dict_day1['HbO'].append(evoked_hbo)\n",
    "                evoked_dict_day1['HbR'].append(evoked_hbr)\n",
    "            elif day == '3':\n",
    "                evoked_dict_day3['HbO'].append(evoked_hbo)\n",
    "                evoked_dict_day3['HbR'].append(evoked_hbr)\n",
    "\n",
    "# Compute grand average for Day 1 and Day 3\n",
    "grand_avg_day1_hbo = mne.grand_average(evoked_dict_day1['HbO'])\n",
    "grand_avg_day1_hbr = mne.grand_average(evoked_dict_day1['HbR'])\n",
    "grand_avg_day3_hbo = mne.grand_average(evoked_dict_day3['HbO'])\n",
    "grand_avg_day3_hbr = mne.grand_average(evoked_dict_day3['HbR'])\n",
    "\n",
    "# Prepare the grand average dictionary for plotting\n",
    "grand_avg_dict_day1 = {'HbO': grand_avg_day1_hbo, 'HbR': grand_avg_day1_hbr}\n",
    "grand_avg_dict_day3 = {'HbO': grand_avg_day3_hbo, 'HbR': grand_avg_day3_hbr}\n",
    "\n",
    "# Define the colors and styles\n",
    "color_dict_day1 = {'HbO': 'red', 'HbR': 'blue'}\n",
    "color_dict_day3 = {'HbO': 'red', 'HbR': 'blue'}\n",
    "styles_dict_day1 = {'HbO': {'linestyle': '--'}, 'HbR': {'linestyle': '--'}}\n",
    "styles_dict_day3 = {'HbO': {'linestyle': '-'}, 'HbR': {'linestyle': '-'}}\n",
    "\n",
    "# Plot for Day 1 using the grand average\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    grand_avg_dict_day1,\n",
    "    ci=0.95, colors=color_dict_day1, styles=styles_dict_day1,\n",
    "    title=f'{channel_of_interest} ({condition} Condition) - Day 1', show=True,\n",
    "    truncate_xaxis=(-2, 15),  # This line sets the x-axis limits\n",
    "    ylim=dict(hbo=(-0.10, 0.10))  # This line sets the y-axis limits\n",
    ")\n",
    "\n",
    "# Plot for Day 3 using the grand average\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    grand_avg_dict_day3,\n",
    "    ci=0.95, colors=color_dict_day3, styles=styles_dict_day3,\n",
    "    title=f'{channel_of_interest} ({condition} Condition) - Day 3', show=True,\n",
    "    truncate_xaxis=(-2, 15),  # This line sets the x-axis limits\n",
    "    ylim=dict(hbo=(-0.10, 0.10))  # This line sets the y-axis limits\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the dictionaries to store evoked data for plotting\n",
    "evoked_dict_day1 = {'HbO': [], 'HbR': []}\n",
    "evoked_dict_day3 = {'HbO': [], 'HbR': []}\n",
    "\n",
    "# Loop over subjects and load their evoked data\n",
    "for subject in trained_subjects:\n",
    "    for day in ['1', '3']:\n",
    "        # Define the filename\n",
    "        fname = op.join(proc_path, f'{subject}_{day}_final-ave.fif')\n",
    "        \n",
    "        # Load the evoked data\n",
    "        evokeds = mne.read_evokeds(fname, condition=condition, baseline=(None, 0), verbose=False)\n",
    "                \n",
    "        # Pick the HbO and HbR data for the channel of interest\n",
    "        evoked_hbo = evokeds.copy().pick([f'{channel_of_interest} hbo'])\n",
    "        evoked_hbr = evokeds.copy().pick([f'{channel_of_interest} hbr'])\n",
    "        \n",
    "        evoked_hbo.rename_channels(lambda x: x[:-4])\n",
    "        evoked_hbr.rename_channels(lambda x: x[:-4])\n",
    "        \n",
    "        # Check if both HbO and HbR data exist for this subject and day\n",
    "        if len(evoked_hbo.ch_names) == 1 and len(evoked_hbr.ch_names) == 1:\n",
    "            # Store in the appropriate evoked dictionary\n",
    "            if day == '1':\n",
    "                evoked_dict_day1['HbO'].append(evoked_hbo)\n",
    "                evoked_dict_day1['HbR'].append(evoked_hbr)\n",
    "            elif day == '3':\n",
    "                evoked_dict_day3['HbO'].append(evoked_hbo)\n",
    "                evoked_dict_day3['HbR'].append(evoked_hbr)\n",
    "\n",
    "# Check if all participants were included\n",
    "expected_subjects = len(trained_subjects)\n",
    "actual_subjects_day1 = len(evoked_dict_day1['HbO'])  # Same number for HbO and HbR\n",
    "actual_subjects_day3 = len(evoked_dict_day3['HbO'])  # Same number for HbO and HbR\n",
    "\n",
    "if actual_subjects_day1 != expected_subjects:\n",
    "    print(f\"Warning: Only {actual_subjects_day1} out of {expected_subjects} subjects were included in the Day 1 analysis.\")\n",
    "else:\n",
    "    print(\"All participants were included in the Day 1 analysis.\")\n",
    "\n",
    "if actual_subjects_day3 != expected_subjects:\n",
    "    print(f\"Warning: Only {actual_subjects_day3} out of {expected_subjects} subjects were included in the Day 3 analysis.\")\n",
    "else:\n",
    "    print(\"All participants were included in the Day 3 analysis.\")\n",
    "\n",
    "# Define the colors and styles\n",
    "color_dict_day1 = {'HbO': 'red', 'HbR': 'blue'}\n",
    "color_dict_day3 = {'HbO': 'red', 'HbR': 'blue'}\n",
    "styles_dict_day1 = {'HbO': {'linestyle': '--'}, 'HbR': {'linestyle': '--'}}\n",
    "styles_dict_day3 = {'HbO': {'linestyle': '-'}, 'HbR': {'linestyle': '-'}}\n",
    "\n",
    "# Plot for Day 1 using the individual evoked responses\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    {'HbO': evoked_dict_day1['HbO'], 'HbR': evoked_dict_day1['HbR']},\n",
    "    ci=0.95, colors=color_dict_day1, styles=styles_dict_day1,\n",
    "    title=f'{channel_of_interest} ({condition} Condition) - Day 1', show=True,\n",
    "    truncate_xaxis=(-2, 15),  # This line sets the x-axis limits\n",
    "    ylim=dict(hbo=(-0.10, 0.10))  # This line sets the y-axis limits\n",
    ")\n",
    "\n",
    "# Plot for Day 3 using the individual evoked responses\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    {'HbO': evoked_dict_day3['HbO'], 'HbR': evoked_dict_day3['HbR']},\n",
    "    ci=0.95, colors=color_dict_day3, styles=styles_dict_day3,\n",
    "    title=f'{channel_of_interest} ({condition} Condition) - Day 3', show=True,\n",
    "    truncate_xaxis=(-2, 15),  # This line sets the x-axis limits\n",
    "    ylim=dict(hbo=(-0.10, 0.10))  # This line sets the y-axis limits\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare thetas in 2 channels\n",
    "\n",
    "# Filter data for the specific channels you want to compare\n",
    "channels_to_analyze = ['S19_D6', 'S15_D14']\n",
    "\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ValueWarning) # type: ignore\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # type: ignore\n",
    "\n",
    "# Load the datasets\n",
    "behavior_df = pd.read_csv(behavior_file)\n",
    "behavior_df['subject'] = behavior_df['subject'].astype(str)\n",
    "\n",
    "theta_df_filtered = df_final.copy()\n",
    "theta_df_filtered['subject'] = theta_df_filtered['subject'].astype(str)\n",
    "\n",
    "# Pivot table to get both Day 1 theta and theta_diff (Day 3 - Day 1)\n",
    "theta_df_filtered = theta_df_filtered.pivot_table(index=['subject', 'group', 'Condition', 'Chroma', 'ch_name'], columns='day', values='theta').reset_index()\n",
    "theta_df_filtered['theta_diff'] = theta_df_filtered[3] - theta_df_filtered[1]  # Calculate theta_diff\n",
    "theta_df_filtered['theta_baseline'] = theta_df_filtered[1]  # Baseline theta (Day 1)\n",
    "theta_df_filtered = theta_df_filtered[theta_df_filtered['group'] != 'unknown']\n",
    "\n",
    "\n",
    "def perform_channel_comparison(group, output_suffix):\n",
    "    # Initialize a list to store significant models\n",
    "    theta_df_filtered['ch_name'] = theta_df_filtered['ch_name'].str.split(' ').str[0]\n",
    "    significant_models = []\n",
    "    \n",
    "    # Iterate over each condition and chroma\n",
    "    for chroma in chromas:\n",
    "        all_p_values = []\n",
    "        model_data = []\n",
    "\n",
    "        for condition in conditions:\n",
    "            print(f\"\\nStarting analysis for condition: {condition}, chroma: {chroma}, group: {group}\\n\")\n",
    "            # Filter the dataset for the current condition and chroma\n",
    "            theta_df_condition = theta_df_filtered[(theta_df_filtered['Chroma'] == chroma) & (theta_df_filtered['Condition'] == condition)]\n",
    "            \n",
    "            # Pivot the theta_df to have channel names as columns\n",
    "            theta_pivot = theta_df_condition.pivot_table(index=['subject', 'group'], columns='ch_name', values=['theta_diff', 'theta_baseline'])\n",
    "            theta_pivot.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in theta_pivot.columns.values]  # Flatten the MultiIndex\n",
    "            \n",
    "            # Reset the index to make 'group' a column again\n",
    "            theta_pivot.reset_index(inplace=True)\n",
    "\n",
    "            # Ensure both channels are in the pivoted dataframe\n",
    "            if f'theta_diff_S19_D6' in theta_pivot.columns and f'theta_diff_S15_D14' in theta_pivot.columns:\n",
    "                df = theta_pivot[[f'theta_diff_S19_D6', f'theta_diff_S15_D14', f'theta_baseline_S19_D6', f'theta_baseline_S15_D14', 'group']].dropna()\n",
    "                \n",
    "                if df.empty:\n",
    "                    print(\"  No data available for the selected channels, skipping.\")\n",
    "                    continue  # Skip this condition if there is no data\n",
    "\n",
    "                # Perform regression analysis for S19_D6 vs. S20_D10\n",
    "                model = smf.ols(f\"theta_diff_S15_D14 ~ theta_diff_S19_D6\", df[df['group'] == group]).fit()\n",
    "                r_sq = model.rsquared\n",
    "                p_value_channel = model.pvalues[f'theta_diff_S19_D6']\n",
    "\n",
    "                # Collect p-values and models for FDR correction\n",
    "                all_p_values.append(p_value_channel)\n",
    "                model_data.append((condition, 'S19_D6_vs_S15_D14', r_sq, p_value_channel, model, chroma, df))\n",
    "            else:\n",
    "                print(f\"  One or both channels S19_D6 and S15_D14 are missing for condition {condition}, skipping.\")\n",
    "                continue  # Skip this condition if there is no data\n",
    "\n",
    "        # Apply FDR correction\n",
    "        if len(all_p_values) > 0:\n",
    "            rejected, p_values_corrected, _, _ = multipletests(all_p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "        # Filter results based on FDR corrected p-values and plot only if significant\n",
    "        for i, (condition, comparison, r_sq, p_value, model, chroma, df) in enumerate(model_data):\n",
    "            if p_values_corrected[i] < 0.05:\n",
    "                print(f\"Comparison: {comparison}, {condition}, chroma: {chroma}, group: {group}\")\n",
    "                print(f\"   p-value: {p_value}, corrected p-value: {p_values_corrected[i]}, R-squared: {r_sq}\\n\")\n",
    "                \n",
    "                significant_models.append({\n",
    "                    'Condition': condition,\n",
    "                    'Comparison': comparison,\n",
    "                    'R-squared': r_sq,\n",
    "                    'P-value': p_value,\n",
    "                    'P-value Corrected': p_values_corrected[i],\n",
    "                    'Model Summary': model.summary().as_text(),\n",
    "                    'Chroma': chroma,\n",
    "                    'Group': group\n",
    "                })\n",
    "\n",
    "                # Plot the significant results\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.scatterplot(x=df[f'theta_diff_S19_D6'], y=df[f'theta_diff_S15_D14'], hue=df['group'], palette=['#92b6f0', 'gray'], s=100)\n",
    "                sns.lineplot(x=df[f'theta_diff_S19_D6'], y=model.predict(df), color='black', linewidth=2)\n",
    "\n",
    "                xlabel = f'{chroma.upper()} Change in S19_D6 (Day 3 - Day 1)'\n",
    "                ylabel = f'{chroma.upper()} Change in S15_D14 (Day 3 - Day 1)'\n",
    "                plt.xlabel(xlabel, fontsize=16)\n",
    "                plt.ylabel(ylabel, fontsize=16)\n",
    "                plt.title(f'Comparison of {xlabel} and {ylabel} in {condition} condition', fontsize=16)\n",
    "                plt.legend(title='Group', loc='upper right')\n",
    "                plt.savefig(op.join(results_path, f'{output_suffix}__{group}_{chroma}_{condition}_S19_D6_vs_S15_D14_plot.png'))\n",
    "                plt.close()\n",
    "\n",
    "    # Save significant models to CSV\n",
    "    if significant_models:\n",
    "        significant_models_df = pd.DataFrame(significant_models).sort_values(by='R-squared', ascending=False)\n",
    "        csv_file = op.join(results_path, f'{output_suffix}_{group}_models.csv')\n",
    "        if not op.isfile(csv_file):\n",
    "            significant_models_df.to_csv(csv_file, index=False, mode='w')\n",
    "        else:\n",
    "            significant_models_df.to_csv(csv_file, index=False, mode='a', header=False)\n",
    "    else:\n",
    "        print(f'No significant models found for group {group}')\n",
    "\n",
    "# Perform the comparison analysis for the trained group\n",
    "perform_channel_comparison('trained', 'S19_D6_vs_S15_D14_comparison')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Run paired t-test over days and plot changes over time\n",
    "\n",
    "# Load the final combined dataframe\n",
    "df_final = pd.read_csv(op.join(results_path, f'df_combined_final_cha_{output_suffix}.csv'))\n",
    "conditions = ['A', 'AV', 'V']\n",
    "groups = ['trained', 'control']\n",
    "chromas = ['hbo', 'hbr', 'hbdiff']\n",
    "\n",
    "df_final['ch_name'] = df_final['ch_name'].str.split(' ').str[0]\n",
    "fname = op.join(proc_path, f'205_1_001_long_hbo_final_raw.fif')\n",
    "use = mne.io.read_raw_fif(fname, preload=True)\n",
    "use.load_data()\n",
    "new_ch_names = {}\n",
    "seen_names = set()\n",
    "for ch_name in use.info['ch_names']:\n",
    "    new_name = ch_name.split(' ')[0]\n",
    "    if new_name not in seen_names:\n",
    "        new_ch_names[ch_name] = new_name\n",
    "        seen_names.add(new_name)\n",
    "\n",
    "use.rename_channels(new_ch_names)\n",
    "use = use.pick_channels(list(new_ch_names.values()))\n",
    "\n",
    "# Perform analysis for each group and Chroma\n",
    "for group in groups:\n",
    "    for chroma in chromas:\n",
    "        # Prepare figure for composite plots\n",
    "        fig, axes = plt.subplots(1, len(conditions), figsize=(15, 5))\n",
    "        for idx, condition in enumerate(conditions):\n",
    "            # Filter data for day 1 and day 3 for the specific group and Chroma\n",
    "            df_day1 = df_final.query(f\"group == '{group}' and Chroma == '{chroma}' and day == 1\").copy()\n",
    "            df_day3 = df_final.query(f\"group == '{group}' and Chroma == '{chroma}' and day == 3\").copy()\n",
    "\n",
    "            # Ensure ch_name and Condition columns are of the same data type\n",
    "            df_day1['ch_name'] = df_day1['ch_name'].astype(str)\n",
    "            df_day1['Condition'] = df_day1['Condition'].astype(str)\n",
    "            df_day3['ch_name'] = df_day3['ch_name'].astype(str)\n",
    "            df_day3['Condition'] = df_day3['Condition'].astype(str)\n",
    "\n",
    "            # Set index and sort\n",
    "            df_day1 = df_day1.set_index(['subject', 'group', 'ch_name', 'Condition', 'Chroma']).sort_index()\n",
    "            df_day3 = df_day3.set_index(['subject', 'group', 'ch_name', 'Condition', 'Chroma']).sort_index()\n",
    "\n",
    "            # Merge dataframes to align day 1 and day 3 data\n",
    "            df_merged = df_day1[['theta']].rename(columns={'theta': 'theta_day1'}).merge(\n",
    "                df_day3[['theta']].rename(columns={'theta': 'theta_day3'}),\n",
    "                left_index=True, right_index=True)\n",
    "\n",
    "            # Calculate the difference and z-score\n",
    "            df_merged['theta_diff'] = df_merged['theta_day3'] - df_merged['theta_day1']\n",
    "            df_merged['z'] = zscore(df_merged['theta_diff'])\n",
    "\n",
    "            # Perform paired t-test for each channel and condition across subjects\n",
    "            t_stats = []\n",
    "            p_values = []\n",
    "            ch_names = []\n",
    "            condition_list = []\n",
    "\n",
    "            for (ch_name, cond), group_df in df_merged.groupby(['ch_name', 'Condition']):\n",
    "                t_stat, p_value = ttest_rel(group_df['theta_day1'], group_df['theta_day3'])\n",
    "                t_stats.append(t_stat)\n",
    "                p_values.append(p_value)\n",
    "                ch_names.append(ch_name)\n",
    "                condition_list.append(cond)\n",
    "\n",
    "            # Create a results DataFrame\n",
    "            results_df = pd.DataFrame({\n",
    "                'ch_name': ch_names,\n",
    "                'Condition': condition_list,\n",
    "                't_stat': t_stats,\n",
    "                'p_value': p_values\n",
    "            })\n",
    "\n",
    "            # Combine with z-score data\n",
    "            z_scores = df_merged.groupby(['ch_name', 'Condition'])['z'].mean().reset_index()\n",
    "            \n",
    "            # Ensure consistent data types before merging\n",
    "            z_scores['ch_name'] = z_scores['ch_name'].astype(str)\n",
    "            z_scores['Condition'] = z_scores['Condition'].astype(str)\n",
    "            results_df['ch_name'] = results_df['ch_name'].astype(str)\n",
    "            results_df['Condition'] = results_df['Condition'].astype(str)\n",
    "\n",
    "            results_df = results_df.merge(z_scores, on=['ch_name', 'Condition'])\n",
    "\n",
    "            # Correct for multiple comparisons\n",
    "            print(f'Correcting for {len(results_df[\"p_value\"])} comparisons using FDR')\n",
    "            _, results_df['P_fdr'] = mne.stats.fdr_correction(results_df['p_value'], method='indep')\n",
    "            results_df['SIG'] = results_df['P_fdr'] < 0.05\n",
    "            \n",
    "            # Print significant results\n",
    "            significant_results = results_df.loc[results_df.SIG == True]\n",
    "            print(significant_results)\n",
    "\n",
    "            # Prepare data for brain plots\n",
    "            ch_of_interest = use.pick_channels([ch_name for ch_name in use.info['ch_names']])\n",
    "            info_of_interest = ch_of_interest.info\n",
    "\n",
    "            zs = {}\n",
    "            condition_data = results_df[(results_df['Condition'] == condition)]\n",
    "                        \n",
    "            zs[condition] = np.array([\n",
    "                condition_data.loc[(condition_data['ch_name'] == ch_name), 'z'].values[0]\n",
    "                if not condition_data.loc[(condition_data['ch_name'] == ch_name), 'z'].empty and condition_data.loc[(condition_data['ch_name'] == ch_name), 'p_value'].values[0] < 0.05\n",
    "                else 0\n",
    "                for ch_name in condition_data['ch_name']\n",
    "            ])\n",
    "            \n",
    "            # Create an EvokedArray for each condition\n",
    "            evoked = mne.EvokedArray(zs[condition][:, np.newaxis], info_of_interest)\n",
    "            picks = np.arange(len(info_of_interest['ch_names']))\n",
    "\n",
    "            stc = mne.stc_near_sensors(\n",
    "                evoked, trans='fsaverage', subject='fsaverage', mode='weighted',\n",
    "                distance=0.02, project=True, picks=picks, subjects_dir=subjects_dir)\n",
    "\n",
    "            # Plot the brain and capture the image in-memory\n",
    "            brain = stc.plot(hemi='both', views=['lat', 'frontal', 'lat'],\n",
    "                             cortex='low_contrast', time_viewer=False, show_traces=False,\n",
    "                             surface='pial', smoothing_steps=0, size=(1200, 400),\n",
    "                             clim=dict(kind='value', pos_lims=[0, 0.75, 1.5]),\n",
    "                             colormap='RdBu_r', view_layout='horizontal',\n",
    "                             colorbar=(0, 1), time_label='', background='w',\n",
    "                             brain_kwargs=dict(units='m'),\n",
    "                             add_data_kwargs=dict(colorbar_kwargs=dict(\n",
    "                                 title_font_size=16, label_font_size=12, n_labels=5,\n",
    "                                 title='z score')), subjects_dir=subjects_dir)\n",
    "            brain.show_view('lat', hemi='lh', row=0, col=0)\n",
    "            brain.show_view(azimuth=270, elevation=90, row=0, col=1)\n",
    "            brain.show_view('lat', hemi='rh', row=0, col=2)\n",
    "\n",
    "            # Capture the plot as an image in memory\n",
    "            screenshot = brain.screenshot(time_viewer=False)\n",
    "            brain.close()\n",
    "\n",
    "            # Display the image in the composite figure\n",
    "            ax = axes[idx]\n",
    "            ax.imshow(screenshot)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f'{group.capitalize()} - Condition {condition} ({chroma})', fontsize=18)\n",
    "\n",
    "            del df_day1, df_day3, df_merged, t_stats, p_values, ch_names, condition_list, results_df, z_scores\n",
    "            gc.collect()  #\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.savefig(op.join(results_path, f'{group}_{chroma}_composite_brain_plots_insig.png'))\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "        del fig, axes, ch_of_interest, info_of_interest, evoked, stc, brain, screenshot\n",
    "        gc.collect()  #\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Plot topographic maps of significant models\n",
    "\n",
    "for group in groups:\n",
    "    df_r2 = pd.read_csv(op.join(results_path, f'{group}_fnirs-behavior-models_{group}.csv'))\n",
    "    df_filtered = df_r2[(df_r2['P-value Corrected'] < 0.05)]\n",
    "    ch_names = df_filtered['Channel'].values \n",
    "    info = use.copy().pick_types(fnirs='hbo', exclude=())\n",
    "    info_picked = info.pick_channels(ch_names)\n",
    "    fig = mne.viz.plot_sensors(info_picked.info, kind='topomap', show_names=True, pointsize=100, linewidth=0]\n",
    "    plt.savefig(op.join(results_path, f'{group}_sig-p-corr_topomap.png'))\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
