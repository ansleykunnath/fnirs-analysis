{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=29\n"
     ]
    }
   ],
   "source": [
    "# Set up the environment and define functions\n",
    "\n",
    "# Import packages\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as op\n",
    "import time\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import mne\n",
    "from mne.preprocessing.nirs import tddr\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix  \n",
    "from mne_nirs.channels import get_long_channels\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.stats import ttest_rel, zscore\n",
    "import mne_nirs\n",
    "\n",
    "mne.viz.set_browser_backend('matplotlib')\n",
    "\n",
    "######### Set these variables as appropriate:\n",
    "raw_path = '../../data'\n",
    "proc_path = '../../processed'\n",
    "results_path = '../../results'\n",
    "subjects_dir = '../../subjects'\n",
    "subject_group_mapping = pd.read_csv('../../subject_group_mapping.csv')\n",
    "behavior_results_path = '../../fnirs-behavior-results'\n",
    "behavior_file = '../../behavior_diff_data.csv'\n",
    "output_suffix = \"bad50\" # used for all file names that are created\n",
    "\n",
    "# Create the subject to group mapping dictionary\n",
    "subject_group_mapping = subject_group_mapping.dropna(subset=['Subject'])  \n",
    "subject_group_mapping['Subject'] = subject_group_mapping['Subject'].astype(int) \n",
    "subject_to_group = dict(zip(subject_group_mapping['Subject'], subject_group_mapping['Group']))\n",
    "subjects = subject_group_mapping['Subject'].astype(str).tolist()\n",
    "\n",
    "sfreq = 4.807692\n",
    "conditions = ('A', 'V', 'AV', 'W')\n",
    "groups = ('trained', 'control')\n",
    "days = ('1', '3')\n",
    "runs = (1, 2)\n",
    "duration = 1.8\n",
    "design = 'event'\n",
    "filt_kwargs = dict(\n",
    "    l_freq=0.01, l_trans_bandwidth=0.01,\n",
    "    h_freq=0.2, h_trans_bandwidth=0.01)\n",
    "n_jobs = 4  # for GLM\n",
    "\n",
    "os.makedirs(proc_path, exist_ok=True)\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "os.makedirs(subjects_dir, exist_ok=True)\n",
    "# mne.datasets.fetch_fsaverage(subjects_dir=subjects_dir, verbose=True)  # Only need to run once\n",
    "\n",
    "use = None\n",
    "all_sci = list()\n",
    "plt.rcParams['axes.titlesize'] = 8\n",
    "plt.rcParams['axes.labelsize'] = 8\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "\n",
    "# Prep making bad channels report\n",
    "bad_channels_filename = op.join(results_path, f'bad_channels_report_{output_suffix}.csv')\n",
    "\n",
    "with open(bad_channels_filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Subject', 'Day', 'Run', 'Percent Bad'])\n",
    "\n",
    "def add_bad_channel_entry(subject, day, run, percentage_bad):\n",
    "    with open(bad_channels_filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([subject, day, run, f'{percentage_bad:.2f}%'])\n",
    "\n",
    "def normalize_channel_names(channels_set):\n",
    "    return {name.split()[0] for name in channels_set}\n",
    "\n",
    "# Sanity check for subjects\n",
    "subjects_check = {int(subject) for subject in subjects}\n",
    "subject_to_group_check = set(subject_to_group.keys())\n",
    "if subjects_check == subject_to_group_check:\n",
    "    print(\"N=\" + str(len(subjects)))\n",
    "    del subjects_check, subject_to_group_check\n",
    "else:\n",
    "    print(\"Error loading subject info\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for preprocessing\n",
    "\n",
    "def preprocess_fnirs_data(raw_intensity, proc_path, base):\n",
    "    # 1. Convert to optical density:\n",
    "    print(f'    Analyzing {base}')\n",
    "    raw_od = mne.preprocessing.nirs.optical_density(raw_intensity, verbose='error')\n",
    "\n",
    "    # 2. Identify bad channels based on flat signal and scalp coupling index:\n",
    "    peaks = np.ptp(raw_od.get_data('fnirs'), axis=-1)\n",
    "    flat_names = [raw_od.ch_names[f].split(' ')[0] for f in np.where(peaks < 0.001)[0]]\n",
    "    sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "    sci_mask = (sci < 0.25)\n",
    "    got = np.where(sci_mask)[0]\n",
    "    percentage_bad = (len(got) / len(raw_od.ch_names)) * 100\n",
    "    assert raw_od.info['bads'] == []\n",
    "    bads = set(raw_od.ch_names[pick] for pick in got)\n",
    "    bads = bads | set(ch_name for ch_name in raw_od.ch_names if ch_name.split(' ')[0] in flat_names)\n",
    "    bads = sorted(bads)\n",
    "\n",
    "    # 3. Apply temporal derivative distribution repair (TDDR), bandpass filter, apply bad channels:\n",
    "    raw_tddr = tddr(raw_od)\n",
    "    raw_tddr_bp = raw_tddr.copy().filter(**filt_kwargs)\n",
    "    raw_tddr_bp.info['bads'] = bads\n",
    "\n",
    "    # 5. Short channel regression (if present): \n",
    "    try:\n",
    "        raw_tddr_bp = mne_nirs.signal_enhancement.short_channel_regression(raw_tddr_bp)\n",
    "    except:\n",
    "        print(f\"No short channels found for {base}.\")\n",
    "\n",
    "    # 6. Convert to hemoglobin concentration:\n",
    "    raw_h = mne.preprocessing.nirs.beer_lambert_law(raw_tddr_bp, 6.)\n",
    "\n",
    "    # 7. Normalize channel names and verify bad channels\n",
    "    h_bads = [ch_name for ch_name in raw_h.ch_names if ch_name.split(' ')[0] in set(bad.split(' ')[0] for bad in bads)]\n",
    "    raw_h.info['bads'] = h_bads\n",
    "    raw_h.info._check_consistency()\n",
    "\n",
    "    # 8. Select long channels and verify that the signal is not flat:\n",
    "    raw_h = get_long_channels(raw_h)\n",
    "    picks = mne.pick_types(raw_h.info, fnirs=True)\n",
    "    peaks = np.ptp(raw_h.get_data(picks), axis=-1)\n",
    "    assert (peaks > 1e-9).all()\n",
    "\n",
    "    # 9. Interpolate bad channels\n",
    "    raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n",
    "    raw_h_interp.save(op.join(proc_path, f'{subject}_{day}_{run:03d}_long_hbo_{output_suffix}_raw.fif'), overwrite=True)\n",
    "    assert len(raw_h.ch_names) == len(raw_h_interp.ch_names)\n",
    "\n",
    "    return raw_h_interp, percentage_bad, bads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../data/Day1/201_1/2023-09-21_001\n",
      "    Analyzing 201_1_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 201_1_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_1_001_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_1_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/201_1/2023-09-21_002\n",
      "    Analyzing 201_1_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No short channels found for 201_1_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_1_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_1_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/201_3/2023-09-23_001\n",
      "    Analyzing 201_3_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 201_3_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_3_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_3_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/201_3/2023-09-23_002\n",
      "    Analyzing 201_3_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 201_3_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_3_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_3_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/202_1/2023-10-03_001\n",
      "    Analyzing 202_1_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 202_1_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_1_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_1_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/202_1/2023-10-03_002\n",
      "    Analyzing 202_1_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 202_1_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_1_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_1_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/202_3/2023-10-05_001\n",
      "    Analyzing 202_3_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 202_3_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_3_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_3_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/202_3/2023-10-05_002\n",
      "    Analyzing 202_3_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 202_3_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_3_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_3_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/203_1/2023-10-06_001\n",
      "    Analyzing 203_1_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No short channels found for 203_1_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_1_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_1_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/203_1/2023-10-06_002\n",
      "    Analyzing 203_1_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 203_1_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_1_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_1_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/203_3/2023-10-08_001\n",
      "    Analyzing 203_3_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 203_3_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_3_001_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_3_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/203_3/2023-10-08_002\n",
      "    Analyzing 203_3_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 203_3_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_3_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_3_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/204_1/2023-10-07_001\n",
      "    Analyzing 204_1_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 204_1_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_1_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_1_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/204_1/2023-10-07_002\n",
      "    Analyzing 204_1_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 204_1_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_1_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_1_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/204_3/2023-10-09_001\n",
      "    Analyzing 204_3_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 204_3_001_long.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_3_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_3_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/204_3/2023-10-09_002\n",
      "    Analyzing 204_3_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 204_3_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_3_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_3_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/205_1/2023-10-13_001\n",
      "    Analyzing 205_1_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 205_1_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_1_001_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_1_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/205_1/2023-10-13_002\n",
      "    Analyzing 205_1_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 205_1_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_1_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_1_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/205_3/2023-10-15_001\n",
      "    Analyzing 205_3_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 205_3_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_3_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_3_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/205_3/2023-10-15_002\n",
      "    Analyzing 205_3_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 205_3_002_long.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_3_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_3_002_long_hbo_bad50_raw.fif\n",
      "[done]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../data/Day1/206_1/2023-10-14_001\n",
      "    Analyzing 206_1_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 206_1_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_1_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_1_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/206_1/2023-10-14_002\n",
      "    Analyzing 206_1_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 206_1_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_1_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_1_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/206_3/2023-10-16_001\n",
      "    Analyzing 206_3_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 206_3_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_3_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_3_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/206_3/2023-10-16_002\n",
      "    Analyzing 206_3_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 206_3_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_3_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_3_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/207_1/2023-10-17_001\n",
      "    Analyzing 207_1_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 207_1_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_1_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_1_001_long_hbo_bad50_raw.fif\n",
      "[done]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../data/Day1/207_1/2023-10-17_002\n",
      "    Analyzing 207_1_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 207_1_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_1_002_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_1_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/207_3/2023-10-19_001\n",
      "    Analyzing 207_3_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 207_3_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_3_001_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_3_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/207_3/2023-10-19_002\n",
      "    Analyzing 207_3_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 207_3_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_3_002_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_3_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/208_1/2023-10-23_001\n",
      "    Analyzing 208_1_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 208_1_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_1_001_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_1_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/208_1/2023-10-23_002\n",
      "    Analyzing 208_1_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 208_1_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_1_002_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_1_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/208_3/2023-10-25_001\n",
      "    Analyzing 208_3_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 208_3_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_3_001_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_3_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/208_3/2023-10-25_002\n",
      "    Analyzing 208_3_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 208_3_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_3_002_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_3_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/209_1/2023-10-26_001\n",
      "    Analyzing 209_1_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 209_1_001_long.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_1_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_1_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/209_1/2023-10-26_002\n",
      "    Analyzing 209_1_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 209_1_002_long.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_1_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_1_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/209_3/2023-10-28_001\n",
      "    Analyzing 209_3_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 209_3_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_3_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_3_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/209_3/2023-10-28_002\n",
      "    Analyzing 209_3_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 209_3_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_3_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_3_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/212_1/2023-11-07_001\n",
      "    Analyzing 212_1_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 212_1_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_1_001_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_1_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/212_1/2023-11-07_002\n",
      "    Analyzing 212_1_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 212_1_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_1_002_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_1_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/212_3/2023-11-09_001\n",
      "    Analyzing 212_3_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 212_3_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_3_001_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_3_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/212_3/2023-11-09_002\n",
      "    Analyzing 212_3_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 212_3_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_3_002_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_3_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/213_1/2023-12-04_001\n",
      "    Analyzing 213_1_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 213_1_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_1_001_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_1_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/213_1/2023-12-04_002\n",
      "    Analyzing 213_1_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 213_1_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_1_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_1_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/213_3/2023-12-06_001\n",
      "    Analyzing 213_3_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 213_3_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_3_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_3_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/213_3/2023-12-06_002\n",
      "    Analyzing 213_3_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 213_3_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_3_002_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_3_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/214_1/2023-11-04_001\n",
      "    Analyzing 214_1_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 214_1_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_1_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_1_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/214_1/2023-11-04_002\n",
      "    Analyzing 214_1_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 214_1_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_1_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_1_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/214_3/2023-11-06_001\n",
      "    Analyzing 214_3_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 214_3_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_3_001_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_3_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/214_3/2023-11-06_002\n",
      "    Analyzing 214_3_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 214_3_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 93.6 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_3_002_long_hbo_bad50_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_3_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/215_1/2023-12-11_001\n",
      "    Analyzing 215_1_001_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 215_1_001_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/215_1_001_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/215_1_001_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/215_1/2023-12-11_002\n",
      "    Analyzing 215_1_002_long\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.01 - 0.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.01\n",
      "- Lower transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.01 Hz)\n",
      "- Upper passband edge: 0.20 Hz\n",
      "- Upper transition bandwidth: 0.01 Hz (-6 dB cutoff frequency: 0.21 Hz)\n",
      "- Filter length: 1587 samples (330.096 s)\n",
      "\n",
      "No short channels found for 215_1_002_long.\n",
      "Setting channel interpolation method to {'fnirs': 'nearest'}.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/215_1_002_long_hbo_bad50_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_35859/308572688.py:46: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/215_1_002_long_hbo_bad50_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/215_3/2023-12-13_001\n",
      "    Analyzing 215_3_001_long\n"
     ]
    }
   ],
   "source": [
    "# Load participant data\n",
    "\n",
    "#subjects = ['223'] #testing\n",
    "\n",
    "for subject in subjects:\n",
    "    for day in days:\n",
    "        for run in runs:\n",
    "            group = subject_to_group.get(int(subject), \"unknown\")\n",
    "            root1 = f'Day{day}'\n",
    "            root2 = f'{subject}_{day}'\n",
    "            root3 = f'*-*-*_{run:03d}'\n",
    "            fname_base = op.join(raw_path, root1, root2, root3)\n",
    "            fname = glob.glob(fname_base)\n",
    "            base = f'{subject}_{day}_{run:03d}'\n",
    "            base_pr = base.ljust(20)\n",
    "            raw_intensity = mne.io.read_raw_nirx(fname[0])\n",
    "            raw_intensity, percentage_bad_long, bads_long = preprocess_fnirs_data(raw_intensity, proc_path, base + '_long')\n",
    "            add_bad_channel_entry(subject, day, run, percentage_bad_long)\n",
    "            del raw_intensity, percentage_bad_long, bads_long\n",
    "            gc.collect()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove subjects with >50% bad channels; SKIP\n",
    "\n",
    "bad_channels_df = pd.read_csv(bad_channels_filename)\n",
    "bad_channels_df['Percent Bad'] = bad_channels_df['Percent Bad'].str.rstrip('%').astype(float)\n",
    "average_bad_channels = bad_channels_df.groupby('Subject')['Percent Bad'].mean()\n",
    "\n",
    "# Find subjects with more than 30% bad channels\n",
    "bad_subjects = average_bad_channels[average_bad_channels > 50].index.tolist()\n",
    "print(\"Subjects with more than 50% bad channels:\", bad_subjects)\n",
    "\n",
    "# Initialize counters for each group\n",
    "removed_trained = 0\n",
    "removed_control = 0\n",
    "remaining_trained = 0\n",
    "remaining_control = 0\n",
    "\n",
    "# Count and remove the subjects\n",
    "for subject in bad_subjects:\n",
    "    subject_int = int(subject) \n",
    "    if subject_int in subject_to_group:\n",
    "        if subject_to_group[subject_int] == \"trained\":\n",
    "            removed_trained += 1\n",
    "        elif subject_to_group[subject_int] == \"control\":\n",
    "            removed_control += 1\n",
    "        subject_to_group.pop(subject_int, None)\n",
    "\n",
    "# Update the subjects list after counting the removed subjects\n",
    "subjects = [subject for subject in subjects if subject not in bad_subjects]\n",
    "for group in subject_to_group.values():\n",
    "    if group == \"trained\":\n",
    "        remaining_trained += 1\n",
    "    elif group == \"control\":\n",
    "        remaining_control += 1\n",
    "\n",
    "# Output the results\n",
    "print(\" \")\n",
    "print(f'Removed {removed_trained} trained subjects.')\n",
    "print(f'Removed {removed_control} control subjects.')\n",
    "print(\" \")\n",
    "print(f'Remaining trained subjects: {remaining_trained}')\n",
    "print(f'Remaining control subjects: {remaining_control}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean events and make design matrix\n",
    "\n",
    "def make_design(raw_h_long, design, subject=None, run=None, day=None, group=None):\n",
    "    annotations_to_remove = raw_h_long.annotations.description == '255.0'\n",
    "    raw_h_long.annotations.delete(annotations_to_remove)\n",
    "    events, _ = mne.events_from_annotations(raw_h_long)\n",
    "    \n",
    "    # Fix mis-codings\n",
    "    rows_to_remove = events[:, -1] == 1\n",
    "    events = events[~rows_to_remove]\n",
    "    if len(events) == 101:\n",
    "        events = events[1:]\n",
    "\n",
    "    n_times = len(raw_h_long.times)\n",
    "    stim = np.zeros((n_times, 4))\n",
    "    events[:, 2] -= 1\n",
    "    assert len(events) == 100, len(events)\n",
    "    want = [0] + [25] * 4\n",
    "    count = np.bincount(events[:, 2])\n",
    "    assert np.array_equal(count, want), count\n",
    "    assert events.shape == (100, 3), events.shape\n",
    "\n",
    "    if design == 'block':\n",
    "        events = events[0::5]\n",
    "        duration = 20.\n",
    "        assert np.array_equal(np.bincount(events[:, 2]), [0] + [5] * 4)\n",
    "    else:\n",
    "#        assert design == 'event'\n",
    "        assert len(events) == 100\n",
    "        duration = 1.8\n",
    "        assert events.shape == (100, 3)\n",
    "        events_r = events[:, 2].reshape(20, 5)\n",
    "        assert (events_r == events_r[:, :1]).all()\n",
    "        del events_r\n",
    "        \n",
    "    idx = (events[:, [0, 2]] - [0, 1]).T\n",
    "    assert np.in1d(idx[1], np.arange(len(conditions))).all()\n",
    "    stim[tuple(idx)] = 1\n",
    "    \n",
    "    n_block = int(np.ceil(duration * sfreq))\n",
    "    stim = signal.fftconvolve(stim, np.ones((n_block, 1)), axes=0)[:n_times]\n",
    "    dm_events = pd.DataFrame({\n",
    "        'trial_type': [conditions[ii] for ii in idx[1]],\n",
    "        'onset': idx[0] / raw_h_long.info['sfreq'],\n",
    "        'duration': n_block / raw_h_long.info['sfreq']})\n",
    "    dm = make_first_level_design_matrix(\n",
    "        raw_h_long.times, dm_events, hrf_model='glover',\n",
    "        drift_model='polynomial', drift_order=0)\n",
    "        \n",
    "    return stim, dm, events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the subject, day, and run to plot different waveforms; SKIP\n",
    "\n",
    "plot_subject = '223'\n",
    "plot_day = 1\n",
    "plot_run = 1\n",
    "\n",
    "fname2 = op.join(proc_path, f'{plot_subject}_{plot_day}_{plot_run:03d}_long_hbo_{output_suffix}_raw.fif')\n",
    "use = mne.io.read_raw_fif(fname2, preload=True)\n",
    "events, _ = mne.events_from_annotations(use)\n",
    "ch_names = [ch_name.rstrip(' hbo') for ch_name in use.ch_names]\n",
    "info = use.info\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(6., 3), constrained_layout=True)\n",
    "ax = axes[0]\n",
    "raw_h = use\n",
    "stim, dm, _ = make_design(raw_h, design)\n",
    "\n",
    "colors = dict(\n",
    "    A='#4477AA',  # blue\n",
    "    AV='#CCBB44',  # yellow\n",
    "    V='#EE7733',  # orange\n",
    "    W='#AA3377',  # purple\n",
    ")\n",
    "\n",
    "for ci, condition in enumerate(conditions):\n",
    "    color = colors[condition]\n",
    "    ax.fill_between(\n",
    "        raw_h.times, stim[:, ci], 0, edgecolor='none', facecolor='k',\n",
    "        alpha=0.5)\n",
    "    model = dm[conditions[ci]].to_numpy()\n",
    "    ax.plot(raw_h.times, model, ls='-', lw=1, color=color)\n",
    "    x = raw_h.times[np.where(model > 0)[0][0]]\n",
    "    ax.text(\n",
    "        x + 10, 1.1, condition, color=color, fontweight='bold', ha='center')\n",
    "ax.set(ylabel='Modeled\\noxyHb', xlabel='', xlim=raw_h.times[[0, -1]])\n",
    "\n",
    "# HbO/HbR\n",
    "ax = axes[1]\n",
    "picks = [pi for pi, ch_name in enumerate(raw_h.ch_names)\n",
    "         if 'S7_D19' in ch_name]\n",
    "colors = dict(hbo='r', hbr='b')\n",
    "ylim = np.array([-1, 1])\n",
    "for pi, pick in enumerate(picks):\n",
    "    color = colors[raw_h.ch_names[pick][-3:]]\n",
    "    data = raw_h.get_data(pick)[0] * 1e6\n",
    "    val = np.ptp(data)\n",
    "    assert val > 0.01\n",
    "    ax.plot(raw_h.times, data, color=color, lw=1.)\n",
    "ax.set(ylim=ylim, xlabel='Time (s)', ylabel='μM',\n",
    "       xlim=raw_h.times[[0, -1]])\n",
    "for ax in axes:\n",
    "    for key in ('top', 'right'):\n",
    "        ax.spines[key].set_visible(False)\n",
    "plt.savefig(op.join(results_path, f'figure_1_{output_suffix}.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GLM analysis and epoching\n",
    "\n",
    "subj_cha_list = []\n",
    "for subject in subjects:\n",
    "    group = subject_to_group.get(int(subject), \"unknown\")\n",
    "    for day in days:\n",
    "        for run in runs:\n",
    "            fname_long = op.join(proc_path, f'{subject}_{day}_{run:03d}_long_hbo_{output_suffix}_raw.fif')\n",
    "            raw_h_long = mne.io.read_raw_fif(fname_long)\n",
    "            _, dm, _ = make_design(raw_h_long, design, subject, run, day, group)\n",
    "            glm_est = mne_nirs.statistics.run_glm(\n",
    "                raw_h_long, dm, noise_model='ols', n_jobs=n_jobs)\n",
    "            cha = glm_est.to_dataframe()\n",
    "            cha['subject'] = subject\n",
    "            cha['run'] = run\n",
    "            cha['day'] = day\n",
    "            cha['group'] = group\n",
    "            subj_cha_list.append(cha)\n",
    "            del raw_h_long, dm, glm_est, cha\n",
    "            gc.collect()  #\n",
    "        print(f'***Finished processing subject {subject} day {day}.')\n",
    "\n",
    "df_cha = pd.concat(subj_cha_list, ignore_index=True)\n",
    "df_cha.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block averages\n",
    "\n",
    "event_id = {condition: ci for ci, condition in enumerate(conditions, 1)}\n",
    "evokeds = {condition: dict() for condition in conditions}\n",
    "for day in days:\n",
    "    for subject in subjects:\n",
    "        fname = op.join(proc_path, f'{subject}_{day}_{output_suffix}-ave.fif')\n",
    "        tmin, tmax = -2, 38\n",
    "        baseline = (None, 0)\n",
    "        t0 = time.time()\n",
    "        print(f'Creating block average for {subject} day {day}... ', end='')\n",
    "        raws = list()\n",
    "        events = list()\n",
    "        for run in runs:\n",
    "            fname2 = op.join(proc_path, f'{subject}_{day}_{run:03d}_long_hbo_{output_suffix}_raw.fif')\n",
    "            raw_h = mne.io.read_raw_fif(fname2)\n",
    "            events.append(make_design(raw_h, None, 'block', subject, run)[2])\n",
    "            raws.append(raw_h)\n",
    "        bads = sorted(set(sum((r.info['bads'] for r in raws), [])))\n",
    "        for r in raws:\n",
    "            r.info['bads'] = bads\n",
    "        raw_h, events = mne.concatenate_raws(raws, events_list=events)\n",
    "        epochs = mne.Epochs(raw_h, events, event_id, tmin=tmin, tmax=tmax,\n",
    "                            baseline=baseline)\n",
    "        this_ev = [epochs[condition].average() for condition in conditions]\n",
    "        assert all(ev.nave > 0 for ev in this_ev)\n",
    "        mne.write_evokeds(fname, this_ev, overwrite=True)\n",
    "        print(f'{time.time() - t0:0.1f} sec')\n",
    "        for condition in conditions:\n",
    "            evokeds[condition][subject] = mne.read_evokeds(fname, condition)\n",
    "        print(f'Done for {group} {subject} day {day} run {run:03d}... ', end='')\n",
    "        del raws, events, raw_h, epochs, this_ev\n",
    "        gc.collect()  #\n",
    "\n",
    "# Mark bad channels\n",
    "bad = dict()\n",
    "bb = dict()\n",
    "\n",
    "for day in days:\n",
    "    for subject in subjects:\n",
    "        for run in runs:\n",
    "            fname2 = op.join(proc_path, f'{subject}_{day}_{run:03d}_long_hbo_{output_suffix}_raw.fif')\n",
    "            this_info = mne.io.read_info(fname2)\n",
    "            bad_channels = [idx - 1 for idx in sorted(\n",
    "                this_info['ch_names'].index(bad) + 1 for bad in this_info['bads'])]\n",
    "            valid_indices = np.arange(len(use.ch_names))\n",
    "            bb = [b for b in bad_channels if b in valid_indices]\n",
    "            bad[(subject, run, day)] = bb\n",
    "        assert np.in1d(bad[(subject, run, day)], np.arange(len(use.ch_names))).all()\n",
    "\n",
    "bad_combo = dict()\n",
    "for day in days:\n",
    "    for (subject, run, day), bb in bad.items():\n",
    "        bad_combo[subject] = sorted(set(bad_combo.get(subject, [])) | set(bb))\n",
    "bad = bad_combo\n",
    "\n",
    "start = len(df_cha)\n",
    "n_drop = 0\n",
    "for day in days:\n",
    "    for (subject, run, day), bb in bad.items():\n",
    "        if not len(bb):\n",
    "            continue\n",
    "        drop_names = [use.ch_names[b] for b in bb]\n",
    "        is_subject = (df_cha['subject'] == subject)\n",
    "        is_day = (df_cha['day'] == day)\n",
    "        drop = df_cha.index[\n",
    "            is_subject &\n",
    "            is_day &\n",
    "            np.in1d(df_cha['ch_name'], drop_names)]\n",
    "        n_drop += len(drop)\n",
    "        if len(drop):\n",
    "            print(f'Dropping {len(drop)} for {subject} day {day}')\n",
    "            df_cha.drop(drop, inplace=True)\n",
    "end = len(df_cha)\n",
    "assert n_drop == start - end, (n_drop, start - end)\n",
    "\n",
    "# Combine runs by averaging\n",
    "sorts = ['subject', 'ch_name', 'Chroma', 'Condition', 'group', 'day', 'run']\n",
    "df_cha.sort_values(sorts, inplace=True)\n",
    "theta = np.array(df_cha['theta']).reshape(-1, len(runs)).mean(-1)\n",
    "df_cha.drop(\n",
    "    [col for col in df_cha.columns if col not in sorts[:-1]], axis='columns',\n",
    "    inplace=True)\n",
    "df_cha.reset_index(drop=True, inplace=True)\n",
    "df_cha = df_cha[::len(runs)]\n",
    "df_cha.reset_index(drop=True, inplace=True)\n",
    "df_cha['theta'] = theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate HbDiff\n",
    "\n",
    "# Load the data\n",
    "df_cha_nolabels = df_cha.copy()\n",
    "df_cha_nolabels['ch_name'] = df_cha_nolabels['ch_name'].str[:-4]\n",
    "\n",
    "# Separate HbO and HbR\n",
    "df_hbo = df_cha_nolabels[df_cha_nolabels['Chroma'].str.endswith('hbo')].set_index(['subject', 'Condition', 'group', 'day', 'ch_name']).sort_index()\n",
    "df_hbr = df_cha_nolabels[df_cha_nolabels['Chroma'].str.endswith('hbr')].set_index(['subject', 'Condition', 'group', 'day', 'ch_name']).sort_index()\n",
    "\n",
    "# Compute the difference\n",
    "df_cha_diff_list = []\n",
    "for ch_name in df_hbo.index.get_level_values('ch_name').unique():\n",
    "    # Get aligned indices\n",
    "    df_hbo_ch = df_hbo.loc[(slice(None), slice(None), slice(None), slice(None), ch_name), :].sort_index()\n",
    "    df_hbr_ch = df_hbr.loc[(slice(None), slice(None), slice(None), slice(None), ch_name), :].sort_index()\n",
    "    \n",
    "    # Ensure df_hbo_ch and df_hbr_ch have the same length\n",
    "    common_index = df_hbo_ch.index.intersection(df_hbr_ch.index)\n",
    "    df_hbo_ch = df_hbo_ch.loc[common_index]\n",
    "    df_hbr_ch = df_hbr_ch.loc[common_index]\n",
    "    \n",
    "    # Calculate the difference\n",
    "    df_diff = df_hbo_ch[['theta']].sub(df_hbr_ch[['theta']])\n",
    "    \n",
    "    # Align df_cha_ch with df_diff\n",
    "    df_cha_ch = df_hbo_ch.reset_index()\n",
    "    df_cha_ch['theta'] = df_diff.values\n",
    "    df_cha_ch['Chroma'] = 'hbdiff'\n",
    "    df_cha_ch['ch_name'] = df_cha_ch['ch_name'] + ' hbdiff'\n",
    "    \n",
    "    if not df_cha_ch.empty:\n",
    "        df_cha_diff_list.append(df_cha_ch)\n",
    "\n",
    "df_cha_diff_concat = pd.concat(df_cha_diff_list, ignore_index=True)\n",
    "\n",
    "# Concatenate original df_cha with df_cha_diff_concat\n",
    "df_final = pd.concat([df_cha, df_cha_diff_concat], ignore_index=True)\n",
    "df_final.to_csv(op.join(results_path, f'df_combined_final_cha_{output_suffix}.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run correlational analyses below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ValueWarning) # type: ignore\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # type: ignore\n",
    "\n",
    "# Load the datasets\n",
    "behavior_file = '../../behavior_diff_data2.csv'\n",
    "behavior_df = pd.read_csv(behavior_file)\n",
    "behavior_df['subject'] = behavior_df['subject'].astype(str)\n",
    "df_final = pd.read_csv(op.join('../../results/df_combined_final_cha_allsubjects.csv'))\n",
    "theta_df = df_final.copy()\n",
    "theta_df['subject'] = theta_df['subject'].astype(str)\n",
    "results_path = '../../results/correlations'\n",
    "\n",
    "# Get the unique conditions\n",
    "conditions = ['A', 'AV', 'V']\n",
    "response_vars = ['AO_WR', 'AV_WR', 'TBW']\n",
    "chromas = ['hbo', 'hbr', 'hbdiff']\n",
    "days = [1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline theta vs. WR changes\n",
    "#output_suffix = theta_vs_behavior\n",
    "\n",
    "def perform_analysis(group, output_suffix):\n",
    "    # Initialize a list to store significant models\n",
    "    significant_models = []\n",
    "    \n",
    "    # Initialize a dictionary to store p-values and model data by condition, response variable, chroma, and day\n",
    "    all_p_values = {condition: {response_var: {chroma: {day: [] for day in days} for chroma in chromas} for response_var in response_vars} for condition in conditions}\n",
    "    all_model_data = {condition: {response_var: {chroma: {day: [] for day in days} for chroma in chromas} for response_var in response_vars} for condition in conditions}\n",
    "\n",
    "    # Track maximum R-squared value\n",
    "    max_r_squared = 0\n",
    "    max_r_squared_model = None\n",
    "\n",
    "    for day in days:\n",
    "        theta_df_filtered = theta_df[theta_df['day'] == day].copy()\n",
    "        theta_df_filtered['ch_name'] = theta_df_filtered['ch_name'].str.split(' ').str[0]\n",
    "\n",
    "        for chroma in chromas:\n",
    "            # Filter theta_df for the specific chroma\n",
    "            theta_dataset = theta_df_filtered[theta_df_filtered['Chroma'] == chroma].copy()\n",
    "            theta_dataset['ch_name'] = theta_dataset['ch_name'].str.split(' ').str[0]\n",
    "\n",
    "            # Collect all p-values and model data for each response variable and condition\n",
    "            for condition in conditions:\n",
    "                for response_var in response_vars:\n",
    "                    # Filter the dataset for the current condition\n",
    "                    theta_df_condition = theta_dataset[theta_dataset['Condition'] == condition]\n",
    "                    \n",
    "                    # Pivot the theta_df to have channel names as columns\n",
    "                    theta_pivot = theta_df_condition.pivot_table(index=['subject', 'group', 'Condition'], columns='ch_name', values='theta').reset_index()\n",
    "                    theta_pivot['subject'] = theta_pivot['subject'].astype(str)  # Ensure subject is string\n",
    "\n",
    "                    # Merge the datasets based on 'subject' and 'group'\n",
    "                    merged_df = pd.merge(theta_pivot, behavior_df[['subject', 'group', 'TBW', 'AO_WR', 'AV_WR', 'VO_WR', 'age', 'AO_WR_1', 'AV_WR_1', 'VO_WR_1', 'TBW_1']], on=['subject', 'group'])\n",
    "                    channels = theta_df_condition['ch_name'].unique()  # list of all channel names\n",
    "\n",
    "                    for channel in channels:\n",
    "                        df = merged_df[[channel, response_var, 'group']].dropna()  # drop rows with missing values\n",
    "                        if df.empty:\n",
    "                            continue  # Skip this channel if there is no data\n",
    "                        model = smf.ols(f\"{response_var} ~ {channel}\", df[df['group'] == group]).fit()\n",
    "                        r_sq = model.rsquared\n",
    "                        p_value_channel = model.pvalues[channel]  # p-value for the channel\n",
    "                        all_p_values[condition][response_var][chroma][day].append(p_value_channel)\n",
    "                        all_model_data[condition][response_var][chroma][day].append((condition, channel, response_var, model, r_sq, p_value_channel, df, chroma, day))\n",
    "\n",
    "                    p_values = all_p_values[condition][response_var][chroma][day]\n",
    "                    model_data = all_model_data[condition][response_var][chroma][day]\n",
    "                    \n",
    "                    if p_values:\n",
    "                        # Apply FDR correction\n",
    "                        rejected, p_values_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "                        \n",
    "                        # Filter results based on FDR corrected p-values\n",
    "                        for (condition, channel, response_var, model, r_sq, p_value, df, chroma, day), p_val_corr, reject in zip(model_data, p_values_corrected, rejected):\n",
    "                            if p_val_corr < 0.05:\n",
    "                                print(f\"Group: {group}, Day: {day}, Chroma: {chroma}, Condition: {condition}, Channel: {channel}, Outcome: {response_var}\\n   R-squared: {r_sq}, corrected p-value: {p_val_corr}\\n\")\n",
    "                                significant_models.append({\n",
    "                                    'Condition': condition,\n",
    "                                    'Channel': channel,\n",
    "                                    'Response Variable': response_var,\n",
    "                                    'R-squared': r_sq,\n",
    "                                    'P-value': p_value,\n",
    "                                    'P-value Corrected': p_val_corr,\n",
    "                                    'Model Summary': model.summary().as_text(),\n",
    "                                    'Chroma': chroma,\n",
    "                                    'Day': day\n",
    "                                })\n",
    "\n",
    "                                # Plot the significant results\n",
    "                                plt.figure(figsize=(8, 6))\n",
    "\n",
    "                                # Plot trained data\n",
    "                                trained_df = df[df['group'] == 'trained']\n",
    "                                if not trained_df.empty:\n",
    "                                    trained_model = smf.ols(f\"{response_var} ~ {channel}\", trained_df).fit()\n",
    "                                    sns.scatterplot(x=trained_df[channel], y=trained_df[response_var], label='Trained', color='#92b6f0', s=100)\n",
    "                                    sns.lineplot(x=trained_df[channel], y=trained_model.predict(trained_df), color='#92b6f0', linewidth=2)\n",
    "\n",
    "                                # Plot control data\n",
    "                                control_df = df[df['group'] == 'control']\n",
    "                                if not control_df.empty:\n",
    "                                    control_model = smf.ols(f\"{response_var} ~ {channel}\", control_df).fit()\n",
    "                                    sns.scatterplot(x=control_df[channel], y=control_df[response_var], label='Control', color='gray', s=100)\n",
    "                                    sns.lineplot(x=control_df[channel], y=control_model.predict(control_df), color='gray', linewidth=2)\n",
    "                                \n",
    "                                xlabel = ('[HbO] on Day ' + str(day) if chroma == 'hbo' else\n",
    "                                        '[HbR] on Day ' + str(day) if chroma == 'hbr' else\n",
    "                                        '[HbDiff] on Day ' + str(day) if chroma == 'hbdiff' else\n",
    "                                        f'{chroma.upper()} on Day ' + str(day))\n",
    "                                ylabel = ('Change in Auditory Word Recognition' if response_var == 'AO_WR' else\n",
    "                                        'Change in Audiovisual Word Recognition' if response_var == 'AV_WR' else\n",
    "                                        'Change in Visual Word Recognition' if response_var == 'VO_WR' else\n",
    "                                        f'Change in {response_var}')\n",
    "                                plt.legend(loc='upper right') \n",
    "                                plt.xlabel(xlabel, fontsize=16)\n",
    "                                plt.ylabel(ylabel, fontsize=16)\n",
    "                                plt.title(f'{ylabel} vs.\\nCortical Response to {condition} Speech ({channel})', fontsize=16)\n",
    "                                plt.savefig(op.join(results_path, f'{output_suffix}__{group}_day{day}_{chroma}_{condition}_{response_var}_{channel}_plot.png'))\n",
    "                                plt.close()\n",
    "                                \n",
    "    if significant_models:\n",
    "        significant_models_df = pd.DataFrame(significant_models).sort_values(by='R-squared', ascending=False)\n",
    "        significant_models_df.to_csv(op.join(results_path, f'{output_suffix}_{group}_models.csv'), index=False)\n",
    "    else:\n",
    "        print(f'No significant models found for {group} group')\n",
    "\n",
    "# Perform analysis for the trained group\n",
    "perform_analysis('trained', output_suffix)\n",
    "\n",
    "# Perform analysis for the control group\n",
    "perform_analysis('control', 'theta_vs_behavior')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes in theta vs. WR changes\n",
    "\n",
    "# Load the datasets\n",
    "theta_df_filtered = theta_df.pivot_table(index=['subject', 'group', 'Condition', 'Chroma', 'ch_name'], columns='day', values='theta').reset_index()\n",
    "theta_df_filtered['theta_diff'] = theta_df_filtered[3] - theta_df_filtered[1]\n",
    "#output_suffix = \"theta_diff_final\"\n",
    "\n",
    "def perform_analysis(group, output_suffix):\n",
    "    # Initialize a list to store significant models\n",
    "    theta_df_filtered['ch_name'] = theta_df_filtered['ch_name'].str.split(' ').str[0]\n",
    "    significant_models = []\n",
    "    all_p_values = {condition: {response_var: {chroma: [] for chroma in chromas} for response_var in response_vars} for condition in conditions}\n",
    "    all_model_data = {condition: {response_var: {chroma: [] for chroma in chromas} for response_var in response_vars} for condition in conditions}\n",
    "\n",
    "    for chroma in chromas:\n",
    "        # Filter theta_df for the specific chroma\n",
    "        theta_dataset = theta_df_filtered[theta_df_filtered['Chroma'] == chroma].copy()\n",
    "        theta_dataset['ch_name'] = theta_dataset['ch_name'].str.split(' ').str[0]\n",
    "\n",
    "        # Collect all p-values and model data for each response variable and condition\n",
    "        for condition in conditions:\n",
    "            for response_var in response_vars:\n",
    "                # Filter the dataset for the current condition\n",
    "                theta_df_condition = theta_dataset[theta_dataset['Condition'] == condition]\n",
    "                \n",
    "                # Pivot the theta_df to have channel names as columns\n",
    "                theta_pivot = theta_df_condition.pivot_table(index=['subject', 'group', 'Condition'], columns='ch_name', values='theta_diff').reset_index()\n",
    "                theta_pivot['subject'] = theta_pivot['subject'].astype(str)  # Ensure subject is string\n",
    "\n",
    "                # Merge the datasets based on 'subject' and 'group'\n",
    "                merged_df = pd.merge(theta_pivot, behavior_df[['subject', 'group', 'TBW', 'AO_WR', 'AV_WR', 'VO_WR', 'age', 'AO_WR_1', 'AV_WR_1', 'VO_WR_1', 'TBW_1']], on=['subject', 'group'])\n",
    "                channels = theta_df_condition['ch_name'].unique()  # list of all channel names\n",
    "\n",
    "                for channel in channels:\n",
    "                    df = merged_df[[channel, response_var, 'group']].dropna()  # drop rows with missing values\n",
    "                    if df.empty:\n",
    "                        continue  # Skip this channel if there is no data\n",
    "                    model = smf.ols(f\"{response_var} ~ {channel}\", df[df['group'] == group]).fit()\n",
    "                    r_sq = model.rsquared\n",
    "                    p_value_channel = model.pvalues[channel]  # p-value for the channel\n",
    "                    all_p_values[condition][response_var][chroma].append(p_value_channel)\n",
    "                    all_model_data[condition][response_var][chroma].append((condition, channel, response_var, model, r_sq, p_value_channel, df, chroma))\n",
    "\n",
    "                p_values = all_p_values[condition][response_var][chroma]\n",
    "                model_data = all_model_data[condition][response_var][chroma]\n",
    "                \n",
    "                if p_values:\n",
    "                    # Apply FDR correction\n",
    "                    rejected, p_values_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "                    \n",
    "                    # Filter results based on FDR corrected p-values\n",
    "                    for (condition, channel, response_var, model, r_sq, p_value, df, chroma), p_val_corr, reject in zip(model_data, p_values_corrected, rejected):\n",
    "                        if p_val_corr < 0.05:\n",
    "                            print(f\"Group: {group}, Chroma: {chroma}, Condition: {condition}, Channel: {channel}, Outcome: {response_var}\\n   R-squared: {r_sq}, p-value: {p_value}, corrected p-value: {p_val_corr}\\n\")\n",
    "                            significant_models.append({\n",
    "                                'Condition': condition,\n",
    "                                'Channel': channel,\n",
    "                                'Response Variable': response_var,\n",
    "                                'R-squared': r_sq,\n",
    "                                'P-value': p_value,\n",
    "                                'P-value Corrected': p_val_corr,\n",
    "                                'Model Summary': model.summary().as_text(),\n",
    "                                'Chroma': chroma,\n",
    "                            })\n",
    "                            # Plot the significant results\n",
    "                            plt.figure(figsize=(8, 6))\n",
    "\n",
    "                            # Plot trained data\n",
    "                            trained_df = df[df['group'] == 'trained']\n",
    "                            if not trained_df.empty:\n",
    "                                trained_model = smf.ols(f\"{response_var} ~ {channel}\", trained_df).fit()\n",
    "                                sns.scatterplot(x=trained_df[channel], y=trained_df[response_var], label='Trained', color='#92b6f0', s=100)\n",
    "                                sns.lineplot(x=trained_df[channel], y=trained_model.predict(trained_df), color='#92b6f0', linewidth=2)\n",
    "\n",
    "                            # Plot control data\n",
    "                            control_df = df[df['group'] == 'control']\n",
    "                            if not control_df.empty:\n",
    "                                control_model = smf.ols(f\"{response_var} ~ {channel}\", control_df).fit()\n",
    "                                sns.scatterplot(x=control_df[channel], y=control_df[response_var], label='Control', color='gray', s=100)\n",
    "                                sns.lineplot(x=control_df[channel], y=control_model.predict(control_df), color='gray', linewidth=2)\n",
    "                            \n",
    "                            xlabel = f'{chroma.upper()} Change (Day 3 - Day 1) on {channel}'\n",
    "                            ylabel = ('Change in Auditory Word Recognition' if response_var == 'AO_WR' else\n",
    "                                    'Change in Audiovisual Word Recognition' if response_var == 'AV_WR' else\n",
    "                                    'Change in Visual Word Recognition' if response_var == 'VO_WR' else\n",
    "                                    f'Change in {response_var}')\n",
    "                            plt.legend(loc='upper right') \n",
    "                            plt.xlabel(xlabel, fontsize=16)\n",
    "                            plt.ylabel(ylabel, fontsize=16)\n",
    "                            plt.title(f'{ylabel} vs.\\nCortical Response to {condition} Speech ({channel})', fontsize=16)\n",
    "                            plt.savefig(op.join(results_path, f'{output_suffix}__{group}_{chroma}_{condition}_{response_var}_{channel}_plot.png'))\n",
    "                            plt.close()\n",
    "                                \n",
    "    if significant_models:\n",
    "        significant_models_df = pd.DataFrame(significant_models).sort_values(by='R-squared', ascending=False)\n",
    "        csv_file = op.join(results_path, f'{output_suffix}_{group}_models.csv')\n",
    "        if not op.isfile(csv_file):\n",
    "            significant_models_df.to_csv(csv_file, index=False, mode='w')\n",
    "        else:\n",
    "            significant_models_df.to_csv(csv_file, index=False, mode='a', header=False)\n",
    "    else:\n",
    "        print(f'No significant models found for {group} group')\n",
    "\n",
    "\n",
    "# Perform analysis for the trained group\n",
    "perform_analysis('trained', output_suffix)\n",
    "\n",
    "# Perform analysis for the control group\n",
    "perform_analysis('control', output_suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run same analysis for baseline theta vs. change in theta values; SKIP\n",
    "\n",
    "# Pivot table to get both Day 1 theta and theta_diff (Day 3 - Day 1)\n",
    "theta_df_filtered = theta_df.pivot_table(index=['subject', 'group', 'Condition', 'Chroma', 'ch_name'], columns='day', values='theta').reset_index()\n",
    "theta_df_filtered['theta_diff'] = theta_df_filtered[3] - theta_df_filtered[1]  # Calculate theta_diff\n",
    "theta_df_filtered['theta_baseline'] = theta_df_filtered[1]  # Baseline theta (Day 1)\n",
    "\n",
    "# Specify the channels you want to analyze\n",
    "channels_to_analyze = ['S3_D4', 'S21_D10']  # Example channels\n",
    "\n",
    "# Get the unique conditions\n",
    "conditions = ['AV']\n",
    "response_vars = ['theta_diff']\n",
    "chromas = ['hbr']\n",
    "#output_suffix = 'theta_diff_vs_baseline'\n",
    "\n",
    "def perform_analysis(group, output_suffix, channels):\n",
    "    # Initialize a list to store significant models\n",
    "    theta_df_filtered['ch_name'] = theta_df_filtered['ch_name'].str.split(' ').str[0]\n",
    "    significant_models = []\n",
    "    \n",
    "    # Reset counters here\n",
    "    total_channels_analyzed = 0\n",
    "    significant_channels_count = 0\n",
    "    \n",
    "    for chroma in chromas:\n",
    "        all_p_values = []\n",
    "        model_data = []\n",
    "\n",
    "        for condition in conditions:\n",
    "            print(f\"\\nStarting analysis for condition: {condition}, chroma: {chroma}, group: {group}\\n\")\n",
    "            # Filter the dataset for the current condition and chroma\n",
    "            theta_df_condition = theta_df_filtered[(theta_df_filtered['Chroma'] == chroma) & (theta_df_filtered['Condition'] == condition)]\n",
    "            \n",
    "            # Filter the dataset to include only the specified channels\n",
    "            theta_df_condition = theta_df_condition[theta_df_condition['ch_name'].isin(channels)]\n",
    "\n",
    "            # Pivot the theta_df to have channel names as columns, keeping 'group' in the index\n",
    "            theta_pivot = theta_df_condition.pivot_table(index=['subject', 'group'], columns='ch_name', values=['theta_diff', 'theta_baseline'])\n",
    "            theta_pivot.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in theta_pivot.columns.values]  # Flatten the MultiIndex\n",
    "            \n",
    "            # Reset the index to make 'group' a column again\n",
    "            theta_pivot.reset_index(inplace=True)\n",
    "            \n",
    "            # Iterate over each channel\n",
    "            for channel in channels:                \n",
    "                # Ensure both baseline and diff values are available\n",
    "                baseline_column = f'theta_baseline_{channel}'\n",
    "                diff_column = f'theta_diff_{channel}'\n",
    "                \n",
    "                if baseline_column in theta_pivot.columns and diff_column in theta_pivot.columns:\n",
    "                    df = theta_pivot[[diff_column, baseline_column, 'group']].dropna()  # drop rows with missing values\n",
    "                    if df.empty:\n",
    "                        print(f\"  No data available for channel: {channel}, skipping.\")\n",
    "                        continue  # Skip this channel if there is no data\n",
    "\n",
    "                    total_channels_analyzed += 1  # Increment the total channels analyzed counter\n",
    "\n",
    "                    # Regression model: theta_diff ~ theta_baseline\n",
    "                    model = smf.ols(f\"{diff_column} ~ {baseline_column}\", df[df['group'] == group]).fit()\n",
    "                    r_sq = model.rsquared\n",
    "                    p_value_channel = model.pvalues[baseline_column]  # p-value for the channel\n",
    "\n",
    "                    # Collect p-values and models for FDR correction\n",
    "                    all_p_values.append(p_value_channel)\n",
    "                    model_data.append((condition, channel, r_sq, p_value_channel, model, chroma, baseline_column, diff_column, df))\n",
    "        \n",
    "        # Apply FDR correction\n",
    "        rejected, p_values_corrected, _, _ = multipletests(all_p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "        # Filter results based on FDR corrected p-values and plot only if significant\n",
    "        for i, (condition, channel, r_sq, p_value, model, chroma, baseline_column, diff_column, df) in enumerate(model_data):\n",
    "            if p_values_corrected[i] < 0.05:\n",
    "                print(f\"Channel: {channel}, {condition}, chroma: {chroma}, group: {group}\")\n",
    "                print(f\"   p-value: {p_value}, corrected p-value: {p_values_corrected[i]}, R-squared: {r_sq}\\n\")\n",
    "                significant_channels_count += 1  # Increment the significant channels counter\n",
    "                significant_models.append({\n",
    "                    'Condition': condition,\n",
    "                    'Channel': channel,\n",
    "                    'R-squared': r_sq,\n",
    "                    'P-value': p_value,\n",
    "                    'P-value Corrected': p_values_corrected[i],\n",
    "                    'Model Summary': model.summary().as_text(),\n",
    "                    'Chroma': chroma,\n",
    "                    'Group': group\n",
    "                })\n",
    "\n",
    "                # Plot the significant results\n",
    "                plt.figure(figsize=(8, 6))\n",
    "\n",
    "                # Plot trained data\n",
    "                trained_df = df[df['group'] == 'trained']\n",
    "                if not trained_df.empty:\n",
    "                    trained_model = smf.ols(f\"{diff_column} ~ {baseline_column}\", trained_df).fit()\n",
    "                    sns.scatterplot(x=trained_df[baseline_column], y=trained_df[diff_column], label='Trained', color='#92b6f0', s=100)\n",
    "                    sns.lineplot(x=trained_df[baseline_column], y=trained_model.predict(trained_df), color='#92b6f0', linewidth=2)\n",
    "\n",
    "                # Plot control data\n",
    "                control_df = df[df['group'] == 'control']\n",
    "                if not control_df.empty:\n",
    "                    control_model = smf.ols(f\"{diff_column} ~ {baseline_column}\", control_df).fit()\n",
    "                    sns.scatterplot(x=control_df[baseline_column], y=control_df[diff_column], label='Control', color='gray', s=100)\n",
    "                    sns.lineplot(x=control_df[baseline_column], y=control_model.predict(control_df), color='gray', linewidth=2)\n",
    "                \n",
    "                xlabel = f'Baseline {chroma.upper()} (Day 1) on {channel}'\n",
    "                ylabel = f'{chroma.upper()} Change (Day 3 - Day 1)'\n",
    "                plt.legend(loc='upper right') \n",
    "                plt.xlabel(xlabel, fontsize=16)\n",
    "                plt.ylabel(ylabel, fontsize=16)\n",
    "                plt.title(f'{ylabel} vs.\\nBaseline Cortical Response to {condition} Speech ({channel})', fontsize=16)\n",
    "                plt.savefig(op.join(results_path, f'{output_suffix}__{group}_{chroma}_{condition}_{channel}_plot.png'))\n",
    "                plt.close()\n",
    "\n",
    "    # Save significant models to CSV\n",
    "    if significant_models:\n",
    "        significant_models_df = pd.DataFrame(significant_models).sort_values(by='R-squared', ascending=False)\n",
    "        csv_file = op.join(results_path, f'{output_suffix}_{group}_models.csv')\n",
    "        #if not op.isfile(csv_file):\n",
    "        #    significant_models_df.to_csv(csv_file, index=False, mode='w')\n",
    "        #else:\n",
    "        significant_models_df.to_csv(csv_file, index=False, mode='a', header=False)\n",
    "    else:\n",
    "        print(f'No significant models found for group {group}')\n",
    "    \n",
    "    # Print the counter\n",
    "    print(f\"\\n{significant_channels_count} out of {total_channels_analyzed} channels were significantly correlated for group {group}.\")\n",
    "\n",
    "# Perform analysis for the trained group\n",
    "perform_analysis('trained', output_suffix, channels_to_analyze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run same analysis for day 1 vs day 3 theta values; SKIP\n",
    "\n",
    "# Load the datasets\n",
    "behavior_df = pd.read_csv(behavior_file)\n",
    "behavior_df['subject'] = behavior_df['subject'].astype(str)\n",
    "\n",
    "theta_df_filtered = df_final.copy()\n",
    "theta_df_filtered['subject'] = theta_df_filtered['subject'].astype(str)\n",
    "\n",
    "# Pivot table to get both Day 1 theta and theta_diff (Day 3 - Day 1)\n",
    "theta_df_filtered = theta_df_filtered.pivot_table(index=['subject', 'group', 'Condition', 'Chroma', 'ch_name'], columns='day', values='theta').reset_index()\n",
    "theta_df_filtered['theta_diff'] = theta_df_filtered['3'] - theta_df_filtered['1']  # Calculate theta_diff\n",
    "theta_df_filtered['theta_baseline'] = theta_df_filtered['1']  # Baseline theta (Day 1)\n",
    "\n",
    "# Specify the channels you want to analyze\n",
    "channels_to_analyze = ['S19_D4', 'S19_D6', 'S25_D14', 'S15_D14', 'S21_D10', 'S3_D4', 'S4_D3', 'S8_D18']  # Example channels\n",
    "\n",
    "# Get the unique conditions\n",
    "conditions = ['A', 'AV', 'V']\n",
    "chromas = ['hbo', 'hbr', 'hbdiff']\n",
    "\n",
    "def perform_analysis(group, output_suffix, channels):\n",
    "    # Initialize a list to store significant models\n",
    "    theta_df_filtered['ch_name'] = theta_df_filtered['ch_name'].str.split(' ').str[0]\n",
    "    significant_models = []\n",
    "    total_channels_analyzed = 0\n",
    "    significant_channels_count = 0\n",
    "\n",
    "    for chroma in chromas:\n",
    "        all_p_values = []\n",
    "        model_data = []\n",
    "\n",
    "        # Filter theta_df for the specific chroma\n",
    "        theta_dataset = theta_df_filtered[theta_df_filtered['Chroma'] == chroma].copy()\n",
    "        theta_dataset['ch_name'] = theta_dataset['ch_name'].str.split(' ').str[0]\n",
    "\n",
    "        # Collect all p-values and model data for each condition\n",
    "        for condition in conditions:\n",
    "            print(f\"\\nStarting analysis for condition: {condition}, chroma: {chroma}, group: {group}\\n\")\n",
    "            # Filter the dataset for the current condition\n",
    "            theta_df_condition = theta_dataset[theta_dataset['Condition'] == condition].copy()\n",
    "            \n",
    "            # Filter the dataset to include only the specified channels\n",
    "            theta_df_condition = theta_df_condition[theta_df_condition['ch_name'].isin(channels)]\n",
    "\n",
    "            # Pivot the theta_df to have channel names as columns, keeping 'group' in the index\n",
    "            theta_pivot = theta_df_condition.pivot_table(index=['subject', 'group'], columns='ch_name', values=['1', '3'])  # 1 for Day 1, 3 for Day 3\n",
    "            theta_pivot.columns = [f'{day}_{ch_name}' for day, ch_name in theta_pivot.columns]  # Simplify the column names\n",
    "            \n",
    "            # Reset the index to make 'group' a column again\n",
    "            theta_pivot.reset_index(inplace=True)\n",
    "            \n",
    "            # Iterate over each channel\n",
    "            for channel in channels:                \n",
    "                # Ensure both Day 1 and Day 3 values are available\n",
    "                day1_column = f'1_{channel}'\n",
    "                day3_column = f'3_{channel}'\n",
    "                \n",
    "                if day1_column in theta_pivot.columns and day3_column in theta_pivot.columns:\n",
    "                    df = theta_pivot[[day1_column, day3_column, 'group']].dropna()  # drop rows with missing values\n",
    "                    if df.empty:\n",
    "                        print(f\"  No data available for channel: {channel}, skipping.\")\n",
    "                        continue  # Skip this channel if there is no data\n",
    "\n",
    "                    total_channels_analyzed += 1  # Increment the total channels analyzed counter\n",
    "\n",
    "                    # Regression model: Day 3 ~ Day 1\n",
    "                    formula = f'Q(\"{day3_column}\") ~ Q(\"{day1_column}\")'\n",
    "                    model = smf.ols(formula, df[df['group'] == group]).fit()\n",
    "                    r_sq = model.rsquared\n",
    "                    p_value_channel = model.pvalues[f'Q(\"{day1_column}\")']  # p-value for the channel\n",
    "\n",
    "                    # Collecting all p-values and models\n",
    "                    all_p_values.append(p_value_channel)\n",
    "                    model_data.append((condition, channel, r_sq, p_value_channel, model, chroma))\n",
    "\n",
    "        if all_p_values:\n",
    "            # Apply FDR correction\n",
    "            rejected, p_values_corrected, _, _ = multipletests(all_p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "            # Filter results based on FDR corrected p-values and plot only if significant\n",
    "            for i, (condition, channel, r_sq, p_value, model, chroma) in enumerate(model_data):\n",
    "                if p_values_corrected[i] < 0.05:\n",
    "                    print(f\"Channel: {channel}, {condition}, chroma: {chroma}, group: {group}\\n   R-squared: {r_sq}, corrected p-value: {p_values_corrected[i]}\\n\")\n",
    "                    significant_channels_count += 1  # Increment the significant channels counter\n",
    "                    significant_models.append({\n",
    "                        'Condition': condition,\n",
    "                        'Channel': channel,\n",
    "                        'R-squared': r_sq,\n",
    "                        'P-value': p_value,\n",
    "                        'P-value Corrected': p_values_corrected[i],\n",
    "                        'Model Summary': model.summary().as_text(),\n",
    "                        'Chroma': chroma,\n",
    "                        'Group': group\n",
    "                    })\n",
    "\n",
    "                    # Plot the significant results\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "\n",
    "                    # Plot trained data\n",
    "                    trained_df = df[df['group'] == 'trained']\n",
    "                    if not trained_df.empty:\n",
    "                        trained_model = smf.ols(formula, trained_df).fit()\n",
    "                        sns.scatterplot(x=trained_df[day1_column], y=trained_df[day3_column], label='Trained', color='#92b6f0', s=100)\n",
    "                        sns.lineplot(x=trained_df[day1_column], y=trained_model.predict(trained_df), color='#92b6f0', linewidth=2)\n",
    "\n",
    "                    # Plot control data\n",
    "                    control_df = df[df['group'] == 'control']\n",
    "                    if not control_df.empty:\n",
    "                        control_model = smf.ols(formula, control_df).fit()\n",
    "                        sns.scatterplot(x=control_df[day1_column], y=control_df[day3_column], label='Control', color='gray', s=100)\n",
    "                        sns.lineplot(x=control_df[day1_column], y=control_model.predict(control_df), color='gray', linewidth=2)\n",
    "\n",
    "                    xlabel = f'{chroma.upper()} (Day 1) on {channel}'\n",
    "                    ylabel = f'{chroma.upper()} (Day 3)'\n",
    "                    plt.legend(loc='upper right') \n",
    "                    plt.xlabel(xlabel, fontsize=16)\n",
    "                    plt.ylabel(ylabel, fontsize=16)\n",
    "                    plt.title(f'{ylabel} vs.\\nCortical Response to {condition} Speech ({channel})', fontsize=16)\n",
    "                    plt.savefig(op.join(results_path, f'{output_suffix}__{group}_{chroma}_{condition}_{channel}_plot.png'))\n",
    "                    plt.close()\n",
    "\n",
    "    # Save significant models to CSV\n",
    "    if significant_models:\n",
    "        significant_models_df = pd.DataFrame(significant_models).sort_values(by='R-squared', ascending=False)\n",
    "        csv_file = op.join(results_path, f'{output_suffix}_{group}_models.csv')\n",
    "        if not op.isfile(csv_file):\n",
    "            significant_models_df.to_csv(csv_file, index=False, mode='w')\n",
    "        else:\n",
    "            significant_models_df.to_csv(csv_file, index=False, mode='a', header=False)\n",
    "    else:\n",
    "        print(f'No significant models found for group {group}')\n",
    "    \n",
    "    # Print the counter\n",
    "    print(f\"\\n{significant_channels_count} out of {total_channels_analyzed} channels were significantly correlated for group {group}.\")\n",
    "\n",
    "# Perform analysis for the trained group\n",
    "perform_analysis('trained', 'theta_day1_vs_day3', channels_to_analyze)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below is still in progress..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sensors of interest\n",
    "\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as op\n",
    "\n",
    "# Use the info from your existing data\n",
    "proc_path = '../../processed'\n",
    "fname = op.join(proc_path, f'205_1_001_long_hbo_final_raw.fif')\n",
    "use = mne.io.read_raw_fif(fname, preload=True)\n",
    "use.load_data()\n",
    "info = use.copy().info\n",
    "\n",
    "# Specify the names of the sensors you want to plot\n",
    "selected_sensors = ['S25_D14 hbo']\n",
    "valid_sensors = [sensor for sensor in selected_sensors if sensor in info['ch_names']]\n",
    "\n",
    "if not valid_sensors:\n",
    "    print(\"No valid sensors selected for plotting.\")\n",
    "else:\n",
    "    # Increase the font size for sensor names\n",
    "    plt.rcParams.update({'font.size': 20})  # Set the desired font size\n",
    "\n",
    "    # Pick only the selected channels\n",
    "    info_picked = mne.pick_info(info, mne.pick_channels(info['ch_names'], valid_sensors))\n",
    "    info_picked.rename_channels({ch: '  ' + ch for ch in info_picked['ch_names']})\n",
    "\n",
    "    # Plot the sensors manually with blue circles\n",
    "    fig = mne.viz.plot_sensors(info_picked, kind='topomap', show_names=False, pointsize=800, linewidth=0)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some waveforms\n",
    "\n",
    "import mne\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load subject group mapping\n",
    "subject_group_mapping = pd.read_csv('../../subject_group_mapping.csv')\n",
    "subject_group_mapping['Subject'] = subject_group_mapping['Subject'].astype(str).str.strip().str.split('.').str[0]\n",
    "subject_to_group = dict(zip(subject_group_mapping['Subject'], subject_group_mapping['Group']))\n",
    "\n",
    "# Ensure all subjects are in string format without decimals\n",
    "subjects = subject_group_mapping['Subject'].tolist()\n",
    "#subjects = [subject for subject in subjects if subject not in ['202', '203', '204', '206', '214', '221', '223', '226', '233']]\n",
    "\n",
    "# Define the subjects in the 'trained' group\n",
    "trained_subjects = [subject for subject in subjects if subject_to_group[subject] == 'control']\n",
    "\n",
    "# Specify the channel of interest\n",
    "channel_of_interest = 'S25_D14'\n",
    "condition = 'AV'\n",
    "\n",
    "# Initialize the dictionaries to store evoked data for averaging and plotting\n",
    "evoked_dict_day1 = {'HbO': [], 'HbR': []}\n",
    "evoked_dict_day3 = {'HbO': [], 'HbR': []}\n",
    "\n",
    "# Loop over subjects and load their evoked data\n",
    "for subject in trained_subjects:\n",
    "    for day in ['1', '3']:\n",
    "        # Define the filename\n",
    "        fname = op.join(proc_path, f'{subject}_{day}_final-ave.fif')\n",
    "        \n",
    "        # Load the evoked data\n",
    "        evokeds = mne.read_evokeds(fname, condition=condition, baseline=(None, 0), verbose=False)\n",
    "                \n",
    "        # Pick the HbO and HbR data for the channel of interest\n",
    "        evoked_hbo = evokeds.copy().pick([f'{channel_of_interest} hbo'])\n",
    "        evoked_hbr = evokeds.copy().pick([f'{channel_of_interest} hbr'])\n",
    "        \n",
    "        evoked_hbo.rename_channels(lambda x: x[:-4])\n",
    "        evoked_hbr.rename_channels(lambda x: x[:-4])\n",
    "        \n",
    "        # Check if both HbO and HbR data exist for this subject and day\n",
    "        if len(evoked_hbo.ch_names) == 1 and len(evoked_hbr.ch_names) == 1:\n",
    "            # Store in the appropriate evoked dictionary\n",
    "            if day == '1':\n",
    "                evoked_dict_day1['HbO'].append(evoked_hbo)\n",
    "                evoked_dict_day1['HbR'].append(evoked_hbr)\n",
    "            elif day == '3':\n",
    "                evoked_dict_day3['HbO'].append(evoked_hbo)\n",
    "                evoked_dict_day3['HbR'].append(evoked_hbr)\n",
    "\n",
    "# Compute grand average for Day 1 and Day 3\n",
    "grand_avg_day1_hbo = mne.grand_average(evoked_dict_day1['HbO'])\n",
    "grand_avg_day1_hbr = mne.grand_average(evoked_dict_day1['HbR'])\n",
    "grand_avg_day3_hbo = mne.grand_average(evoked_dict_day3['HbO'])\n",
    "grand_avg_day3_hbr = mne.grand_average(evoked_dict_day3['HbR'])\n",
    "\n",
    "# Prepare the grand average dictionary for plotting\n",
    "grand_avg_dict_day1 = {'HbO': grand_avg_day1_hbo, 'HbR': grand_avg_day1_hbr}\n",
    "grand_avg_dict_day3 = {'HbO': grand_avg_day3_hbo, 'HbR': grand_avg_day3_hbr}\n",
    "\n",
    "# Define the colors and styles\n",
    "color_dict_day1 = {'HbO': 'red', 'HbR': 'blue'}\n",
    "color_dict_day3 = {'HbO': 'red', 'HbR': 'blue'}\n",
    "styles_dict_day1 = {'HbO': {'linestyle': '--'}, 'HbR': {'linestyle': '--'}}\n",
    "styles_dict_day3 = {'HbO': {'linestyle': '-'}, 'HbR': {'linestyle': '-'}}\n",
    "\n",
    "# Plot for Day 1 using the grand average\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    grand_avg_dict_day1,\n",
    "    ci=0.95, colors=color_dict_day1, styles=styles_dict_day1,\n",
    "    title=f'{channel_of_interest} ({condition} Condition) - Day 1', show=True,\n",
    "    truncate_xaxis=(-2, 15),  # This line sets the x-axis limits\n",
    "    ylim=dict(hbo=(-0.10, 0.10))  # This line sets the y-axis limits\n",
    ")\n",
    "\n",
    "# Plot for Day 3 using the grand average\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    grand_avg_dict_day3,\n",
    "    ci=0.95, colors=color_dict_day3, styles=styles_dict_day3,\n",
    "    title=f'{channel_of_interest} ({condition} Condition) - Day 3', show=True,\n",
    "    truncate_xaxis=(-2, 15),  # This line sets the x-axis limits\n",
    "    ylim=dict(hbo=(-0.10, 0.10))  # This line sets the y-axis limits\n",
    ")\n",
    "\n",
    "# Initialize the dictionaries to store evoked data for plotting\n",
    "evoked_dict_day1 = {'HbO': [], 'HbR': []}\n",
    "evoked_dict_day3 = {'HbO': [], 'HbR': []}\n",
    "\n",
    "# Loop over subjects and load their evoked data\n",
    "for subject in trained_subjects:\n",
    "    for day in ['1', '3']:\n",
    "        # Define the filename\n",
    "        fname = op.join(proc_path, f'{subject}_{day}_final-ave.fif')\n",
    "        \n",
    "        # Load the evoked data\n",
    "        evokeds = mne.read_evokeds(fname, condition=condition, baseline=(None, 0), verbose=False)\n",
    "                \n",
    "        # Pick the HbO and HbR data for the channel of interest\n",
    "        evoked_hbo = evokeds.copy().pick([f'{channel_of_interest} hbo'])\n",
    "        evoked_hbr = evokeds.copy().pick([f'{channel_of_interest} hbr'])\n",
    "        \n",
    "        evoked_hbo.rename_channels(lambda x: x[:-4])\n",
    "        evoked_hbr.rename_channels(lambda x: x[:-4])\n",
    "        \n",
    "        # Check if both HbO and HbR data exist for this subject and day\n",
    "        if len(evoked_hbo.ch_names) == 1 and len(evoked_hbr.ch_names) == 1:\n",
    "            # Store in the appropriate evoked dictionary\n",
    "            if day == '1':\n",
    "                evoked_dict_day1['HbO'].append(evoked_hbo)\n",
    "                evoked_dict_day1['HbR'].append(evoked_hbr)\n",
    "            elif day == '3':\n",
    "                evoked_dict_day3['HbO'].append(evoked_hbo)\n",
    "                evoked_dict_day3['HbR'].append(evoked_hbr)\n",
    "\n",
    "# Check if all participants were included\n",
    "expected_subjects = len(trained_subjects)\n",
    "actual_subjects_day1 = len(evoked_dict_day1['HbO'])  # Same number for HbO and HbR\n",
    "actual_subjects_day3 = len(evoked_dict_day3['HbO'])  # Same number for HbO and HbR\n",
    "\n",
    "if actual_subjects_day1 != expected_subjects:\n",
    "    print(f\"Warning: Only {actual_subjects_day1} out of {expected_subjects} subjects were included in the Day 1 analysis.\")\n",
    "else:\n",
    "    print(\"All participants were included in the Day 1 analysis.\")\n",
    "\n",
    "if actual_subjects_day3 != expected_subjects:\n",
    "    print(f\"Warning: Only {actual_subjects_day3} out of {expected_subjects} subjects were included in the Day 3 analysis.\")\n",
    "else:\n",
    "    print(\"All participants were included in the Day 3 analysis.\")\n",
    "\n",
    "# Define the colors and styles\n",
    "color_dict_day1 = {'HbO': 'red', 'HbR': 'blue'}\n",
    "color_dict_day3 = {'HbO': 'red', 'HbR': 'blue'}\n",
    "styles_dict_day1 = {'HbO': {'linestyle': '--'}, 'HbR': {'linestyle': '--'}}\n",
    "styles_dict_day3 = {'HbO': {'linestyle': '-'}, 'HbR': {'linestyle': '-'}}\n",
    "\n",
    "# Plot for Day 1 using the individual evoked responses\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    {'HbO': evoked_dict_day1['HbO'], 'HbR': evoked_dict_day1['HbR']},\n",
    "    ci=0.95, colors=color_dict_day1, styles=styles_dict_day1,\n",
    "    title=f'{channel_of_interest} ({condition} Condition) - Day 1', show=True,\n",
    "    truncate_xaxis=(-2, 15),  # This line sets the x-axis limits\n",
    "    ylim=dict(hbo=(-0.10, 0.10))  # This line sets the y-axis limits\n",
    ")\n",
    "\n",
    "# Plot for Day 3 using the individual evoked responses\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    {'HbO': evoked_dict_day3['HbO'], 'HbR': evoked_dict_day3['HbR']},\n",
    "    ci=0.95, colors=color_dict_day3, styles=styles_dict_day3,\n",
    "    title=f'{channel_of_interest} ({condition} Condition) - Day 3', show=True,\n",
    "    truncate_xaxis=(-2, 15),  # This line sets the x-axis limits\n",
    "    ylim=dict(hbo=(-0.10, 0.10))  # This line sets the y-axis limits\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare thetas in 2 specific channels\n",
    "\n",
    "# Filter data for the specific channels you want to compare\n",
    "channels_to_analyze = ['S19_D6', 'S15_D14']\n",
    "\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ValueWarning) # type: ignore\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # type: ignore\n",
    "\n",
    "# Load the datasets\n",
    "behavior_df = pd.read_csv(behavior_file)\n",
    "behavior_df['subject'] = behavior_df['subject'].astype(str)\n",
    "\n",
    "theta_df_filtered = df_final.copy()\n",
    "theta_df_filtered['subject'] = theta_df_filtered['subject'].astype(str)\n",
    "\n",
    "# Pivot table to get both Day 1 theta and theta_diff (Day 3 - Day 1)\n",
    "theta_df_filtered = theta_df_filtered.pivot_table(index=['subject', 'group', 'Condition', 'Chroma', 'ch_name'], columns='day', values='theta').reset_index()\n",
    "theta_df_filtered['theta_diff'] = theta_df_filtered[3] - theta_df_filtered[1]  # Calculate theta_diff\n",
    "theta_df_filtered['theta_baseline'] = theta_df_filtered[1]  # Baseline theta (Day 1)\n",
    "theta_df_filtered = theta_df_filtered[theta_df_filtered['group'] != 'unknown']\n",
    "\n",
    "\n",
    "def perform_channel_comparison(group, output_suffix):\n",
    "    # Initialize a list to store significant models\n",
    "    theta_df_filtered['ch_name'] = theta_df_filtered['ch_name'].str.split(' ').str[0]\n",
    "    significant_models = []\n",
    "    \n",
    "    # Iterate over each condition and chroma\n",
    "    for chroma in chromas:\n",
    "        all_p_values = []\n",
    "        model_data = []\n",
    "\n",
    "        for condition in conditions:\n",
    "            print(f\"\\nStarting analysis for condition: {condition}, chroma: {chroma}, group: {group}\\n\")\n",
    "            # Filter the dataset for the current condition and chroma\n",
    "            theta_df_condition = theta_df_filtered[(theta_df_filtered['Chroma'] == chroma) & (theta_df_filtered['Condition'] == condition)]\n",
    "            \n",
    "            # Pivot the theta_df to have channel names as columns\n",
    "            theta_pivot = theta_df_condition.pivot_table(index=['subject', 'group'], columns='ch_name', values=['theta_diff', 'theta_baseline'])\n",
    "            theta_pivot.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in theta_pivot.columns.values]  # Flatten the MultiIndex\n",
    "            \n",
    "            # Reset the index to make 'group' a column again\n",
    "            theta_pivot.reset_index(inplace=True)\n",
    "\n",
    "            # Ensure both channels are in the pivoted dataframe\n",
    "            if f'theta_diff_S19_D6' in theta_pivot.columns and f'theta_diff_S15_D14' in theta_pivot.columns:\n",
    "                df = theta_pivot[[f'theta_diff_S19_D6', f'theta_diff_S15_D14', f'theta_baseline_S19_D6', f'theta_baseline_S15_D14', 'group']].dropna()\n",
    "                \n",
    "                if df.empty:\n",
    "                    print(\"  No data available for the selected channels, skipping.\")\n",
    "                    continue  # Skip this condition if there is no data\n",
    "\n",
    "                # Perform regression analysis for S19_D6 vs. S20_D10\n",
    "                model = smf.ols(f\"theta_diff_S15_D14 ~ theta_diff_S19_D6\", df[df['group'] == group]).fit()\n",
    "                r_sq = model.rsquared\n",
    "                p_value_channel = model.pvalues[f'theta_diff_S19_D6']\n",
    "\n",
    "                # Collect p-values and models for FDR correction\n",
    "                all_p_values.append(p_value_channel)\n",
    "                model_data.append((condition, 'S19_D6_vs_S15_D14', r_sq, p_value_channel, model, chroma, df))\n",
    "            else:\n",
    "                print(f\"  One or both channels S19_D6 and S15_D14 are missing for condition {condition}, skipping.\")\n",
    "                continue  # Skip this condition if there is no data\n",
    "\n",
    "        # Apply FDR correction\n",
    "        if len(all_p_values) > 0:\n",
    "            rejected, p_values_corrected, _, _ = multipletests(all_p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "        # Filter results based on FDR corrected p-values and plot only if significant\n",
    "        for i, (condition, comparison, r_sq, p_value, model, chroma, df) in enumerate(model_data):\n",
    "            if p_values_corrected[i] < 0.05:\n",
    "                print(f\"Comparison: {comparison}, {condition}, chroma: {chroma}, group: {group}\")\n",
    "                print(f\"   p-value: {p_value}, corrected p-value: {p_values_corrected[i]}, R-squared: {r_sq}\\n\")\n",
    "                \n",
    "                significant_models.append({\n",
    "                    'Condition': condition,\n",
    "                    'Comparison': comparison,\n",
    "                    'R-squared': r_sq,\n",
    "                    'P-value': p_value,\n",
    "                    'P-value Corrected': p_values_corrected[i],\n",
    "                    'Model Summary': model.summary().as_text(),\n",
    "                    'Chroma': chroma,\n",
    "                    'Group': group\n",
    "                })\n",
    "\n",
    "                # Plot the significant results\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.scatterplot(x=df[f'theta_diff_S19_D6'], y=df[f'theta_diff_S15_D14'], hue=df['group'], palette=['#92b6f0', 'gray'], s=100)\n",
    "                sns.lineplot(x=df[f'theta_diff_S19_D6'], y=model.predict(df), color='black', linewidth=2)\n",
    "\n",
    "                xlabel = f'{chroma.upper()} Change in S19_D6 (Day 3 - Day 1)'\n",
    "                ylabel = f'{chroma.upper()} Change in S15_D14 (Day 3 - Day 1)'\n",
    "                plt.xlabel(xlabel, fontsize=16)\n",
    "                plt.ylabel(ylabel, fontsize=16)\n",
    "                plt.title(f'Comparison of {xlabel} and {ylabel} in {condition} condition', fontsize=16)\n",
    "                plt.legend(title='Group', loc='upper right')\n",
    "                plt.savefig(op.join(results_path, f'{output_suffix}__{group}_{chroma}_{condition}_S19_D6_vs_S15_D14_plot.png'))\n",
    "                plt.close()\n",
    "\n",
    "    # Save significant models to CSV\n",
    "    if significant_models:\n",
    "        significant_models_df = pd.DataFrame(significant_models).sort_values(by='R-squared', ascending=False)\n",
    "        csv_file = op.join(results_path, f'{output_suffix}_{group}_models.csv')\n",
    "        if not op.isfile(csv_file):\n",
    "            significant_models_df.to_csv(csv_file, index=False, mode='w')\n",
    "        else:\n",
    "            significant_models_df.to_csv(csv_file, index=False, mode='a', header=False)\n",
    "    else:\n",
    "        print(f'No significant models found for group {group}')\n",
    "\n",
    "# Perform the comparison analysis for the trained group\n",
    "perform_channel_comparison('trained', 'S19_D6_vs_S15_D14_comparison')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run paired t-test over days and plot changes over time\n",
    "\n",
    "# Load the final combined dataframe\n",
    "df_final = pd.read_csv(op.join(results_path, f'df_combined_final_cha_{output_suffix}.csv'))\n",
    "conditions = ['A', 'AV', 'V']\n",
    "groups = ['trained', 'control']\n",
    "chromas = ['hbo', 'hbr', 'hbdiff']\n",
    "\n",
    "df_final['ch_name'] = df_final['ch_name'].str.split(' ').str[0]\n",
    "fname = op.join(proc_path, f'205_1_001_long_hbo_final_raw.fif')\n",
    "use = mne.io.read_raw_fif(fname, preload=True)\n",
    "use.load_data()\n",
    "new_ch_names = {}\n",
    "seen_names = set()\n",
    "for ch_name in use.info['ch_names']:\n",
    "    new_name = ch_name.split(' ')[0]\n",
    "    if new_name not in seen_names:\n",
    "        new_ch_names[ch_name] = new_name\n",
    "        seen_names.add(new_name)\n",
    "\n",
    "use.rename_channels(new_ch_names)\n",
    "use = use.pick_channels(list(new_ch_names.values()))\n",
    "\n",
    "# Perform analysis for each group and Chroma\n",
    "for group in groups:\n",
    "    for chroma in chromas:\n",
    "        # Prepare figure for composite plots\n",
    "        fig, axes = plt.subplots(1, len(conditions), figsize=(15, 5))\n",
    "        for idx, condition in enumerate(conditions):\n",
    "            # Filter data for day 1 and day 3 for the specific group and Chroma\n",
    "            df_day1 = df_final.query(f\"group == '{group}' and Chroma == '{chroma}' and day == 1\").copy()\n",
    "            df_day3 = df_final.query(f\"group == '{group}' and Chroma == '{chroma}' and day == 3\").copy()\n",
    "\n",
    "            # Ensure ch_name and Condition columns are of the same data type\n",
    "            df_day1['ch_name'] = df_day1['ch_name'].astype(str)\n",
    "            df_day1['Condition'] = df_day1['Condition'].astype(str)\n",
    "            df_day3['ch_name'] = df_day3['ch_name'].astype(str)\n",
    "            df_day3['Condition'] = df_day3['Condition'].astype(str)\n",
    "\n",
    "            # Set index and sort\n",
    "            df_day1 = df_day1.set_index(['subject', 'group', 'ch_name', 'Condition', 'Chroma']).sort_index()\n",
    "            df_day3 = df_day3.set_index(['subject', 'group', 'ch_name', 'Condition', 'Chroma']).sort_index()\n",
    "\n",
    "            # Merge dataframes to align day 1 and day 3 data\n",
    "            df_merged = df_day1[['theta']].rename(columns={'theta': 'theta_day1'}).merge(\n",
    "                df_day3[['theta']].rename(columns={'theta': 'theta_day3'}),\n",
    "                left_index=True, right_index=True)\n",
    "\n",
    "            # Calculate the difference and z-score\n",
    "            df_merged['theta_diff'] = df_merged['theta_day3'] - df_merged['theta_day1']\n",
    "            df_merged['z'] = zscore(df_merged['theta_diff'])\n",
    "\n",
    "            # Perform paired t-test for each channel and condition across subjects\n",
    "            t_stats = []\n",
    "            p_values = []\n",
    "            ch_names = []\n",
    "            condition_list = []\n",
    "\n",
    "            for (ch_name, cond), group_df in df_merged.groupby(['ch_name', 'Condition']):\n",
    "                t_stat, p_value = ttest_rel(group_df['theta_day1'], group_df['theta_day3'])\n",
    "                t_stats.append(t_stat)\n",
    "                p_values.append(p_value)\n",
    "                ch_names.append(ch_name)\n",
    "                condition_list.append(cond)\n",
    "\n",
    "            # Create a results DataFrame\n",
    "            results_df = pd.DataFrame({\n",
    "                'ch_name': ch_names,\n",
    "                'Condition': condition_list,\n",
    "                't_stat': t_stats,\n",
    "                'p_value': p_values\n",
    "            })\n",
    "\n",
    "            # Combine with z-score data\n",
    "            z_scores = df_merged.groupby(['ch_name', 'Condition'])['z'].mean().reset_index()\n",
    "            \n",
    "            # Ensure consistent data types before merging\n",
    "            z_scores['ch_name'] = z_scores['ch_name'].astype(str)\n",
    "            z_scores['Condition'] = z_scores['Condition'].astype(str)\n",
    "            results_df['ch_name'] = results_df['ch_name'].astype(str)\n",
    "            results_df['Condition'] = results_df['Condition'].astype(str)\n",
    "\n",
    "            results_df = results_df.merge(z_scores, on=['ch_name', 'Condition'])\n",
    "\n",
    "            # Correct for multiple comparisons\n",
    "            print(f'Correcting for {len(results_df[\"p_value\"])} comparisons using FDR')\n",
    "            _, results_df['P_fdr'] = mne.stats.fdr_correction(results_df['p_value'], method='indep')\n",
    "            results_df['SIG'] = results_df['P_fdr'] < 0.05\n",
    "            \n",
    "            # Print significant results\n",
    "            significant_results = results_df.loc[results_df.SIG == True]\n",
    "            print(significant_results)\n",
    "\n",
    "            # Prepare data for brain plots\n",
    "            ch_of_interest = use.pick_channels([ch_name for ch_name in use.info['ch_names']])\n",
    "            info_of_interest = ch_of_interest.info\n",
    "\n",
    "            zs = {}\n",
    "            condition_data = results_df[(results_df['Condition'] == condition)]\n",
    "                        \n",
    "            zs[condition] = np.array([\n",
    "                condition_data.loc[(condition_data['ch_name'] == ch_name), 'z'].values[0]\n",
    "\n",
    "                if not condition_data.loc[(condition_data['ch_name'] == ch_name), 'z'].empty and condition_data.loc[(condition_data['ch_name'] == ch_name), 'p_value'].values[0] < 0.05\n",
    "                else 0\n",
    "                for ch_name in condition_data['ch_name']\n",
    "            ])\n",
    "            \n",
    "            # Create an EvokedArray for each condition\n",
    "            evoked = mne.EvokedArray(zs[condition][:, np.newaxis], info_of_interest)\n",
    "            picks = np.arange(len(info_of_interest['ch_names']))\n",
    "\n",
    "            stc = mne.stc_near_sensors(\n",
    "                evoked, trans='fsaverage', subject='fsaverage', mode='weighted',\n",
    "                distance=0.02, project=True, picks=picks, subjects_dir=subjects_dir)\n",
    "\n",
    "            # Plot the brain and capture the image in-memory\n",
    "            brain = stc.plot(hemi='both', views=['lat', 'frontal', 'lat'],\n",
    "                             cortex='low_contrast', time_viewer=False, show_traces=False,\n",
    "                             surface='pial', smoothing_steps=0, size=(1200, 400),\n",
    "                             clim=dict(kind='value', pos_lims=[0, 0.5, 1.0]),\n",
    "                             colormap='RdBu_r', view_layout='horizontal',\n",
    "                             colorbar=(0, 1), time_label='', background='w',\n",
    "                             brain_kwargs=dict(units='m'),\n",
    "                             add_data_kwargs=dict(colorbar_kwargs=dict(\n",
    "                                 title_font_size=16, label_font_size=12, n_labels=5,\n",
    "                                 title='z score')), subjects_dir=subjects_dir)\n",
    "            brain.show_view('lat', hemi='lh', row=0, col=0)\n",
    "            brain.show_view(azimuth=270, elevation=90, row=0, col=1)\n",
    "            brain.show_view('lat', hemi='rh', row=0, col=2)\n",
    "\n",
    "            # Capture the plot as an image in memory\n",
    "            screenshot = brain.screenshot(time_viewer=False)\n",
    "            brain.close()\n",
    "\n",
    "            # Display the image in the composite figure\n",
    "            ax = axes[idx]\n",
    "            ax.imshow(screenshot)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f'{group.capitalize()} - Condition {condition} ({chroma})', fontsize=18)\n",
    "\n",
    "            del df_day1, df_day3, df_merged, t_stats, p_values, ch_names, condition_list, results_df, z_scores\n",
    "            gc.collect()  #\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.savefig(op.join(results_path, f'{group}_{chroma}_composite_brain_plots_FDRinsig.png'))\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "        del fig, axes, ch_of_interest, info_of_interest, evoked, stc, brain, screenshot\n",
    "        gc.collect()  #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Plot topographic maps of significant models\n",
    "\n",
    "for group in groups:\n",
    "    df_r2 = pd.read_csv(op.join(results_path, f'{group}_fnirs-behavior-models_{group}.csv'))\n",
    "    df_filtered = df_r2[(df_r2['P-value Corrected'] < 0.05)]\n",
    "    ch_names = df_filtered['Channel'].values \n",
    "    info = use.copy().pick_types(fnirs='hbo', exclude=())\n",
    "    info_picked = info.pick_channels(ch_names)\n",
    "    fig = mne.viz.plot_sensors(info_picked.info, kind='topomap', show_names=True, pointsize=100, linewidth=0]\n",
    "    plt.savefig(op.join(results_path, f'{group}_sig-p-corr_topomap.png'))\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ROI AND CHANNEL T-TESTS\n",
    "\n",
    "# Define ROIs and their corresponding channels\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as op\n",
    "import time\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import mne\n",
    "from mne.preprocessing.nirs import tddr\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix  \n",
    "from mne_nirs.channels import get_long_channels\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.stats import ttest_rel, zscore\n",
    "import mne_nirs\n",
    "\n",
    "mne.set_config('SUBJECTS_DIR', None)\n",
    "mne.set_config('SUBJECTS_DIR', '/Users/ansle/Documents/Github/fnirs-analysis/subjects', set_env=True)\n",
    "\n",
    "results_path = '../../results'\n",
    "proc_path = '../../processed'\n",
    "df_final = pd.read_csv(op.join(results_path, f'df_combined_final_cha_final.csv'))\n",
    "df_final['ch_name'] = df_final['ch_name'].str.split(' ').str[0]\n",
    "output_csv_path = op.join(results_path, 'correlations/roi_ttest_results.csv')\n",
    "\n",
    "# Define ROIs and their corresponding channels\n",
    "roi_channels = {\n",
    "    'STG': ['S10_D20', 'S19_D6', 'S12_D22', 'S21_D8', 'S12_D20', 'S12_D24', 'S21_D10', 'S21_D6', 'S8_D20'],\n",
    "    'V1': ['S25_D14', 'S16_D14', 'S15_D14', 'S17_D14', 'S16_D25', 'S25_D11', 'S25_D13', 'S15_D25', \n",
    "           'S16_D27', 'S25_D12', 'S15_D11', 'S16_D26'],\n",
    "    'V2': ['S16_D26', 'S15_D25', 'S25_D12', 'S15_D11', 'S25_D13', 'S16_D27', 'S25_D11', 'S16_D25', \n",
    "           'S17_D14', 'S15_D14', 'S16_D14', 'S25_D14', 'S13_D25', 'S17_D13', 'S24_D11', 'S17_D27'],\n",
    "    'V3': ['S24_D11', 'S14_D26', 'S13_D25', 'S23_D12', 'S18_D26', 'S26_D12', 'S24_D9', 'S13_D23', \n",
    "           'S23_D10', 'S25_D12', 'S15_D11', 'S24_D10', 'S15_D25', 'S14_D24', 'S13_D24', 'S16_D26'],\n",
    "    'AC': ['S21_D6'],\n",
    "    'MTG': ['S10_D18', 'S19_D4', 'S10_D22', 'S19_D8', 'S10_D20', 'S19_D6', 'S12_D22', 'S21_D8', \n",
    "            'S21_D10', 'S14_D22', 'S12_D24']\n",
    "}\n",
    "\n",
    "# Define conditions, groups, and chromas\n",
    "conditions = ['A', 'AV', 'V']\n",
    "groups = ['trained', 'control']\n",
    "chromas = ['hbo', 'hbr', 'hbdiff']\n",
    "\n",
    "\"\"\" \n",
    "# Prepare for output\n",
    "output_csv_path = op.join(results_path, 'correlations/roi_ttest_results.csv')\n",
    "all_results_df = pd.DataFrame()\n",
    "\n",
    "# Perform analysis for each group, chroma, and ROI\n",
    "for group in groups:\n",
    "    for chroma in chromas:\n",
    "        for roi_name, channels in roi_channels.items():\n",
    "            for condition in conditions:\n",
    "                # Filter data for day 1 and day 3 for the specific group, chroma, and ROI channels\n",
    "                df_day1 = df_final.query(f\"group == '{group}' and Chroma == '{chroma}' and day == 1 and Condition == '{condition}' and ch_name in {channels}\").copy()\n",
    "                df_day3 = df_final.query(f\"group == '{group}' and Chroma == '{chroma}' and day == 3 and Condition == '{condition}' and ch_name in {channels}\").copy()\n",
    "\n",
    "                # Ensure 'ch_name' and 'Condition' columns are of the same data type\n",
    "                df_day1['ch_name'] = df_day1['ch_name'].astype(str)\n",
    "                df_day1['Condition'] = df_day1['Condition'].astype(str)\n",
    "                df_day3['ch_name'] = df_day3['ch_name'].astype(str)\n",
    "                df_day3['Condition'] = df_day3['Condition'].astype(str)\n",
    "\n",
    "                # Set index and sort\n",
    "                df_day1 = df_day1.set_index(['subject', 'group', 'Condition', 'Chroma', 'ch_name']).sort_index()\n",
    "                df_day3 = df_day3.set_index(['subject', 'group', 'Condition', 'Chroma', 'ch_name']).sort_index()\n",
    "\n",
    "                # Merge day 1 and day 3 data\n",
    "                df_merged = df_day1[['theta']].rename(columns={'theta': 'theta_day1'}).merge(\n",
    "                    df_day3[['theta']].rename(columns={'theta': 'theta_day3'}),\n",
    "                    left_index=True, right_index=True)\n",
    "\n",
    "                # Average across channels in the ROI (for each subject, condition, and chroma)\n",
    "                df_merged = df_merged.groupby(['subject', 'Condition', 'Chroma']).agg({\n",
    "                    'theta_day1': 'mean',\n",
    "                    'theta_day3': 'mean'}).reset_index()\n",
    "\n",
    "                # Before performing the t-test, print some diagnostic information\n",
    "                print(f\"Analyzing ROI: {roi_name}, Condition: {condition}, Group: {group}, Chroma: {chroma}\")\n",
    "\n",
    "                # Perform paired t-test\n",
    "                t_stat, p_value = ttest_rel(df_merged['theta_day1'], df_merged['theta_day3'])\n",
    "\n",
    "                # Create a results DataFrame for this ROI and condition\n",
    "                results_df = pd.DataFrame({\n",
    "                    'ROI': [roi_name],\n",
    "                    'Group': [group],\n",
    "                    'Condition': [condition],\n",
    "                    'Chroma': [chroma],\n",
    "                    't_stat': [t_stat],\n",
    "                    'p_value': [p_value],\n",
    "                })\n",
    "                \n",
    "                # Only concatenate significant results\n",
    "                if p_value < 0.05:\n",
    "                    print(results_df) \n",
    "                    all_results_df = pd.concat([all_results_df, results_df], ignore_index=True)\n",
    "                    \n",
    "# Save the final results to a CSV\n",
    "all_results_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "\"\"\" \n",
    "\"\"\" \n",
    "output_csv_path = op.join(results_path, 'correlations/channel_ttest_results.csv')\n",
    "all_results_df = pd.DataFrame()\n",
    "results_list = []\n",
    "\n",
    "# Perform analysis for each group, chroma, and channel\n",
    "for group in groups:\n",
    "    for chroma in chromas:\n",
    "        for condition in conditions:\n",
    "            for channel in df_final['ch_name'].unique():\n",
    "                # Filter data for day 1 and day 3 for the specific group, chroma, condition, and channel\n",
    "                df_day1 = df_final.query(f\"group == '{group}' and Chroma == '{chroma}' and day == 1 and Condition == '{condition}' and ch_name == '{channel}'\").copy()\n",
    "                df_day3 = df_final.query(f\"group == '{group}' and Chroma == '{chroma}' and day == 3 and Condition == '{condition}' and ch_name == '{channel}'\").copy()\n",
    "\n",
    "                # Ensure 'ch_name' and 'Condition' columns are of the same data type\n",
    "                df_day1['ch_name'] = df_day1['ch_name'].astype(str)\n",
    "                df_day1['Condition'] = df_day1['Condition'].astype(str)\n",
    "                df_day3['ch_name'] = df_day3['ch_name'].astype(str)\n",
    "                df_day3['Condition'] = df_day3['Condition'].astype(str)\n",
    "\n",
    "                # Set index and sort by subject, group, condition, chroma, and channel\n",
    "                df_day1 = df_day1.set_index(['subject', 'group', 'Condition', 'Chroma', 'ch_name']).sort_index()\n",
    "                df_day3 = df_day3.set_index(['subject', 'group', 'Condition', 'Chroma', 'ch_name']).sort_index()\n",
    "\n",
    "                # Check if any data was retrieved\n",
    "                if df_day1.empty or df_day3.empty:\n",
    "                    continue\n",
    "\n",
    "                # Merge day 1 and day 3 data\n",
    "                df_merged = df_day1[['theta']].rename(columns={'theta': 'theta_day1'}).merge(\n",
    "                    df_day3[['theta']].rename(columns={'theta': 'theta_day3'}),\n",
    "                    left_index=True, right_index=True)\n",
    "\n",
    "                # Ensure there are no missing values or nan issues\n",
    "                if df_merged.empty or df_merged['theta_day1'].isnull().all() or df_merged['theta_day3'].isnull().all():\n",
    "                    continue\n",
    "\n",
    "                # Perform paired t-test by subject for each channel\n",
    "                t_stat, p_value = ttest_rel(df_merged['theta_day1'], df_merged['theta_day3'])\n",
    "\n",
    "                # If significant, store the z-score (based on the t-statistic)\n",
    "                if p_value < 0.05:\n",
    "                    # Store the significant channel information\n",
    "                    results_list.append({\n",
    "                        'Channel': channel,\n",
    "                        'Group': group,\n",
    "                        'Condition': condition,\n",
    "                        'Chroma': chroma,\n",
    "                        'z_score': t_stat,\n",
    "                        'p_value': p_value\n",
    "                    })\n",
    "\n",
    "\n",
    "# Convert results_list to DataFrame\n",
    "significant_channels_df = pd.DataFrame(results_list)\n",
    "significant_channels_df.to_csv(output_csv_path, index=False)\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import os.path as op  # Import os.path as op\n",
    "\n",
    "# Load dataframes\n",
    "results_path = '../../results'\n",
    "df_combined_final = pd.read_csv(op.join(results_path, 'df_combined_final_cha_final.csv'))\n",
    "df_combined_final['ch_name'] = df_combined_final['ch_name'].astype(str).str.strip().str.split(' ').str[0]\n",
    "\n",
    "# Further filter to only include 'control' and 'trained' groups\n",
    "filtered_df = df_combined_final[df_combined_final['group'].isin(['control', 'trained'])]\n",
    "\n",
    "# Ensure 'ch_name' and 'Condition' columns are of the same data type\n",
    "filtered_df['day'] = filtered_df['day'].astype(str)\n",
    "filtered_df['ch_name'] = filtered_df['ch_name'].astype(str)\n",
    "filtered_df['Condition'] = filtered_df['Condition'].astype(str)\n",
    "\n",
    "# Prepare the data for analysis\n",
    "data = filtered_df[['subject', 'group', 'day', 'ch_name', 'Condition', 'Chroma', 'theta']].copy()\n",
    "results_list = []\n",
    "\n",
    "# List of unique chromas and conditions\n",
    "chromas = data['Chroma'].unique()\n",
    "conditions = data['Condition'].unique()\n",
    "\n",
    "# Run mixed-effects model for each combination of chroma and condition\n",
    "for chroma in filtered_df['Chroma'].unique():\n",
    "    for condition in filtered_df['Condition'].unique():\n",
    "        subset = data[(data['Chroma'] == chroma) & (data['Condition'] == condition)]\n",
    "        \n",
    "        if subset.empty:\n",
    "            continue\n",
    "        \n",
    "        # Compute mean activity for each ROI\n",
    "        for roi, channels in roi_channels.items():\n",
    "            roi_data = subset[subset['ch_name'].isin(channels)]\n",
    "            \n",
    "            if roi_data.empty:\n",
    "                continue\n",
    "            \n",
    "            roi_means = roi_data.groupby(['subject', 'group', 'day']).agg({\n",
    "                'theta': 'mean'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Run mixed-effects model: day * group interaction\n",
    "            model = smf.mixedlm('theta ~ day * group', roi_means, groups=roi_means['subject'])\n",
    "            result = model.fit()\n",
    "            \n",
    "            # Extract p-value\n",
    "            p_values = result.pvalues\n",
    "            if 'day[T.3]:group[T.trained]' in p_values:\n",
    "                p_value = p_values['day[T.3]:group[T.trained]']\n",
    "                results_list.append({\n",
    "                    'Chroma': chroma,\n",
    "                    'Condition': condition,\n",
    "                    'ROI': roi,\n",
    "                    't_stat': result.tvalues['day[T.3]:group[T.trained]'],\n",
    "                    'p_value_unadj': p_value,\n",
    "                    'summary': result.summary().as_text()\n",
    "                })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Apply FDR correction separately for each chroma/condition combo\n",
    "def fdr_correction(group_df):\n",
    "    p_values = group_df['p_value_unadj'].values\n",
    "    rejected, corrected_p_values, _, _ = multipletests(p_values, method='fdr_bh')\n",
    "    group_df['p_value_adj'] = corrected_p_values\n",
    "    return group_df\n",
    "\n",
    "# Apply FDR correction separately for each combination of chroma and condition\n",
    "results_df = results_df.groupby(['Chroma', 'Condition']).apply(fdr_correction).reset_index(drop=True)\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv(op.join(results_path, 'correlations/roi_day_group_interactions_fdr.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path as op  # Import os.path as op\n",
    "\n",
    "# Load dataframes\n",
    "results_path = '../../results'\n",
    "df_combined_final = pd.read_csv(op.join(results_path, 'df_combined_final_cha_final.csv'))\n",
    "df_combined_final['ch_name'] = df_combined_final['ch_name'].astype(str).str.strip().str.split(' ').str[0]\n",
    "channel_ttest_results = pd.read_csv(op.join(results_path, 'correlations/channel_ttest_results.csv'))\n",
    "significant_channels = channel_ttest_results['Channel'].unique()\n",
    "\n",
    "# Filter df_combined_final based on significant channels\n",
    "filtered_df = df_combined_final[df_combined_final['ch_name'].isin(significant_channels)]\n",
    "\n",
    "# Further filter to only include 'control' and 'trained' groups\n",
    "filtered_df = filtered_df[filtered_df['group'].isin(['control', 'trained'])]\n",
    "\n",
    "# Ensure the two groups exist\n",
    "groups = filtered_df['group'].unique()\n",
    "if len(groups) != 2:\n",
    "    raise ValueError(\"The dataset should only contain 'control' and 'trained' groups.\")\n",
    "\n",
    "# Unique days present in the data\n",
    "days = filtered_df['day'].unique()\n",
    "\n",
    "for channel in significant_channels:\n",
    "    channel_data = filtered_df[filtered_df['ch_name'] == channel]\n",
    "    \n",
    "    if not channel_data.empty:\n",
    "        # Create a summary DataFrame with means and standard errors for each group and day\n",
    "        summary_df = channel_data.groupby(['group', 'day'])['theta'].agg(['mean', 'sem']).reset_index()\n",
    "\n",
    "        # Create figure and axes for the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Bar width and index setup\n",
    "        bar_width = 0.35\n",
    "        index = np.arange(len(groups))\n",
    "        colors = {'control': 'lightblue', 'trained': 'lightgray'}\n",
    "\n",
    "        # Loop over days to plot each day's bar\n",
    "        for i, group in enumerate(groups):\n",
    "            group_data = summary_df[summary_df['group'] == group]\n",
    "            plt.bar(index + i * bar_width, group_data['mean'], bar_width,\n",
    "                    yerr=group_data['sem'], label=f'{group}',\n",
    "                    color=['lightgray'], alpha=0.75)\n",
    "\n",
    "        # Set x-axis labels and title\n",
    "        plt.xlabel('Group')\n",
    "        plt.ylabel('Mean Theta Difference')\n",
    "        plt.title(f'Mean Theta Difference by Group and Day with SEM for Channel {channel}')\n",
    "        plt.xticks(index + bar_width / 2, groups)\n",
    "\n",
    "        # Add legend and adjust layout\n",
    "        plt.legend(title='Day')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_analysis(group, output_suffix):\n",
    "    significant_models = []\n",
    "    all_p_values = {condition: {response_var: {chroma: {day: [] for day in days} for chroma in chromas} for response_var in response_vars} for condition in conditions}\n",
    "    all_model_data = {condition: {response_var: {chroma: {day: [] for day in days} for chroma in chromas} for response_var in response_vars} for condition in conditions}\n",
    "    max_r_squared = 0\n",
    "    max_r_squared_model = None\n",
    "\n",
    "    for day in days:\n",
    "        theta_df_filtered = theta_df[theta_df['day'] == day].copy()\n",
    "        theta_df_filtered['ch_name'] = theta_df_filtered['ch_name'].str.split(' ').str[0]\n",
    "\n",
    "        for chroma in chromas:\n",
    "            theta_dataset = theta_df_filtered[theta_df_filtered['Chroma'] == chroma].copy()\n",
    "\n",
    "            # Aggregate channels into ROI activity\n",
    "            roi_data = pd.DataFrame()\n",
    "            for roi, channels in roi_channels.items():\n",
    "                roi_data[roi] = theta_dataset[theta_dataset['ch_name'].isin(channels)].groupby(['subject', 'group', 'Condition'])['theta'].mean()\n",
    "\n",
    "            # Add ROI data to theta_pivot\n",
    "            theta_pivot = roi_data.reset_index()\n",
    "\n",
    "            for condition in conditions:\n",
    "                for response_var in response_vars:\n",
    "                    # Ensure response_var exists in the merged DataFrame\n",
    "                    theta_df_condition = theta_dataset[theta_dataset['Condition'] == condition]\n",
    "                    \n",
    "                    # Merge the theta data with behavioral data\n",
    "                    merged_df = pd.merge(theta_pivot, behavior_df[['subject', 'group', 'TBW', 'AO_WR', 'AV_WR', 'VO_WR', 'age', 'AO_WR_1', 'AV_WR_1', 'VO_WR_1', 'TBW_1']], on=['subject', 'group'])\n",
    "\n",
    "                    for roi in roi_channels.keys():  # iterate over ROIs instead of individual channels\n",
    "                        if roi not in merged_df.columns:\n",
    "                            print(f\"Warning: {roi} not in merged DataFrame columns\")\n",
    "                            continue\n",
    "\n",
    "                        # Ensure response variable exists in merged_df\n",
    "                        if response_var not in merged_df.columns:\n",
    "                            print(f\"Warning: {response_var} not in merged DataFrame columns\")\n",
    "                            continue\n",
    "\n",
    "                        df = merged_df[[roi, response_var, 'group']].dropna()  # drop rows with missing values\n",
    "                        if df.empty:\n",
    "                            continue\n",
    "\n",
    "                        model = smf.ols(f\"{response_var} ~ {roi}\", df[df['group'] == group]).fit()\n",
    "                        r_sq = model.rsquared\n",
    "                        p_value_roi = model.pvalues[roi]\n",
    "                        all_p_values[condition][response_var][chroma][day].append(p_value_roi)\n",
    "                        all_model_data[condition][response_var][chroma][day].append((condition, roi, response_var, model, r_sq, p_value_roi, df, chroma, day))\n",
    "\n",
    "                    p_values = all_p_values[condition][response_var][chroma][day]\n",
    "                    model_data = all_model_data[condition][response_var][chroma][day]\n",
    "\n",
    "                    if p_values:\n",
    "                        rejected, p_values_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "                        for (condition, roi, response_var, model, r_sq, p_value, df, chroma, day), p_val_corr, reject in zip(model_data, p_values_corrected, rejected):\n",
    "                            if p_val_corr < 0.05:\n",
    "                                print(f\"Group: {group}, Day: {day}, Chroma: {chroma}, Condition: {condition}, ROI: {roi}, Outcome: {response_var}\\n   R-squared: {r_sq}, corrected p-value: {p_val_corr}\\n\")\n",
    "                                significant_models.append({\n",
    "                                    'Condition': condition,\n",
    "                                    'ROI': roi,\n",
    "                                    'Response Variable': response_var,\n",
    "                                    'R-squared': r_sq,\n",
    "                                    'P-value': p_value,\n",
    "                                    'P-value Corrected': p_val_corr,\n",
    "                                    'Model Summary': model.summary().as_text(),\n",
    "                                    'Chroma': chroma,\n",
    "                                    'Day': day\n",
    "                                })\n",
    "\n",
    "    if significant_models:\n",
    "        significant_models_df = pd.DataFrame(significant_models).sort_values(by='R-squared', ascending=False)\n",
    "        significant_models_df.to_csv(op.join(results_path, f'{output_suffix}_{group}_models.csv'), index=False)\n",
    "    else:\n",
    "        print(f'No significant models found for {group} group')\n",
    "\n",
    "# Perform analysis for the trained group\n",
    "perform_analysis('trained', output_suffix)\n",
    "\n",
    "# Perform analysis for the control group\n",
    "perform_analysis('control', output_suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
