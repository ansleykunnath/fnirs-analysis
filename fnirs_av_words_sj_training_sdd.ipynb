{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SJ Training fNIRS Data Analysis</h1>\n",
    "Written by Ansley Kunnath\n",
    "Updated July 29, 2024\n",
    "Python v3.9.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import PyQt5\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import pooch\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "#import pyvistaqt\n",
    "#import pyvista\n",
    "from itertools import compress\n",
    "from collections import defaultdict\n",
    "from scipy import signal, stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_rel, zscore\n",
    "import os.path as op\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import mne_nirs.preprocessing\n",
    "import mne_nirs.statistics\n",
    "import mne_nirs.utils\n",
    "import mne_nirs.statistics\n",
    "import mne\n",
    "from mne.viz import plot_compare_evokeds\n",
    "from mne.preprocessing.nirs import tddr\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix, compute_regressor  \n",
    "from mne_nirs.channels import get_long_channels, get_short_channels, picks_pair_to_idx\n",
    "\n",
    "# Open plots in new window\n",
    "%matplotlib qt \n",
    "\n",
    "#warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n",
      "N=29\n"
     ]
    }
   ],
   "source": [
    "# Locate files\n",
    "\n",
    "subjects = ('201 202 203 204 205 206 207 208 209 212 213 214 215 216 217 218 219 221 223 224 225 226 228 229 230 231 232 233 234').split() #\n",
    "\n",
    "# Mapping of subjects to groups\n",
    "subject_to_group = {\n",
    "    201: \"trained\",\n",
    "    202: \"control\",\n",
    "    203: \"trained\",\n",
    "    204: \"control\",\n",
    "    205: \"control\",\n",
    "    206: \"control\",\n",
    "    207: \"trained\",\n",
    "    208: \"control\",\n",
    "    209: \"control\",\n",
    "    212: \"trained\",\n",
    "    213: \"trained\",\n",
    "    214: \"trained\",\n",
    "    215: \"control\",\n",
    "    216: \"trained\",\n",
    "    217: \"control\",\n",
    "    218: \"control\",\n",
    "    219: \"trained\",\n",
    "#    220: \"control\", #sampling rate = 2.40 Hz instead of 4.8\n",
    "    221: \"trained\",\n",
    "    223: \"trained\",\n",
    "    224: \"control\",\n",
    "    225: \"trained\",\n",
    "    226: \"control\",\n",
    "    228: \"trained\",\n",
    "    229: \"control\",\n",
    "    230: \"trained\",\n",
    "    231: \"trained\",\n",
    "    232: \"control\",\n",
    "    233: \"trained\",\n",
    "    234: \"control\",\n",
    "}\n",
    "\n",
    "sfreq = 4.807692\n",
    "conditions = ('A', 'V', 'AV', 'W')\n",
    "groups = ('trained','control')\n",
    "days = ('1', '3')\n",
    "runs = (1, 2)\n",
    "\n",
    "condition_colors = dict(  # https://personal.sron.nl/~pault/data/colourschemes.pdf\n",
    "    A='#4477AA',  # sblue\n",
    "    AV='#CCBB44',  # yellow\n",
    "    V='#EE7733',  # orange\n",
    "    W='#AA3377',  # purple\n",
    ")\n",
    "exp_name = 'av'\n",
    "duration = 1.8\n",
    "design = 'event'\n",
    "plot_subject = '205'\n",
    "plot_day = 1\n",
    "plot_run = 1\n",
    "beh_title, beh_idx = 'AV', 0\n",
    "filt_kwargs = dict(\n",
    "    l_freq=0.01, l_trans_bandwidth=0.02,\n",
    "    h_freq=0.2, h_trans_bandwidth=0.02) \n",
    "run_h = True  # regenerate HbO/HbR\n",
    "n_jobs = 4  # for GLM\n",
    "\n",
    "# SET FOLDER LOCATIONS\n",
    "# I save the output files outside of the folder that's uploaded to Github\n",
    "current_directory = %pwd\n",
    "os.chdir(current_directory)\n",
    "\n",
    "raw_path = '../../data'\n",
    "proc_path = '../../processed'\n",
    "results_path = '../../results'\n",
    "subjects_dir = '../../subjects'\n",
    "os.makedirs(proc_path, exist_ok=True)\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "os.makedirs(subjects_dir, exist_ok=True)\n",
    "#mne.datasets.fetch_fsaverage(subjects_dir=subjects_dir, verbose=True)\n",
    "\n",
    "use = None\n",
    "all_sci = list()\n",
    "plt.rcParams['axes.titlesize'] = 8\n",
    "plt.rcParams['axes.labelsize'] = 8\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "got_bad = 0\n",
    "got_total = 0\n",
    "\n",
    "# Prep making bad channels report\n",
    "bad_channels_filename = op.join(results_path, 'bad_channels_report.csv')\n",
    "with open(bad_channels_filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Subject', 'Day', 'Run', 'Short Channels', 'Percent Bad'])\n",
    "\n",
    "def normalize_channel_names(channels_set):\n",
    "    return {name.split()[0] for name in channels_set}\n",
    "\n",
    "def add_bad_channel_entry(subject, day, run, short_chans, percentage_bad):\n",
    "    with open(bad_channels_filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([subject, day, run, len(short_chans), f'{percentage_bad:.2f}%'])\n",
    "\n",
    "def piecewise_linear_detrend(raw, sfreq, segment_length):\n",
    "    data = raw.get_data()\n",
    "    n_channels, n_samples = data.shape\n",
    "    segment_samples = int(segment_length * sfreq)    \n",
    "    for ch in range(n_channels):\n",
    "        for start in range(0, n_samples, segment_samples):\n",
    "            end = min(start + segment_samples, n_samples)\n",
    "            segment = data[ch, start:end]\n",
    "            time = np.arange(start, end)\n",
    "            p = np.polyfit(time, segment, 1)\n",
    "            trend = np.polyval(p, time)\n",
    "            data[ch, start:end] -= trend    \n",
    "    raw._data = data\n",
    "    return raw\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_fnirs_data(raw_data, proc_path, base):\n",
    "    raw_od = mne.preprocessing.nirs.optical_density(raw_data, verbose='error')\n",
    "\n",
    "    # Identify bad channels\n",
    "    peaks = np.ptp(raw_od.get_data('fnirs'), axis=-1)\n",
    "    flat_names = [raw_od.ch_names[f].split(' ')[0] for f in np.where(peaks < 0.001)[0]]\n",
    "    sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "    sci_mask = (sci < 0.25)\n",
    "    got = np.where(sci_mask)[0]\n",
    "    percentage_bad = (len(got) / len(raw_od.ch_names)) * 100\n",
    "    print(f'    Run {base}')\n",
    "\n",
    "    # Assign bads\n",
    "    assert raw_od.info['bads'] == []\n",
    "    bads = set(raw_od.ch_names[pick] for pick in got)\n",
    "    bads = bads | set(ch_name for ch_name in raw_od.ch_names if ch_name.split(' ')[0] in flat_names)\n",
    "    bads = sorted(bads)\n",
    "\n",
    "    # Further preprocessing\n",
    "    #raw_detrend = piecewise_linear_detrend(raw_data, sfreq, segment_length=60) # detrend??\n",
    "    #raw_tddr = tddr(raw_od) # DON'T TDDR?\n",
    "    #raw_tddr_bp = raw_od.copy().filter(**filt_kwargs) # DON'T BANDPASS FILTER?\n",
    "    raw_tddr_bp = raw_od.copy() # Alternative Code\n",
    "    raw_tddr_bp.info['bads'] = bads\n",
    "    picks = mne.pick_types(raw_tddr_bp.info, fnirs=True)\n",
    "    peaks = np.ptp(raw_tddr_bp.get_data(picks), axis=-1)\n",
    "    assert (peaks > 1e-5).all()\n",
    "    raw_tddr_bp.info['bads'] = [] \n",
    "    raw_h = mne.preprocessing.nirs.beer_lambert_law(raw_tddr_bp, 6.)\n",
    "\n",
    "    # Normalize and verify bad channels\n",
    "    h_bads = [ch_name for ch_name in raw_h.ch_names if ch_name.split(' ')[0] in set(bad.split(' ')[0] for bad in bads)]\n",
    "    set_bads = set(bads)\n",
    "    set_h_bads = set(h_bads)\n",
    "    normalized_bads = normalize_channel_names(set_bads)\n",
    "    normalized_h_bads = normalize_channel_names(set_h_bads)\n",
    "    assert normalized_bads == normalized_h_bads\n",
    "    raw_h.info['bads'] = h_bads\n",
    "    raw_h.info._check_consistency()\n",
    "\n",
    "    # Further verification\n",
    "    picks = mne.pick_types(raw_h.info, fnirs=True)\n",
    "    peaks = np.ptp(raw_h.get_data(picks), axis=-1)\n",
    "    assert (peaks > 1e-9).all()\n",
    "\n",
    "    # Interpolate bad channels\n",
    "    raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method=dict(fnirs='nearest'))\n",
    "    raw_h_interp.save(op.join(proc_path, f'{base}_hbo_raw.fif'), overwrite=True)\n",
    "    assert len(raw_h.ch_names) == len(raw_h_interp.ch_names)\n",
    "\n",
    "    return raw_h_interp, percentage_bad, bads\n",
    "\n",
    "# Sanity check for subjects\n",
    "subjects_check = {int(subject) for subject in subjects}\n",
    "subject_to_group_check = set(subject_to_group.keys())\n",
    "if subjects_check == subject_to_group_check:\n",
    "    print(\"All done!\") \n",
    "    print(\"N=\" + str(len(subjects)))\n",
    "    del subjects_check\n",
    "    del subject_to_group_check\n",
    "else:\n",
    "    print(\"Error loading subject info\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Saved plot file!\n",
      "Opening raw data file ../../processed/205_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3864 =      0.000 ...   803.712 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Saved plot file!\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Load participant data\n",
    "\n",
    "#subjects = ('201 202').split() #for testing\n",
    "\n",
    "for subject in subjects:\n",
    "    for day in days:\n",
    "        for run in runs:\n",
    "            group = subject_to_group.get(int(subject), \"unknown\")\n",
    "            root1 = f'Day{day}'\n",
    "            root2 = f'{subject}_{day}'\n",
    "            root3 = f'*-*-*_{run:03d}'\n",
    "            fname_base = op.join(raw_path, root1, root2, root3)\n",
    "            fname = glob.glob(fname_base)\n",
    "            base = f'{subject}_{day}_{run:03d}'\n",
    "            base_pr = base.ljust(20)\n",
    "            # Save the plot subject\n",
    "            if (op.isfile(op.join(proc_path, f'{base}_hbo_raw.fif')) and subject == plot_subject and run == plot_run):\n",
    "                fname2 = op.join(proc_path, f'{subject}_{day}_{run:03d}_hbo_raw.fif')\n",
    "                use = mne.io.read_raw_fif(fname2)\n",
    "                events, _ = mne.events_from_annotations(use)\n",
    "                ch_names = [ch_name.rstrip(' hbo') for ch_name in use.ch_names]\n",
    "                info = use.info\n",
    "                print(\"Saved plot file!\")\n",
    "            if not op.isfile(op.join(proc_path, f'{base}_hbo_raw.fif')):\n",
    "                raw_intensity = mne.io.read_raw_nirx(fname[0])\n",
    "                short_chans = []\n",
    "                long_chans = get_long_channels(raw_intensity)\n",
    "                try:\n",
    "                    short_chans = get_short_channels(raw_intensity)\n",
    "                    print(\"Found some short channels!\")\n",
    "                except ValueError:\n",
    "                    print(\"No short channels found.\")\n",
    "                    pass\n",
    "                raw_h_long, percentage_bad_long, bads_long = preprocess_fnirs_data(long_chans, proc_path, base + '_long')\n",
    "                if len(short_chans) > 0:\n",
    "                    # Preprocess short channels\n",
    "                    raw_h_short, percentage_bad_short, bads_short = preprocess_fnirs_data(short_chans, proc_path, base + '_short')\n",
    "                else:\n",
    "                    raw_h_short = None\n",
    "                add_bad_channel_entry(subject, day, run, short_chans, percentage_bad_long)\n",
    "                if raw_h_short:\n",
    "                    add_bad_channel_entry(subject, day, run, short_chans, percentage_bad_short)\n",
    "                del raw_intensity\n",
    "\n",
    "print(\"All done!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Plot bad channels\n",
    " \n",
    "bad_channels_df = pd.read_csv(bad_channels_filename)\n",
    "bad_channels_df['Percent Bad'] = bad_channels_df['Percent Bad'].str.rstrip('%').astype(float)\n",
    "bad_channels_df['Subject'] = bad_channels_df['Subject'].astype(str)\n",
    "\n",
    "# Sort the DataFrame by percentage_bad in ascending order\n",
    "bad_channels_df = bad_channels_df.groupby('Subject')['Percent Bad'].mean().reset_index()\n",
    "bad_channels_df = bad_channels_df.sort_values(by='Percent Bad')\n",
    "bad_channels_df['Group'] = bad_channels_df['Subject'].map(lambda x: subject_to_group[int(x)])\n",
    "\n",
    "# Sort the DataFrame by Percent Bad in ascending order\n",
    "colors = bad_channels_df['Group'].map({'control': 'gray', 'trained': 'lightblue'})\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(bad_channels_df['Subject'], bad_channels_df['Percent Bad'], color=colors)\n",
    "plt.axhline(30, color='red', linestyle='--')  # Horizontal line at 30%\n",
    "plt.xlabel('Subject', fontsize=14)\n",
    "plt.ylabel('Percent Bad', fontsize=14)\n",
    "plt.title('Percentage of Bad Channels per Participant', fontsize=14)\n",
    "plt.ylim(0, 100)  # Set y-axis from 0 to 100%\n",
    "plt.yticks(np.arange(0, 101, 10), [f'{i}%' for i in np.arange(0, 101, 10)])  # Set y-axis ticks from 0 to 100 with a step of 10 and add percentage signs\n",
    "\n",
    "output_filename = op.join(results_path, 'bad_channels_report_detrend.png')\n",
    "plt.savefig(output_filename, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 5 trained subjects.\n",
      "Removed 4 control subjects.\n",
      "Remaining trained subjects: 10\n",
      "Remaining control subjects: 10\n"
     ]
    }
   ],
   "source": [
    "# Remove patients with over 30% bad channels on average across days and runs\n",
    "\n",
    "subjects_to_remove = ['202', '203', '204', '206', '214', '221', '223', '226', '233'] #201, 209?\n",
    "\n",
    "# Initialize counters for each group\n",
    "removed_trained = 0\n",
    "removed_control = 0\n",
    "\n",
    "remaining_trained = 0\n",
    "remaining_control = 0\n",
    "\n",
    "# Count and remove the subjects\n",
    "for subject in subjects_to_remove:\n",
    "    subject_int = int(subject)  # Convert to integer for dictionary key comparison\n",
    "    if subject_int in subject_to_group:\n",
    "        # Increment the appropriate counter based on the group of the subject\n",
    "        if subject_to_group[subject_int] == \"trained\":\n",
    "            removed_trained += 1\n",
    "        elif subject_to_group[subject_int] == \"control\":\n",
    "            removed_control += 1\n",
    "        # Remove the subject from the dictionary\n",
    "        subject_to_group.pop(subject_int, None)\n",
    "\n",
    "# Update the subjects list after counting the removed subjects\n",
    "subjects = [subject for subject in subjects if subject not in subjects_to_remove]\n",
    "\n",
    "for group in subject_to_group.values():\n",
    "    if group == \"trained\":\n",
    "        remaining_trained += 1\n",
    "    elif group == \"control\":\n",
    "        remaining_control += 1\n",
    "\n",
    "# Output the results\n",
    "print(f'Removed {removed_trained} trained subjects.')\n",
    "print(f'Removed {removed_control} control subjects.')\n",
    "print(f'Remaining trained subjects: {remaining_trained}')\n",
    "print(f'Remaining control subjects: {remaining_control}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../processed/201_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3994 =      0.000 ...   830.752 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/201_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3838 =      0.000 ...   798.304 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/201_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/201_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/205_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 4933 =      0.000 ...  1026.064 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/205_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3864 =      0.000 ...   803.712 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/205_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3943 =      0.000 ...   820.144 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/207_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3929 =      0.000 ...   817.232 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '255.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/207_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3958 =      0.000 ...   823.264 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/207_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3927 =      0.000 ...   816.816 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/207_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/208_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/208_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3931 =      0.000 ...   817.648 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/208_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3971 =      0.000 ...   825.968 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/208_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3726 =      0.000 ...   775.008 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/209_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3966 =      0.000 ...   824.928 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/209_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3965 =      0.000 ...   824.720 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/209_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3965 =      0.000 ...   824.720 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/209_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/212_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3946 =      0.000 ...   820.768 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/212_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/212_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4047 =      0.000 ...   841.776 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/212_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3973 =      0.000 ...   826.384 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/213_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3972 =      0.000 ...   826.176 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/213_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3951 =      0.000 ...   821.808 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/213_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3989 =      0.000 ...   829.712 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/213_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3974 =      0.000 ...   826.592 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/215_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4361 =      0.000 ...   907.088 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/215_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3923 =      0.000 ...   815.984 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/215_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3934 =      0.000 ...   818.272 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/215_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3975 =      0.000 ...   826.800 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/216_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3936 =      0.000 ...   818.688 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/216_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3931 =      0.000 ...   817.648 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/216_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3940 =      0.000 ...   819.520 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/216_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3927 =      0.000 ...   816.816 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/217_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3930 =      0.000 ...   817.440 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/217_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3943 =      0.000 ...   820.144 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/217_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3922 =      0.000 ...   815.776 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/217_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3946 =      0.000 ...   820.768 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/218_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3968 =      0.000 ...   825.344 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/218_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3953 =      0.000 ...   822.224 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/218_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3951 =      0.000 ...   821.808 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/218_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3931 =      0.000 ...   817.648 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/219_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3969 =      0.000 ...   825.552 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/219_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3924 =      0.000 ...   816.192 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/219_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3934 =      0.000 ...   818.272 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/219_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3924 =      0.000 ...   816.192 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/224_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3924 =      0.000 ...   816.192 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/224_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3935 =      0.000 ...   818.480 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/224_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3952 =      0.000 ...   822.016 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/224_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/225_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3942 =      0.000 ...   819.936 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/225_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3939 =      0.000 ...   819.312 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/225_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/225_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3943 =      0.000 ...   820.144 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/228_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3978 =      0.000 ...   827.424 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/228_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3978 =      0.000 ...   827.424 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/228_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3992 =      0.000 ...   830.336 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/228_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3954 =      0.000 ...   822.432 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/229_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3956 =      0.000 ...   822.848 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/229_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3968 =      0.000 ...   825.344 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/229_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/229_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3934 =      0.000 ...   818.272 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/230_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/230_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3925 =      0.000 ...   816.400 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/230_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3986 =      0.000 ...   829.088 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/230_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 4007 =      0.000 ...   833.456 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/231_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3927 =      0.000 ...   816.816 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/231_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3937 =      0.000 ...   818.896 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/231_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3930 =      0.000 ...   817.440 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/231_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3949 =      0.000 ...   821.392 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/232_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3944 =      0.000 ...   820.352 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/232_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3935 =      0.000 ...   818.480 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/232_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3942 =      0.000 ...   819.936 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/232_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3946 =      0.000 ...   820.768 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/234_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3964 =      0.000 ...   824.512 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/234_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3938 =      0.000 ...   819.104 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/234_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3883 =      0.000 ...   807.664 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/234_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3956 =      0.000 ...   822.848 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Define make_design\n",
    "\n",
    "sfreq = 4.807692\n",
    "\n",
    "for subject in subjects:\n",
    "    for day in days:\n",
    "        for run in runs:\n",
    "            fname = op.join(proc_path, f'{subject}_{day}_{run:03d}_hbo_raw.fif')\n",
    "            raw_h = mne.io.read_raw_fif(fname)\n",
    "            events, _ = mne.events_from_annotations(raw_h)\n",
    "            #print(len(events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "def _make_design(raw_h_long, raw_h_short, design, subject=None, run=None, day=None, group=None):\n",
    "    annotations_to_remove = raw_h_long.annotations.description == '255.0'\n",
    "    raw_h_long.annotations.delete(annotations_to_remove)\n",
    "    events, _ = mne.events_from_annotations(raw_h_long)\n",
    "    rows_to_remove = events[:, -1] == 1\n",
    "    events = events[~rows_to_remove]\n",
    "    \n",
    "    # Mis-codings\n",
    "    if len(events) == 101:\n",
    "        events = events[1:]\n",
    "        \n",
    "    n_times = len(raw_h_long.times)\n",
    "    stim = np.zeros((n_times, 4))\n",
    "    events[:, 2] -= 1\n",
    "    assert len(events) == 100, len(events)\n",
    "    want = [0] + [25] * 4\n",
    "    count = np.bincount(events[:, 2])\n",
    "    assert np.array_equal(count, want), count\n",
    "    assert events.shape == (100, 3), events.shape\n",
    "    if design == 'block':\n",
    "        events = events[0::5]\n",
    "        duration = 20.\n",
    "        assert np.array_equal(np.bincount(events[:, 2]), [0] + [5] * 4)\n",
    "    else:\n",
    "        assert design == 'event'\n",
    "        assert len(events) == 100\n",
    "        duration = 1.8\n",
    "        assert events.shape == (100, 3)\n",
    "        events_r = events[:, 2].reshape(20, 5)\n",
    "        assert (events_r == events_r[:, :1]).all()\n",
    "        del events_r\n",
    "        \n",
    "    idx = (events[:, [0, 2]] - [0, 1]).T\n",
    "    assert np.in1d(idx[1], np.arange(len(conditions))).all()\n",
    "    stim[tuple(idx)] = 1\n",
    "    \n",
    "    # assert raw_h.info['sfreq'] == sfreq  # necessary for below logic to work\n",
    "    n_block = int(np.ceil(duration * sfreq))\n",
    "    stim = signal.fftconvolve(stim, np.ones((n_block, 1)), axes=0)[:n_times]\n",
    "    dm_events = pd.DataFrame({\n",
    "        'trial_type': [conditions[ii] for ii in idx[1]],\n",
    "        'onset': idx[0] / raw_h_long.info['sfreq'],\n",
    "        'duration': n_block / raw_h_long.info['sfreq']})\n",
    "    dm = make_first_level_design_matrix(\n",
    "        raw_h_long.times, dm_events, hrf_model='glover',\n",
    "        drift_model='polynomial', drift_order=0)\n",
    "    \n",
    "    # Add short channel data to the design matrix\n",
    "    if raw_h_short is not None:\n",
    "        short_data = raw_h_short.get_data().mean(axis=0)  # Ensure short_data is 1D\n",
    "        short_data = short_data - np.mean(short_data)  # Center the short channel data\n",
    "        \n",
    "        # Ensure short_data is aligned with raw_h_short.times\n",
    "        if len(raw_h_short.times) == len(short_data):\n",
    "            short_data_resampled = np.interp(raw_h_long.times, raw_h_short.times, short_data)\n",
    "            dm['short_channel'] = short_data_resampled\n",
    "        else:\n",
    "            print(f\"Length mismatch: raw_h_short.times ({len(raw_h_short.times)}) vs short_data ({len(short_data)})\")\n",
    "    \n",
    "    return stim, dm, events\n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variable</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Chroma</th>\n",
       "      <th>ch_name</th>\n",
       "      <th>subject</th>\n",
       "      <th>day</th>\n",
       "      <th>group</th>\n",
       "      <th>theta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>hbo</td>\n",
       "      <td>S10_D18 hbo</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>control</td>\n",
       "      <td>-2.242495e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>hbo</td>\n",
       "      <td>S10_D18 hbo</td>\n",
       "      <td>221</td>\n",
       "      <td>3</td>\n",
       "      <td>control</td>\n",
       "      <td>-9.336126e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AV</td>\n",
       "      <td>hbo</td>\n",
       "      <td>S10_D18 hbo</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>control</td>\n",
       "      <td>1.442632e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AV</td>\n",
       "      <td>hbo</td>\n",
       "      <td>S10_D18 hbo</td>\n",
       "      <td>221</td>\n",
       "      <td>3</td>\n",
       "      <td>control</td>\n",
       "      <td>7.213588e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V</td>\n",
       "      <td>hbo</td>\n",
       "      <td>S10_D18 hbo</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>control</td>\n",
       "      <td>3.123630e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23179</th>\n",
       "      <td>W</td>\n",
       "      <td>hbr</td>\n",
       "      <td>S9_D21 hbr</td>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "      <td>control</td>\n",
       "      <td>1.347406e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23180</th>\n",
       "      <td>constant</td>\n",
       "      <td>hbr</td>\n",
       "      <td>S9_D21 hbr</td>\n",
       "      <td>234</td>\n",
       "      <td>1</td>\n",
       "      <td>control</td>\n",
       "      <td>7.562976e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23181</th>\n",
       "      <td>constant</td>\n",
       "      <td>hbr</td>\n",
       "      <td>S9_D21 hbr</td>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "      <td>control</td>\n",
       "      <td>-2.444132e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23182</th>\n",
       "      <td>short_channel</td>\n",
       "      <td>hbr</td>\n",
       "      <td>S9_D21 hbr</td>\n",
       "      <td>234</td>\n",
       "      <td>1</td>\n",
       "      <td>control</td>\n",
       "      <td>7.957498e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23183</th>\n",
       "      <td>short_channel</td>\n",
       "      <td>hbr</td>\n",
       "      <td>S9_D21 hbr</td>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "      <td>control</td>\n",
       "      <td>1.818560e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23184 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "variable      Condition Chroma      ch_name subject day    group         theta\n",
       "0                     A    hbo  S10_D18 hbo     221   1  control -2.242495e-07\n",
       "1                     A    hbo  S10_D18 hbo     221   3  control -9.336126e-07\n",
       "2                    AV    hbo  S10_D18 hbo     221   1  control  1.442632e-06\n",
       "3                    AV    hbo  S10_D18 hbo     221   3  control  7.213588e-07\n",
       "4                     V    hbo  S10_D18 hbo     221   1  control  3.123630e-08\n",
       "...                 ...    ...          ...     ...  ..      ...           ...\n",
       "23179                 W    hbr   S9_D21 hbr     234   3  control  1.347406e-07\n",
       "23180          constant    hbr   S9_D21 hbr     234   1  control  7.562976e-09\n",
       "23181          constant    hbr   S9_D21 hbr     234   3  control -2.444132e-08\n",
       "23182     short_channel    hbr   S9_D21 hbr     234   1  control  7.957498e-04\n",
       "23183     short_channel    hbr   S9_D21 hbr     234   3  control  1.818560e-01\n",
       "\n",
       "[23184 rows x 7 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subjects = ('224 225 226 228 229 230 231 232 233 234').split() #\n",
    "\n",
    "df_cha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../processed/201_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3994 =      0.000 ...   830.752 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/201_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3838 =      0.000 ...   798.304 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 201 day 1.\n",
      "Opening raw data file ../../processed/201_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/201_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 201 day 3.\n",
      "Opening raw data file ../../processed/205_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/205_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 4933 =      0.000 ...  1026.064 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 205 day 1.\n",
      "Opening raw data file ../../processed/205_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3864 =      0.000 ...   803.712 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/205_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3943 =      0.000 ...   820.144 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 205 day 3.\n",
      "Opening raw data file ../../processed/207_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3929 =      0.000 ...   817.232 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/207_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3958 =      0.000 ...   823.264 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 207 day 1.\n",
      "Opening raw data file ../../processed/207_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3927 =      0.000 ...   816.816 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/207_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 207 day 3.\n",
      "Opening raw data file ../../processed/208_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/208_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3931 =      0.000 ...   817.648 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 208 day 1.\n",
      "Opening raw data file ../../processed/208_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3971 =      0.000 ...   825.968 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/208_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3726 =      0.000 ...   775.008 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 208 day 3.\n",
      "Opening raw data file ../../processed/209_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3966 =      0.000 ...   824.928 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/209_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3965 =      0.000 ...   824.720 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 209 day 1.\n",
      "Opening raw data file ../../processed/209_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3965 =      0.000 ...   824.720 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/209_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 209 day 3.\n",
      "Opening raw data file ../../processed/212_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3946 =      0.000 ...   820.768 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/212_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 212 day 1.\n",
      "Opening raw data file ../../processed/212_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 4047 =      0.000 ...   841.776 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/212_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3973 =      0.000 ...   826.384 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 212 day 3.\n",
      "Opening raw data file ../../processed/213_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3972 =      0.000 ...   826.176 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/213_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3951 =      0.000 ...   821.808 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 213 day 1.\n",
      "Opening raw data file ../../processed/213_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3989 =      0.000 ...   829.712 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/213_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3974 =      0.000 ...   826.592 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 213 day 3.\n",
      "Opening raw data file ../../processed/215_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 4361 =      0.000 ...   907.088 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/215_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3923 =      0.000 ...   815.984 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 215 day 1.\n",
      "Opening raw data file ../../processed/215_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3934 =      0.000 ...   818.272 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/215_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3975 =      0.000 ...   826.800 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 215 day 3.\n",
      "Opening raw data file ../../processed/216_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3936 =      0.000 ...   818.688 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/216_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3931 =      0.000 ...   817.648 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 216 day 1.\n",
      "Opening raw data file ../../processed/216_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3940 =      0.000 ...   819.520 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/216_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3927 =      0.000 ...   816.816 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 216 day 3.\n",
      "Opening raw data file ../../processed/217_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3930 =      0.000 ...   817.440 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/217_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3943 =      0.000 ...   820.144 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 217 day 1.\n",
      "Opening raw data file ../../processed/217_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3922 =      0.000 ...   815.776 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/217_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3946 =      0.000 ...   820.768 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 217 day 3.\n",
      "Opening raw data file ../../processed/218_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3968 =      0.000 ...   825.344 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/218_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3953 =      0.000 ...   822.224 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 218 day 1.\n",
      "Opening raw data file ../../processed/218_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3951 =      0.000 ...   821.808 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/218_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3931 =      0.000 ...   817.648 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 218 day 3.\n",
      "Opening raw data file ../../processed/219_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3969 =      0.000 ...   825.552 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/219_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3924 =      0.000 ...   816.192 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 219 day 1.\n",
      "Opening raw data file ../../processed/219_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3934 =      0.000 ...   818.272 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/219_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3924 =      0.000 ...   816.192 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 219 day 3.\n",
      "Opening raw data file ../../processed/224_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3924 =      0.000 ...   816.192 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/224_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3935 =      0.000 ...   818.480 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 224 day 1.\n",
      "Opening raw data file ../../processed/224_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3952 =      0.000 ...   822.016 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/224_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 224 day 3.\n",
      "Opening raw data file ../../processed/225_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3942 =      0.000 ...   819.936 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/225_1_001_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3942 =      0.000 ...   819.936 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/225_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3939 =      0.000 ...   819.312 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/225_1_002_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3939 =      0.000 ...   819.312 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 225 day 1.\n",
      "Opening raw data file ../../processed/225_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/225_3_001_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/225_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3943 =      0.000 ...   820.144 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/225_3_002_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3943 =      0.000 ...   820.144 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 225 day 3.\n",
      "Opening raw data file ../../processed/228_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3978 =      0.000 ...   827.424 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/228_1_001_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3978 =      0.000 ...   827.424 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/228_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3978 =      0.000 ...   827.424 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/228_1_002_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3978 =      0.000 ...   827.424 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 228 day 1.\n",
      "Opening raw data file ../../processed/228_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3992 =      0.000 ...   830.336 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/228_3_001_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3992 =      0.000 ...   830.336 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/228_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3954 =      0.000 ...   822.432 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/228_3_002_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3954 =      0.000 ...   822.432 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 228 day 3.\n",
      "Opening raw data file ../../processed/229_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3956 =      0.000 ...   822.848 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/229_1_001_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3956 =      0.000 ...   822.848 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/229_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3968 =      0.000 ...   825.344 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/229_1_002_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3968 =      0.000 ...   825.344 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 229 day 1.\n",
      "Opening raw data file ../../processed/229_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/229_3_001_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/229_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3934 =      0.000 ...   818.272 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/229_3_002_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3934 =      0.000 ...   818.272 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 229 day 3.\n",
      "Opening raw data file ../../processed/230_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/230_1_001_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/230_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3925 =      0.000 ...   816.400 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/230_1_002_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3925 =      0.000 ...   816.400 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 230 day 1.\n",
      "Opening raw data file ../../processed/230_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3986 =      0.000 ...   829.088 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/230_3_001_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3986 =      0.000 ...   829.088 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/230_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 4007 =      0.000 ...   833.456 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/230_3_002_short_hbo_raw.fif...\n",
      "    Range : 0 ... 4007 =      0.000 ...   833.456 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 230 day 3.\n",
      "Opening raw data file ../../processed/231_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3927 =      0.000 ...   816.816 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/231_1_001_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3927 =      0.000 ...   816.816 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/231_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3937 =      0.000 ...   818.896 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/231_1_002_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3937 =      0.000 ...   818.896 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 231 day 1.\n",
      "Opening raw data file ../../processed/231_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3930 =      0.000 ...   817.440 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/231_3_001_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3930 =      0.000 ...   817.440 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/231_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3949 =      0.000 ...   821.392 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/231_3_002_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3949 =      0.000 ...   821.392 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 231 day 3.\n",
      "Opening raw data file ../../processed/232_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3944 =      0.000 ...   820.352 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/232_1_001_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3944 =      0.000 ...   820.352 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/232_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3935 =      0.000 ...   818.480 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/232_1_002_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3935 =      0.000 ...   818.480 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 232 day 1.\n",
      "Opening raw data file ../../processed/232_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3942 =      0.000 ...   819.936 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/232_3_001_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3942 =      0.000 ...   819.936 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/232_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3946 =      0.000 ...   820.768 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/232_3_002_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3946 =      0.000 ...   820.768 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 232 day 3.\n",
      "Opening raw data file ../../processed/234_1_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3964 =      0.000 ...   824.512 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/234_1_001_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3964 =      0.000 ...   824.512 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/234_1_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3938 =      0.000 ...   819.104 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/234_1_002_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3938 =      0.000 ...   819.104 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 234 day 1.\n",
      "Opening raw data file ../../processed/234_3_001_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3883 =      0.000 ...   807.664 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/234_3_001_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3883 =      0.000 ...   807.664 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/234_3_002_long_hbo_raw.fif...\n",
      "    Range : 0 ... 3956 =      0.000 ...   822.848 secs\n",
      "Ready.\n",
      "Opening raw data file ../../processed/234_3_002_short_hbo_raw.fif...\n",
      "    Range : 0 ... 3956 =      0.000 ...   822.848 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 234 day 3.\n",
      "Finished processing all subjects.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Run GLM analysis and epoching\n",
    "\n",
    "sfreq = 4.807692050933838\n",
    "\n",
    "subj_cha_list = []\n",
    "for subject in subjects:\n",
    "    group = subject_to_group.get(int(subject), \"unknown\")\n",
    "    # Check if short-distance channels exist for both day 1 and day 3\n",
    "    include_short_channels = all(op.isfile(op.join(proc_path, f'{subject}_{day}_001_short_hbo_raw.fif')) for day in [1, 3])\n",
    "    \n",
    "    for day in days:\n",
    "        for run in runs:\n",
    "            fname_long = op.join(proc_path, f'{subject}_{day}_{run:03d}_long_hbo_raw.fif')\n",
    "            fname_short = op.join(proc_path, f'{subject}_{day}_{run:03d}_short_hbo_raw.fif')\n",
    "            raw_h_long = mne.io.read_raw_fif(fname_long)\n",
    "            raw_h_short = None\n",
    "            if include_short_channels and op.isfile(fname_short):\n",
    "                raw_h_short = mne.io.read_raw_fif(fname_short)\n",
    "            _, dm, _ = _make_design(raw_h_long, raw_h_short, design, subject, run, day, group)\n",
    "            glm_est = mne_nirs.statistics.run_glm(\n",
    "                raw_h_long, dm, noise_model='ols', n_jobs=n_jobs)\n",
    "            cha = glm_est.to_dataframe()\n",
    "            cha['subject'] = subject\n",
    "            cha['run'] = run\n",
    "            cha['day'] = day\n",
    "            cha['group'] = group\n",
    "            subj_cha_list.append(cha)\n",
    "            del raw_h_long\n",
    "            if raw_h_short:\n",
    "                del raw_h_short\n",
    "        print(f'***Finished processing subject {subject} day {day}.')\n",
    "\n",
    "df_cha = pd.concat(subj_cha_list, ignore_index=True)\n",
    "df_cha.reset_index(drop=True, inplace=True)\n",
    "print(\"Finished processing all subjects.\")\n",
    "\n",
    "# Block averages\n",
    "event_id = {condition: ci for ci, condition in enumerate(conditions, 1)}\n",
    "evokeds = {condition: dict() for condition in conditions}\n",
    "for day in days:\n",
    "    for subject in subjects:\n",
    "        fname = op.join(proc_path, f'{subject}_{day}_{exp_name}-ave.fif')\n",
    "        if not op.isfile(fname):\n",
    "            tmin, tmax = -2, 38\n",
    "            baseline = (None, 0)\n",
    "            t0 = time.time()\n",
    "            print(f'Creating block average for {subject} day {day}... ', end='')\n",
    "            raws = list()\n",
    "            events = list()\n",
    "            for run in runs:\n",
    "                fname2 = op.join(proc_path, f'{subject}_{day}_{run:03d}_hbo_raw.fif')\n",
    "                raw_h = mne.io.read_raw_fif(fname2)\n",
    "                events.append(_make_design(raw_h, None, 'block', subject, run)[2])\n",
    "                raws.append(raw_h)\n",
    "            bads = sorted(set(sum((r.info['bads'] for r in raws), [])))\n",
    "            for r in raws:\n",
    "                r.info['bads'] = bads\n",
    "            raw_h, events = mne.concatenate_raws(raws, events_list=events)\n",
    "            epochs = mne.Epochs(raw_h, events, event_id, tmin=tmin, tmax=tmax,\n",
    "                                baseline=baseline)\n",
    "            this_ev = [epochs[condition].average() for condition in conditions]\n",
    "            assert all(ev.nave > 0 for ev in this_ev)\n",
    "            mne.write_evokeds(fname, this_ev, overwrite=True)\n",
    "            print(f'{time.time() - t0:0.1f} sec')\n",
    "            for condition in conditions:\n",
    "                evokeds[condition][subject] = mne.read_evokeds(fname, condition)\n",
    "            print(f'Done for {group} {subject} day {day} run {run:03d}... ', end='')\n",
    "\n",
    "\n",
    "# Mark bad channels\n",
    "bad = dict()\n",
    "bb = dict()\n",
    "\n",
    "for day in days:\n",
    "    for subject in subjects:\n",
    "        for run in runs:\n",
    "            fname2 = op.join(proc_path, f'{subject}_{day}_{run:03d}_hbo_raw.fif')\n",
    "            this_info = mne.io.read_info(fname2)\n",
    "            bad_channels = [idx - 1 for idx in sorted(\n",
    "                this_info['ch_names'].index(bad) + 1 for bad in this_info['bads'])]\n",
    "            valid_indices = np.arange(len(use.ch_names))\n",
    "            bb = [b for b in bad_channels if b in valid_indices]\n",
    "            bad[(subject, run, day)] = bb\n",
    "        assert np.in1d(bad[(subject, run, day)], np.arange(len(use.ch_names))).all()  # noqa: E501\n",
    "\n",
    "bad_combo = dict()\n",
    "for day in days:\n",
    "    for (subject, run, day), bb in bad_channels:\n",
    "        bad_combo[subject] = sorted(set(bad_combo.get(subject, [])) | set(bb))\n",
    "bad = bad_combo\n",
    "#assert set(bad) == set(subjects)\n",
    "\n",
    "start = len(df_cha)\n",
    "n_drop = 0\n",
    "for day in days:\n",
    "    for (subject, run, day), bb in bad_channels:\n",
    "        if not len(bb):\n",
    "            continue\n",
    "        drop_names = [use.ch_names[b] for b in bb]\n",
    "        is_subject = (df_cha['subject'] == subject)\n",
    "        is_day = (df_cha['day'] == day)\n",
    "        assert len(is_subject) == len(df_cha)\n",
    "        is_day = (df_cha['day'] == day)\n",
    "        drop = df_cha.index[\n",
    "            is_subject &\n",
    "            is_day &\n",
    "            np.in1d(df_cha['ch_name'], drop_names)]\n",
    "        n_drop += len(drop)\n",
    "        if len(drop):\n",
    "            print(f'Dropping {len(drop)} for {subject} day {day}')  # {run}')\n",
    "            df_cha.drop(drop, inplace=True)\n",
    "end = len(df_cha)\n",
    "assert n_drop == start - end, (n_drop, start - end)\n",
    "\n",
    "# Combine runs by averaging\n",
    "sorts = ['subject', 'ch_name', 'Chroma', 'Condition', 'group', 'day', 'run']\n",
    "df_cha.sort_values(\n",
    "    sorts, inplace=True)\n",
    "theta = np.array(df_cha['theta']).reshape(-1, len(runs)).mean(-1)\n",
    "df_cha.drop(\n",
    "    [col for col in df_cha.columns if col not in sorts[:-1]], axis='columns',\n",
    "    inplace=True)\n",
    "df_cha.reset_index(drop=True, inplace=True)\n",
    "df_cha = df_cha[::len(runs)]\n",
    "df_cha.reset_index(drop=True, inplace=True)\n",
    "df_cha['theta'] = theta\n",
    "df_cha.to_csv(op.join(results_path, 'df_cha_theta_sdd.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To skip above block, just run this:\n",
    "#df_cha = pd.read_csv(op.join(results_path, 'df_cha_theta.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS FOR CHANNEL x CONDITION ANALYSIS ONLY\n",
    "\n",
    "# Mixed linear model\n",
    "def _mixed_df(ch_summary):\n",
    "    formula = \"theta ~ -1 + ch_name:Condition\" \n",
    "    ch_model = smf.mixedlm(\n",
    "        formula, ch_summary, groups=ch_summary[\"subject\"]).fit(method='powell')\n",
    "    ch_model_df = mne_nirs.statistics.statsmodels_to_results(ch_model)\n",
    "    ch_model_df['P>|z|'] = ch_model.pvalues\n",
    "    ch_model_df.drop([idx for idx in ch_model_df.index if '[constant]' in idx],\n",
    "                    inplace=True)\n",
    "    return ch_model_df\n",
    "\n",
    "print(\"All done!\") \n",
    "\n",
    "# Make separate subject lists for trained and untrained (TEST)\n",
    "#trained_subjects = {'201 203 207 212 213 214 216 219 221 223'}\n",
    "#control_subjects = {'202 204 205 206 208 209 215 217 218 224'}\n",
    "\n",
    "# Run group level model and convert to dataframe\n",
    "ch_summary = df_cha.query(\"Chroma in ['hbo']\").copy()   ### ALSO RUN HBR ANALYSIS\n",
    "#ch_summary = df_cha.query(\"Chroma in ['hbo'] and group in ['trained'] and day in ['3']\").copy()\n",
    "ch_model_df = _mixed_df(ch_summary) \n",
    "ch_model_df.reset_index(inplace=True)\n",
    "\n",
    "# Correct for multiple comparisons\n",
    "print(f'Correcting for {len(ch_model_df[\"P>|z|\"])} comparisons using FDR')\n",
    "_, ch_model_df['P_fdr'] = mne.stats.fdr_correction(\n",
    "    ch_model_df['P>|z|'], method='indep')\n",
    "ch_model_df['SIG'] = ch_model_df['P_fdr'] < 0.05\n",
    "ch_model_df.to_csv(op.join(results_path, 'ch_model_corrected_hbo.csv'), index=False)\n",
    "ch_model_df.loc[ch_model_df.SIG == True]\n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Plot significant channels\n",
    "\n",
    "sig_chs = dict()\n",
    "zs = dict()\n",
    "for condition in conditions:\n",
    "    sig_df = ch_model_df[\n",
    "        (ch_model_df['P_fdr'] < 0.05) &\n",
    "        (ch_model_df['Condition'] == condition) &\n",
    "        (ch_model_df['ch_name'].isin(use.ch_names))\n",
    "        ]\n",
    "    sig_chs[(condition)] = sorted(\n",
    "        (use.ch_names.index(row[1]['ch_name']), row[1]['P_fdr'])\n",
    "        for row in sig_df.iterrows())\n",
    "    ch_model_df[ch_model_df['Condition'] == condition]\n",
    "    zs[condition] = np.array([\n",
    "        ch_model_df.loc[(ch_model_df['Condition'] == condition) & \n",
    "                        (ch_model_df['ch_name'] == ch_name), 'z'].iloc[0]\n",
    "        for ch_name in info['ch_names'][::2]], float)\n",
    "    assert zs[condition].shape == (84,)\n",
    "    assert np.isfinite(zs[condition]).all()\n",
    "\n",
    "def _plot_sig_chs(sigs, ax):\n",
    "    if sigs and isinstance(sigs[0], tuple):\n",
    "        sigs = [s[0] for s in sigs]\n",
    "    ch_groups = [sigs, np.setdiff1d(np.arange(info['nchan']), sigs)]\n",
    "    mne.viz.plot_sensors(\n",
    "        info, 'topomap', 'hbo', title='', axes=ax,\n",
    "        show_names=True, ch_groups=ch_groups)\n",
    "    ax.collections[0].set(lw=0)\n",
    "    c = ax.collections[0].get_facecolor()\n",
    "    c[(c[:, :3] == (0.5, 0, 0)).all(-1)] = (0., 0., 0., 0.1)\n",
    "    c[(c[:, :3] == (0, 0, 0.5)).all(-1)] = (0., 1., 0., 0.5)\n",
    "    ax.collections[0].set_facecolor(c)\n",
    "    ch_names = [info['ch_names'][idx] for idx in sigs]\n",
    "    texts = list(ax.texts)\n",
    "    got = []\n",
    "    for text in list(texts):\n",
    "        try:\n",
    "            idx = ch_names.index(text.get_text())\n",
    "        except ValueError:\n",
    "            text.remove()\n",
    "        else:\n",
    "            got.append(idx)\n",
    "            text.set_text(f'{sigs[idx] // 2 + 1}')\n",
    "            text.set(fontsize='xx-small', zorder=5, ha='center')\n",
    "    assert len(got) == len(sigs), (got, list(sigs))\n",
    "\n",
    "def _plot_sigs(sig_chs, all_corrs=()):\n",
    "    n_col = max(len(x) for x in sig_chs.values()) + 1\n",
    "    n_row = len(conditions)\n",
    "    figsize = (n_col * 1.0, n_row * 1.0)\n",
    "    fig, axes = plt.subplots(\n",
    "        n_row, n_col, figsize=figsize, constrained_layout=True, squeeze=False)\n",
    "    h_colors = {0: 'r', 1: 'b'}\n",
    "    xticks = [0, 10, 20, 30]\n",
    "    ylim = [-0.2, 0.3]\n",
    "    yticks = [-0.2, -0.1, 0, 0.1, 0.2, 0.3]\n",
    "    xlim = [times[0], 35]\n",
    "    ylim = np.array(ylim)\n",
    "    yticks = np.array(yticks)\n",
    "    for ci, condition in enumerate(conditions):\n",
    "        ii = 0\n",
    "        sigs = sig_chs[condition]\n",
    "        if len(sigs) == 0:\n",
    "            sigs = [(None, None)]\n",
    "        for ii, (ch_idx, ch_p) in enumerate(sigs):\n",
    "            ax = axes[ci, ii]\n",
    "            if ch_idx is not None:\n",
    "                for jj in range(2):  # HbO, HbR\n",
    "                    color = h_colors[jj]\n",
    "                    a = 1e6 * np.array(\n",
    "                        [evokeds[condition][subject].data[ch_idx + jj]\n",
    "                         for subject in use_subjects\n",
    "                         if ch_idx + jj not in bad.get(subject, [])], float)\n",
    "                    m = np.mean(a, axis=0)\n",
    "                    lower, upper = stats.t.interval(\n",
    "                        0.95, len(a) - 1, loc=m, scale=stats.sem(a, axis=0))\n",
    "                    ax.fill_between(\n",
    "                        times, lower, upper, facecolor=color,\n",
    "                        edgecolor='none', lw=0, alpha=0.25, zorder=3,\n",
    "                        clip_on=False)\n",
    "                    ax.plot(times, m, color=color, lw=1, zorder=4,\n",
    "                            clip_on=False)\n",
    "                # Correlations\n",
    "                this_df = ch_summary_use.query(\n",
    "                    f'ch_name == {repr(use.ch_names[ch_idx])} and '\n",
    "                    f'Chroma == \"hbo\" and '\n",
    "                    f'Condition == {repr(condition)}')\n",
    "                #assert 8 <= len(this_df) <= len(subjects), len(this_df)\n",
    "                a = np.array(this_df['theta'])\n",
    "                cs = list()\n",
    "                if len(cs):\n",
    "                    cs = [''] + cs\n",
    "                c = '\\n'.join(cs)\n",
    "                ax.text(times[-1], ylim[1],\n",
    "                        f'ch{ch_idx // 2 + 1}\\np={ch_p:0.5f}{c}',\n",
    "                        ha='right', va='top', fontsize='x-small')\n",
    "            ax.axvline(20, ls=':', color='0.5', zorder=2, lw=1)\n",
    "            ax.axhline(0, ls='-', color='k', zorder=2, lw=0.5)\n",
    "            ax.set(xticks=xticks, yticks=yticks)\n",
    "            ax.set(xlim=xlim, ylim=ylim)\n",
    "            for key in ('top', 'right'):\n",
    "                ax.spines[key].set_visible(False)\n",
    "            if ax.get_subplotspec().is_last_row():\n",
    "                ax.set(xlabel='Time (sec)')\n",
    "            else:\n",
    "                ax.set_xticklabels([''] * len(xticks))\n",
    "            if ax.get_subplotspec().is_first_col():\n",
    "                ax.set_ylabel(condition)\n",
    "            else:\n",
    "                ax.set_yticklabels([''] * len(yticks))\n",
    "            for key in ('top', 'right'):\n",
    "                ax.spines[key].set_visible(False)\n",
    "        for ii in range(ii + 1, n_col - 1):\n",
    "            fig.delaxes(axes[ci, ii])\n",
    "        # montage\n",
    "        ax = axes[ci, -1]\n",
    "        if sigs[0][0] is None:\n",
    "            fig.delaxes(ax)\n",
    "        else:\n",
    "            # plot montage\n",
    "            _plot_sig_chs(sigs, ax)\n",
    "    return fig\n",
    "\n",
    "times = evokeds[conditions[0]][subjects[0]].times\n",
    "info = evokeds[conditions[0]][subjects[0]].info\n",
    "fig = _plot_sigs(sig_chs)\n",
    "for ext in ('png', 'svg'):\n",
    "    fig.savefig(op.join(results_path, f'stats_{exp_name}.{ext}'))\n",
    "\n",
    "print(\"All done!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY PLOT SENSORS\n",
    "\n",
    "def _plot_sigs(sig_chs, all_corrs=()):\n",
    "    n_col = 1  # Only need one column for sensor locations\n",
    "    n_row = len(conditions)\n",
    "    figsize = (n_col * 2, n_row * 2)  # Increase figure size for better label visibility\n",
    "    fig, axes = plt.subplots(\n",
    "        n_row, n_col, figsize=figsize, constrained_layout=True)\n",
    "\n",
    "    # Handle the case of a single subplot\n",
    "    if n_row == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ci, condition in enumerate(conditions):\n",
    "        sigs = sig_chs[condition]\n",
    "        ax = axes[ci]  # Direct reference to each subplot's axis\n",
    "        \n",
    "        # Ensure labels are set even if there are no significant sensors\n",
    "        ax.set_ylabel(condition, labelpad=100)  # Increase labelpad if necessary\n",
    "\n",
    "        # Only attempt to plot sensor locations if there are significant sensors\n",
    "        if sigs[0][0] is not None:\n",
    "            _plot_sig_chs(sigs, ax)\n",
    "        else:\n",
    "            # Optionally, clear the axes but keep them to display the label\n",
    "            ax.clear()  # Clear the axes of any plotted data\n",
    "            ax.set_xticks([])  # Remove x-ticks\n",
    "            ax.set_yticks([])  # Remove y-ticks\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "    return fig\n",
    "\n",
    "times = evokeds[conditions[0]][subjects[8]].times\n",
    "info = evokeds[conditions[0]][subjects[8]].info\n",
    "fig = _plot_sigs(sig_chs)\n",
    "\n",
    "for ext in ('png', 'svg'):\n",
    "    fig.savefig(op.join(results_path, f'sensors_{exp_name}.{ext}'))\n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Source space projection\n",
    "import pyvista\n",
    "\n",
    "info = use.copy().pick_types(fnirs='hbo', exclude=()).info\n",
    "info['bads'] = []\n",
    "assert tuple(zs) == conditions\n",
    "\n",
    "evoked = mne.EvokedArray(np.array(list(zs.values())).T, info)\n",
    "picks = np.arange(len(evoked.ch_names))\n",
    "\n",
    "for ch in evoked.info['chs']:\n",
    "    assert ch['coord_frame'] == mne.io.constants.FIFF.FIFFV_COORD_HEAD\n",
    "stc = mne.stc_near_sensors(\n",
    "    evoked, trans='fsaverage', subject='fsaverage', mode='weighted',\n",
    "    distance=0.02, project=True, picks=picks, subjects_dir=subjects_dir)\n",
    "# Split channel indices by left lat, posterior, right lat:\n",
    "num_map = {name: str(ii) for ii, name in enumerate(evoked.ch_names)}\n",
    "evoked.copy().rename_channels(num_map) #.plot_sensors(show_names=True)\n",
    "view_map = [np.arange(19), np.arange(19, 33), np.arange(33, 52)]\n",
    "surf = mne.read_bem_surfaces(  # brain surface\n",
    "    f'{subjects_dir}/fsaverage/bem/fsaverage-5120-5120-5120-bem.fif', s_id=1)\n",
    "\n",
    "for ci, condition in enumerate(conditions):\n",
    "    this_sig = [v[0] // 2 for v in sig_chs[condition]]\n",
    "    #assert np.in1d(this_sig, np.arange(52)).all()\n",
    "    pos = np.array([info['chs'][idx]['loc'][:3] for idx in this_sig])\n",
    "    pos.shape = (-1, 3)  # can be empty\n",
    "    trans = mne.transforms._get_trans('fsaverage', 'head', 'mri')[0]\n",
    "    pos = mne.transforms.apply_trans(trans, pos)  # now in MRI coords\n",
    "    pos = mne.surface._project_onto_surface(pos, surf, project_rrs=True)[2]\n",
    "    # plot\n",
    "    brain = stc.plot(hemi='both', views=['lat', 'frontal', 'lat'],\n",
    "                    initial_time=evoked.times[ci], cortex='low_contrast',\n",
    "                    time_viewer=False, show_traces=False,\n",
    "                    surface='pial', smoothing_steps=0, size=(1200, 400),\n",
    "                    clim=dict(kind='value', pos_lims=[0., 1.25, 2.5]),\n",
    "                    colormap='RdBu_r', view_layout='horizontal',\n",
    "                    colorbar=(0, 1), time_label='', background='w',\n",
    "                    brain_kwargs=dict(units='m'),\n",
    "                    add_data_kwargs=dict(colorbar_kwargs=dict(\n",
    "                        title_font_size=24, label_font_size=24, n_labels=5,\n",
    "                        title='z score')), subjects_dir=subjects_dir)\n",
    "    brain.show_view('lat', hemi='lh', row=0, col=0)\n",
    "    brain.show_view(azimuth=270, elevation=90, row=0, col=1)\n",
    "    pl = brain.plotter\n",
    "    used = np.zeros(len(this_sig))\n",
    "    brain.show_view('lat', hemi='rh', row=0, col=2)\n",
    "    plt.imsave(\n",
    "        op.join(results_path, f'all_brain_{exp_name}_{condition}.png'), pl.image)\n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vtk==9.1.0 --> redid with conda --> 9.0.3 --> 9.0.3 ??\n",
    "# pyvistaqt==0.2.0 --> upgrade --> 0.11.1\n",
    "# pyvista==0.32.0 --> redid with pip --> 0.44.1\n",
    "# numpy 1.26.4 --> 1.19.5 --> 1.23.0\n",
    "# pip install vtk==9.1.0 --no-cache-dir\n",
    "# \"pip show X\" to see version // conda list vtk\n",
    "# conda install -c conda-forge vtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fOLD specificity\n",
    "import xlrd\n",
    "\n",
    "fold_files = ['10-10.xls', '10-5.xls']\n",
    "for fname in fold_files:\n",
    "    if not op.isfile(fname):\n",
    "        pooch.retrieve(f'https://github.com/nirx/fOLD-public/raw/master/Supplementary/{fname}', None, fname, path=os.getcwd())  # noqa\n",
    "raw_spec = use.copy()\n",
    "raw_spec.pick_channels(raw_spec.ch_names[::2])\n",
    "specs = mne_nirs.io.fold_channel_specificity(raw_spec, fold_files, 'Brodmann')\n",
    "for si, spec in enumerate(specs, 1):\n",
    "    spec['Channel'] = si\n",
    "    spec['negspec'] = -spec['Specificity']\n",
    "specs = pd.concat(specs, ignore_index=True)\n",
    "specs.drop(['Source', 'Detector', 'Distance (mm)', 'brainSens',\n",
    "            'X (mm)', 'Y (mm)', 'Z (mm)'], axis=1, inplace=True)\n",
    "specs.sort_values(['Channel', 'negspec'], inplace=True)\n",
    "specs.drop('negspec', axis=1, inplace=True)\n",
    "specs.reset_index(inplace=True, drop=True)\n",
    "specs.to_csv(op.join(results_path, 'specificity.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Condition  Chroma        ch_name  subject  day    group  \\\n",
      "0                  A     hbo    S10_D18 hbo      201    1  trained   \n",
      "1                  A     hbo    S10_D18 hbo      201    3  trained   \n",
      "2                 AV     hbo    S10_D18 hbo      201    1  trained   \n",
      "3                 AV     hbo    S10_D18 hbo      201    3  trained   \n",
      "4                  V     hbo    S10_D18 hbo      201    1  trained   \n",
      "...              ...     ...            ...      ...  ...      ...   \n",
      "53923              W  hbdiff  S9_D21 hbdiff      234    3  control   \n",
      "53924       constant  hbdiff  S9_D21 hbdiff      234    1  control   \n",
      "53925       constant  hbdiff  S9_D21 hbdiff      234    3  control   \n",
      "53926  short_channel  hbdiff  S9_D21 hbdiff      234    1  control   \n",
      "53927  short_channel  hbdiff  S9_D21 hbdiff      234    3  control   \n",
      "\n",
      "              theta  \n",
      "0     -2.686171e-07  \n",
      "1     -4.071789e-07  \n",
      "2      7.349320e-09  \n",
      "3      2.241841e-08  \n",
      "4      1.001392e-06  \n",
      "...             ...  \n",
      "53923 -1.568535e-07  \n",
      "53924  4.459984e-08  \n",
      "53925 -5.876935e-09  \n",
      "53926  3.464919e-01  \n",
      "53927  9.847671e-01  \n",
      "\n",
      "[53928 rows x 7 columns]\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# CALCULATE HbDIFF\n",
    "\n",
    "# Load the data\n",
    "df_cha = pd.read_csv(op.join(results_path, 'df_cha_theta_sdd.csv'))\n",
    "df_cha_nolabels = df_cha.copy()\n",
    "df_cha_nolabels['ch_name'] = df_cha_nolabels['ch_name'].str[:-4]\n",
    "\n",
    "# Separate HbO and HbR\n",
    "df_hbo = df_cha_nolabels[df_cha_nolabels['Chroma'].str.endswith('hbo')].set_index(['subject', 'Condition', 'group', 'day', 'ch_name']).sort_index()\n",
    "df_hbr = df_cha_nolabels[df_cha_nolabels['Chroma'].str.endswith('hbr')].set_index(['subject', 'Condition', 'group', 'day', 'ch_name']).sort_index()\n",
    "\n",
    "# Compute the difference\n",
    "df_cha_diff_list = []\n",
    "for ch_name in df_hbo.index.get_level_values('ch_name').unique():\n",
    "    # Get aligned indices\n",
    "    df_hbo_ch = df_hbo.loc[(slice(None), slice(None), slice(None), slice(None), ch_name), :].sort_index()\n",
    "    df_hbr_ch = df_hbr.loc[(slice(None), slice(None), slice(None), slice(None), ch_name), :].sort_index()\n",
    "    \n",
    "    # Ensure df_hbo_ch and df_hbr_ch have the same length\n",
    "    common_index = df_hbo_ch.index.intersection(df_hbr_ch.index)\n",
    "    df_hbo_ch = df_hbo_ch.loc[common_index]\n",
    "    df_hbr_ch = df_hbr_ch.loc[common_index]\n",
    "    \n",
    "    # Calculate the difference\n",
    "    df_diff = df_hbo_ch[['theta']].sub(df_hbr_ch[['theta']])\n",
    "    \n",
    "    # Align df_cha_ch with df_diff\n",
    "    df_cha_ch = df_hbo_ch.reset_index()\n",
    "    df_cha_ch['theta'] = df_diff.values\n",
    "    df_cha_ch['Chroma'] = 'hbdiff'\n",
    "    df_cha_ch['ch_name'] = df_cha_ch['ch_name'] + ' hbdiff'\n",
    "    \n",
    "    if not df_cha_ch.empty:\n",
    "        df_cha_diff_list.append(df_cha_ch)\n",
    "\n",
    "df_cha_diff_concat = pd.concat(df_cha_diff_list, ignore_index=True)\n",
    "\n",
    "# Concatenate original df_cha with df_cha_diff_concat\n",
    "df_final = pd.concat([df_cha, df_cha_diff_concat], ignore_index=True)\n",
    "df_final.to_csv(op.join(results_path, 'df_combined_final_cha_sdd.csv'), index=False)\n",
    "\n",
    "# Print the head of the resulting dataframe\n",
    "print(df_final)\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on float64 and object columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Combine with z-score data\u001b[39;00m\n\u001b[1;32m     70\u001b[0m z_scores \u001b[38;5;241m=\u001b[39m df_merged\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mch_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCondition\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m---> 71\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43mresults_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mch_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCondition\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Correct for multiple comparisons\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrecting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_value\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m comparisons using FDR\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:10093\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10074\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m  10075\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m  10076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10089\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  10090\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m  10091\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m> 10093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10094\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10102\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10103\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:110\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m--> 110\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:707\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    699\u001b[0m (\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[1;32m    703\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merge_keys()\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m--> 707\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1340\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1336\u001b[0m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1337\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1338\u001b[0m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1339\u001b[0m     ):\n\u001b[0;32m-> 1340\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on float64 and object columns. If you wish to proceed you should use pd.concat"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 18:21:10.142 python[22930:859093] NSKeyBindingManager: Bad key binding selector for '^z' = 'undo:'\n"
     ]
    }
   ],
   "source": [
    "# RUN T-TEST AND PLOT THE CHANGES OVER TIME\n",
    "\n",
    "# Source space projection - significant changes over time\n",
    "import pandas as pd\n",
    "import mne\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_rel, zscore\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from io import BytesIO\n",
    "\n",
    "raw_path = '../../data'\n",
    "proc_path = '../../processed'\n",
    "results_path = '../../results'\n",
    "subjects_dir = '../../subjects'\n",
    "\n",
    "groups = ['trained', 'control']\n",
    "chromas = ['hbo', 'hbr', 'hbdiff']\n",
    "conditions = ['A', 'V', 'AV']\n",
    "\n",
    "df_final = pd.read_csv(op.join(results_path, 'df_combined_final_cha_sdd.csv'))\n",
    "\n",
    "# Perform analysis for each group and Chroma\n",
    "for group in groups:\n",
    "    for chroma in chromas:\n",
    "        # Prepare figure for composite plots\n",
    "        fig, axes = plt.subplots(1, len(conditions), figsize=(15, 5))\n",
    "        for idx, condition in enumerate(conditions):\n",
    "            # Filter data for day 1 and day 3 for the specific group and Chroma\n",
    "            df_day1 = df_final.query(f\"group == '{group}' and Chroma == '{chroma}' and day == 1\").copy()\n",
    "            df_day3 = df_final.query(f\"group == '{group}' and Chroma == '{chroma}' and day == 3\").copy()\n",
    "\n",
    "            # Set index and sort\n",
    "            df_day1 = df_day1.set_index(['subject', 'ch_name', 'Condition', 'Chroma']).sort_index()\n",
    "            df_day3 = df_day3.set_index(['subject', 'ch_name', 'Condition', 'Chroma']).sort_index()\n",
    "            \n",
    "            # Merge dataframes to align day 1 and day 3 data\n",
    "            df_merged = df_day1[['theta']].rename(columns={'theta': 'theta_day1'}).merge(\n",
    "                df_day3[['theta']].rename(columns={'theta': 'theta_day3'}),\n",
    "                left_index=True, right_index=True)\n",
    "\n",
    "            # Calculate the difference and z-score\n",
    "            df_merged['theta_diff'] = df_merged['theta_day3'] - df_merged['theta_day1']\n",
    "            df_merged['z'] = zscore(df_merged['theta_diff'])\n",
    "\n",
    "            # Perform paired t-test for each channel and condition across subjects\n",
    "            t_stats = []\n",
    "            p_values = []\n",
    "            ch_names = []\n",
    "            condition_list = []\n",
    "\n",
    "            for (ch_name, cond), group_df in df_merged.groupby(['ch_name', 'Condition']):\n",
    "                t_stat, p_value = ttest_rel(group_df['theta_day1'], group_df['theta_day3'])\n",
    "                t_stats.append(t_stat)\n",
    "                p_values.append(p_value)\n",
    "                ch_names.append(ch_name)\n",
    "                condition_list.append(cond)\n",
    "\n",
    "            # Create a results DataFrame\n",
    "            results_df = pd.DataFrame({\n",
    "                'ch_name': ch_names,\n",
    "                'Condition': condition_list,\n",
    "                't_stat': t_stats,\n",
    "                'p_value': p_values\n",
    "            })\n",
    "\n",
    "            # Combine with z-score data\n",
    "            z_scores = df_merged.groupby(['ch_name', 'Condition'])['z'].mean().reset_index()\n",
    "            results_df = results_df.merge(z_scores, on=['ch_name', 'Condition'])\n",
    "\n",
    "            # Correct for multiple comparisons\n",
    "            print(f'Correcting for {len(results_df[\"p_value\"])} comparisons using FDR')\n",
    "            _, results_df['P_fdr'] = mne.stats.fdr_correction(results_df['p_value'], method='indep')\n",
    "            results_df['SIG'] = results_df['p_value'] < 0.05\n",
    "            \n",
    "            # Print significant results\n",
    "            significant_results = results_df.loc[results_df.SIG == True]\n",
    "            print(significant_results)\n",
    "\n",
    "            # Prepare data for brain plots\n",
    "            fname = op.join(proc_path, f'205_1_001_hbo_raw.fif')\n",
    "            use = mne.io.read_raw_fif(fname)\n",
    "            info = use.info\n",
    "\n",
    "            ch_of_interest = use.pick_channels([ch_name for ch_name in use.info['ch_names']])\n",
    "            info_of_interest = ch_of_interest.info\n",
    "\n",
    "            zs = {}\n",
    "            condition_data = results_df[(results_df['Condition'] == condition) & (results_df['SIG'] == True)]\n",
    "            \n",
    "            # Debugging prints\n",
    "            print(f\"\\nCondition: {condition}\")\n",
    "            print(f\"Group: {group}\")\n",
    "            print(f\"Chroma: {chroma}\")\n",
    "            print(f\"Condition Data:\\n{condition_data.head()}\")\n",
    "            \n",
    "            zs[condition] = np.array([\n",
    "                condition_data.loc[(condition_data['ch_name'] == ch_name) & (condition_data['SIG'] == True), 'z'].values[0]\n",
    "                if not condition_data.loc[(condition_data['ch_name'] == ch_name) & (condition_data['SIG'] == True), 'z'].empty\n",
    "                else 0\n",
    "                for ch_name in info_of_interest['ch_names']\n",
    "            ])\n",
    "            \n",
    "            # Create an EvokedArray for each condition\n",
    "            evoked = mne.EvokedArray(zs[condition][:, np.newaxis], info_of_interest)\n",
    "            picks = np.arange(len(info_of_interest['ch_names']))\n",
    "\n",
    "            stc = mne.stc_near_sensors(\n",
    "                evoked, trans='fsaverage', subject='fsaverage', mode='weighted',\n",
    "                distance=0.02, project=True, picks=picks, subjects_dir=subjects_dir)\n",
    "\n",
    "            # Plot the brain and capture the image in-memory\n",
    "            brain = stc.plot(hemi='both', views=['lat', 'frontal', 'lat'],\n",
    "                             cortex='low_contrast', time_viewer=False, show_traces=False,\n",
    "                             surface='pial', smoothing_steps=0, size=(1200, 400),\n",
    "                             clim=dict(kind='value', pos_lims=[0, 0.5, 1]),\n",
    "                             colormap='RdBu_r', view_layout='horizontal',\n",
    "                             colorbar=(0, 1), time_label='', background='w',\n",
    "                             brain_kwargs=dict(units='m'),\n",
    "                             add_data_kwargs=dict(colorbar_kwargs=dict(\n",
    "                                 title_font_size=24, label_font_size=20, n_labels=5,\n",
    "                                 title='z score')), subjects_dir=subjects_dir)\n",
    "            brain.show_view('lat', hemi='lh', row=0, col=0)\n",
    "            brain.show_view(azimuth=270, elevation=90, row=0, col=1)\n",
    "            brain.show_view('lat', hemi='rh', row=0, col=2)\n",
    "\n",
    "            # Capture the plot as an image in memory\n",
    "            screenshot = brain.screenshot(time_viewer=False)\n",
    "            brain.close()\n",
    "\n",
    "            # Display the image in the composite figure\n",
    "            ax = axes[idx]\n",
    "            ax.imshow(screenshot)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f'{group.capitalize()} - Condition {condition} ({chroma})', fontsize=18)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.savefig(op.join(results_path, f'{group}_{chroma}_composite_brain_plots_bp.png'))\n",
    "        plt.show()\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
