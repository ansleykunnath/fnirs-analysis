{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SJ Training fNIRS Data Analysis</h1>\n",
    "Written by Ansley Kunnath\n",
    "Updated July 29, 2024\n",
    "Python v3.9.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import PyQt5\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import pooch\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "#import pyvistaqt\n",
    "#import pyvista\n",
    "from os import path as op\n",
    "from itertools import compress\n",
    "from collections import defaultdict\n",
    "from scipy import signal, stats\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import mne_nirs.preprocessing\n",
    "import mne_nirs.statistics\n",
    "import mne_nirs.utils\n",
    "import mne_nirs.statistics\n",
    "import mne\n",
    "from mne.viz import plot_compare_evokeds\n",
    "from mne.preprocessing.nirs import tddr\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix, compute_regressor  \n",
    "\n",
    "# Open plots in new window\n",
    "%matplotlib qt \n",
    "\n",
    "#warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# Locate files\n",
    "\n",
    "subjects = ('201 202 203 204 205 206 207 208 209 212 213 214 215 216 217 218 219 221 223 224 225 226 228 229 230 231 234').split() #\n",
    "#subjects = ('201 205 207 208 209 212 213 215 216 217 218 219 224').split() #\n",
    "\n",
    "# Mapping of subjects to groups\n",
    "subject_to_group = {\n",
    "    201: \"trained\",\n",
    "    202: \"control\",\n",
    "    203: \"trained\",\n",
    "    204: \"control\",\n",
    "    205: \"control\",\n",
    "    206: \"control\",\n",
    "    207: \"trained\",\n",
    "    208: \"control\",\n",
    "    209: \"control\",\n",
    "    212: \"trained\",\n",
    "    213: \"trained\",\n",
    "    214: \"trained\",\n",
    "    215: \"control\",\n",
    "    216: \"trained\",\n",
    "    217: \"control\",\n",
    "    218: \"control\",\n",
    "    219: \"trained\",\n",
    "#    220: \"control\", #sampling rate = 2.40 Hz instead of 4.8\n",
    "    221: \"trained\",\n",
    "    223: \"trained\",\n",
    "    224: \"control\",\n",
    "    225: \"trained\",\n",
    "    226: \"control\",\n",
    "    228: \"trained\",\n",
    "    229: \"control\",\n",
    "    230: \"trained\",\n",
    "    231: \"trained\",\n",
    "    234: \"control\",\n",
    "}\n",
    "\n",
    "sfreq = 4.807692\n",
    "conditions = ('A', 'V', 'AV', 'W')\n",
    "groups = ('trained','control')\n",
    "days = ('1', '3')\n",
    "runs = (1, 2)\n",
    "\n",
    "condition_colors = dict(  # https://personal.sron.nl/~pault/data/colourschemes.pdf\n",
    "    A='#4477AA',  # sblue\n",
    "    AV='#CCBB44',  # yellow\n",
    "    V='#EE7733',  # orange\n",
    "    W='#AA3377',  # purple\n",
    ")\n",
    "exp_name = 'av'\n",
    "duration = 1.8\n",
    "design = 'event'\n",
    "plot_subject = '201'\n",
    "plot_day = 1\n",
    "plot_run = 1\n",
    "beh_title, beh_idx = 'AV', 0\n",
    "filt_kwargs = dict(\n",
    "    l_freq=0.02, l_trans_bandwidth=0.02,\n",
    "    h_freq=0.2, h_trans_bandwidth=0.02)\n",
    "run_h = True  # regenerate HbO/HbR\n",
    "n_jobs = 8  # for GLM\n",
    "\n",
    "# SET FOLDER LOCATIONS\n",
    "# I save the output files outside of the folder that's uploaded to Github\n",
    "current_directory = %pwd\n",
    "os.chdir(current_directory)\n",
    "\n",
    "raw_path = '../../data'\n",
    "proc_path = '../../processed'\n",
    "results_path = '../../results'\n",
    "subjects_dir = '../../subjects'\n",
    "os.makedirs(proc_path, exist_ok=True)\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "os.makedirs(subjects_dir, exist_ok=True)\n",
    "#mne.datasets.fetch_fsaverage(subjects_dir=subjects_dir, verbose=True)\n",
    "\n",
    "use = None\n",
    "all_sci = list()\n",
    "plt.rcParams['axes.titlesize'] = 8\n",
    "plt.rcParams['axes.labelsize'] = 8\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "got_bad = 0\n",
    "got_total = 0\n",
    "\n",
    "# Prep making bad channels report\n",
    "bad_channels_filename = op.join(results_path, 'bad_channels_report.csv')\n",
    "if not op.isfile(bad_channels_filename):\n",
    "    with open(bad_channels_filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Subject', 'Day', 'Run', 'Percent Bad'])\n",
    "\n",
    "def normalize_channel_names(channels_set):\n",
    "    return {name.split()[0] for name in channels_set}\n",
    "\n",
    "def add_bad_channel_entry(subject, day, run, percentage_bad):\n",
    "    with open(bad_channels_filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([subject, day, run, f'{percentage_bad:.2f}%'])\n",
    "\n",
    "# Sanity check for subjects\n",
    "subjects_check = {int(subject) for subject in subjects}\n",
    "subject_to_group_check = set(subject_to_group.keys())\n",
    "if subjects_check == subject_to_group_check:\n",
    "    print(\"All done!\") \n",
    "    del subjects_check\n",
    "    del subject_to_group_check\n",
    "else:\n",
    "    print(\"Error loading subject info\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load participant data\n",
    "\n",
    "#subjects = ('201 202').split() #for testing\n",
    "\n",
    "for subject in subjects:\n",
    "    for day in days:\n",
    "        for run in runs:\n",
    "            group = subject_to_group.get(int(subject), \"unknown\")\n",
    "            root1 = f'Day{day}'\n",
    "            root2 = f'{subject}_{day}'\n",
    "            root3 = f'*-*-*_{run:03d}'\n",
    "            fname_base = op.join(raw_path, root1, root2, root3)\n",
    "            fname = glob.glob(fname_base)\n",
    "            base = f'{subject}_{day}_{run:03d}'\n",
    "            base_pr = base.ljust(20)\n",
    "            # Save the plot subject\n",
    "            if (op.isfile(op.join(proc_path, f'{base}_hbo_raw.fif')) and subject == plot_subject and run == plot_run):\n",
    "                fname = op.join(proc_path, f'{subject}_{day}_{run:03d}_hbo_raw.fif')\n",
    "                use = mne.io.read_raw_fif(fname)\n",
    "                events, _ = mne.events_from_annotations(use)\n",
    "                ch_names = [ch_name.rstrip(' hbo') for ch_name in use.ch_names]\n",
    "                info = use.info\n",
    "                print(\"Saved plot file!\")\n",
    "            # Don't resave old subjects...\n",
    "            if not op.isfile(op.join(proc_path, f'{base}_hbo_raw.fif')):\n",
    "                raw_intensity = mne.io.read_raw_nirx(fname[0])\n",
    "                raw_od = mne.preprocessing.nirs.optical_density(\n",
    "                    raw_intensity, verbose='error')\n",
    "                # identify bad channels\n",
    "                peaks = np.ptp(raw_od.get_data('fnirs'), axis=-1)\n",
    "                flat_names = [\n",
    "                    raw_od.ch_names[f].split(' ')[0]\n",
    "                    for f in np.where(peaks < 0.001)[0]]\n",
    "                sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "                all_sci.extend(sci)\n",
    "                sci_mask = (sci < 0.25)\n",
    "                got = np.where(sci_mask)[0]\n",
    "                percentage_bad = (len(got) / len(raw_od.ch_names)) * 100\n",
    "                print(f'    Run {base_pr}')\n",
    "                # assign bads\n",
    "                assert raw_od.info['bads'] == []\n",
    "                bads = set(raw_od.ch_names[pick] for pick in got)\n",
    "                bads = bads | set(ch_name for ch_name in raw_od.ch_names\n",
    "                                if ch_name.split(' ')[0] in flat_names)\n",
    "                bads = sorted(bads)\n",
    "    #            raw_tddr = tddr(raw_od) # DON'T TDDR\n",
    "    #            raw_tddr_bp = raw_tddr.copy()\n",
    "    #            raw_tddr_bp = raw_tddr.copy().filter(**filt_kwargs) # DON'T BANDPASS FILTER\n",
    "                raw_tddr_bp = raw_od.copy() # new\n",
    "                raw_tddr_bp.info['bads'] = bads\n",
    "                picks = mne.pick_types(raw_tddr_bp.info, fnirs=True)\n",
    "                peaks = np.ptp(raw_tddr_bp.get_data(picks), axis=-1)\n",
    "                assert (peaks > 1e-5).all()\n",
    "                raw_tddr_bp.info['bads'] = [] \n",
    "                raw_h = mne.preprocessing.nirs.beer_lambert_law(raw_tddr_bp, 6.)\n",
    "                h_bads = [\n",
    "                    ch_name for ch_name in raw_h.ch_names\n",
    "                    if ch_name.split(' ')[0] in set(bad.split(' ')[0] for bad in bads)]\n",
    "                set_bads = set(bads) \n",
    "                set_h_bads = set(h_bads)\n",
    "                normalized_bads = normalize_channel_names(set_bads)\n",
    "                normalized_h_bads = normalize_channel_names(set_h_bads)\n",
    "                assert normalized_bads == normalized_h_bads\n",
    "    #            assert len(bads) == len(h_bads)\n",
    "                raw_h.info['bads'] = h_bads\n",
    "                raw_h.info._check_consistency()\n",
    "                picks = mne.pick_types(raw_h.info, fnirs=True)\n",
    "                peaks = np.ptp(raw_h.get_data(picks), axis=-1)\n",
    "                assert (peaks > 1e-9).all()  \n",
    "                # Interpolate bad channels (flat or low SCI)\n",
    "                raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n",
    "                raw_h_interp.save(op.join(proc_path, f'{base}_hbo_raw.fif'), \n",
    "                        overwrite=True)  \n",
    "                assert len(raw_h.ch_names) == len(raw_h_interp.ch_names)\n",
    "                # Save the bad channel data\n",
    "                add_bad_channel_entry(subject, day, run, percentage_bad)\n",
    "                del raw_intensity, raw_od, raw_tddr_bp, raw_h #, raw_tddr\n",
    "                del normalized_h_bads, normalized_bads, set_bads, set_h_bads\n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove patients with over 25% bad channels on average across days and runs\n",
    "\n",
    "subjects_to_remove = ['202', '203', '204', '206', '214', '221', '223', '226']\n",
    "\n",
    "# Initialize counters for each group\n",
    "removed_trained = 0\n",
    "removed_control = 0\n",
    "\n",
    "# Count and remove the subjects\n",
    "for subject in subjects_to_remove:\n",
    "    subject_int = int(subject)  # Convert to integer for dictionary key comparison\n",
    "    if subject_int in subject_to_group:\n",
    "        # Increment the appropriate counter based on the group of the subject\n",
    "        if subject_to_group[subject_int] == \"trained\":\n",
    "            removed_trained += 1\n",
    "        elif subject_to_group[subject_int] == \"control\":\n",
    "            removed_control += 1\n",
    "        # Remove the subject from the dictionary\n",
    "        subject_to_group.pop(subject_int, None)\n",
    "\n",
    "# Update the subjects list after counting the removed subjects\n",
    "subjects = [subject for subject in subjects if subject not in subjects_to_remove]\n",
    "\n",
    "# Output the results\n",
    "print(f'Removed {removed_trained} trained subjects.')\n",
    "print(f'Removed {removed_control} control subjects.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Channel example figure\n",
    "sfreq = 4.807692\n",
    "\n",
    "for subject in subjects:\n",
    "    for day in days:\n",
    "        for run in runs:\n",
    "            fname = op.join(proc_path, f'{subject}_{day}_{run:03d}_hbo_raw.fif')\n",
    "            raw_h = mne.io.read_raw_fif(fname)\n",
    "            events, _ = mne.events_from_annotations(raw_h)\n",
    "            #print(len(events))\n",
    "\n",
    "def _make_design(raw_h, design, subject=None, run=None, day=None, group=None):\n",
    "    annotations_to_remove = raw_h.annotations.description == '255.0'\n",
    "    raw_h.annotations.delete(annotations_to_remove)\n",
    "    events, _ = mne.events_from_annotations(raw_h)\n",
    "    rows_to_remove = events[:, -1] == 1\n",
    "    events = events[~rows_to_remove]\n",
    "    # mis-codings\n",
    "    if len(events)==101:\n",
    "        events = events[1:]\n",
    "    n_times = len(raw_h.times)\n",
    "    stim = np.zeros((n_times, 4))\n",
    "    events[:, 2] -= 1\n",
    "    assert len(events) == 100, len(events)\n",
    "    want = [0] + [25] * 4\n",
    "    count = np.bincount(events[:, 2])\n",
    "    assert np.array_equal(count, want), count\n",
    "    assert events.shape == (100, 3), events.shape\n",
    "    if design == 'block':\n",
    "        events = events[0::5]\n",
    "        duration = 20.\n",
    "        assert np.array_equal(np.bincount(events[:, 2]), [0] + [5] * 4)\n",
    "    else:\n",
    "        assert design == 'event'\n",
    "        assert len(events) == 100\n",
    "        duration = 1.8\n",
    "        assert events.shape == (100, 3)\n",
    "        events_r = events[:, 2].reshape(20, 5)\n",
    "        assert (events_r == events_r[:, :1]).all()\n",
    "        del events_r\n",
    "    idx = (events[:, [0, 2]] - [0, 1]).T\n",
    "    assert np.in1d(idx[1], np.arange(len(conditions))).all()\n",
    "    stim[tuple(idx)] = 1\n",
    "#    assert raw_h.info['sfreq'] == sfreq  # necessary for below logic to work\n",
    "    n_block = int(np.ceil(duration * sfreq))\n",
    "    stim = signal.fftconvolve(stim, np.ones((n_block, 1)), axes=0)[:n_times]\n",
    "    dm_events = pd.DataFrame({\n",
    "        'trial_type': [conditions[ii] for ii in idx[1]],\n",
    "        'onset': idx[0] / raw_h.info['sfreq'],\n",
    "        'duration': n_block / raw_h.info['sfreq']})\n",
    "    dm = make_first_level_design_matrix(\n",
    "        raw_h.times, dm_events, hrf_model='glover',\n",
    "        drift_model='polynomial', drift_order=0)\n",
    "    return stim, dm, events\n",
    "\n",
    "#mne.viz.plot_events(events)\n",
    "\n",
    "###############################################################################\n",
    "# Plot the design matrix and some raw traces\n",
    "\n",
    "#fig, axes = plt.subplots(2, 1, figsize=(6., 3), constrained_layout=True)\n",
    "# Design\n",
    "#ax = axes[0]\n",
    "#raw_h = use\n",
    "#stim, dm, _ = _make_design(raw_h, design)\n",
    "\n",
    "# for ci, condition in enumerate(conditions):\n",
    "#     color = condition_colors[condition]\n",
    "#     ax.fill_between(\n",
    "#         raw_h.times, stim[:, ci], 0, edgecolor='none', facecolor='k',\n",
    "#         alpha=0.5)\n",
    "#     model = dm[conditions[ci]].to_numpy()\n",
    "#     ax.plot(raw_h.times, model, ls='-', lw=1, color=color)\n",
    "#     x = raw_h.times[np.where(model > 0)[0][0]]\n",
    "#     ax.text(\n",
    "#         x + 10, 1.1, condition, color=color, fontweight='bold', ha='center')\n",
    "# ax.set(ylabel='Modeled\\noxyHb', xlabel='', xlim=raw_h.times[[0, -1]])\n",
    "\n",
    "# # HbO/HbR\n",
    "# ax = axes[1]\n",
    "# picks = [pi for pi, ch_name in enumerate(raw_h.ch_names)\n",
    "#          if 'S1_D2' in ch_name]\n",
    "# assert len(picks) == 2\n",
    "# fnirs_colors = dict(hbo='r', hbr='b')\n",
    "# #ylim = np.array([-1, 1])\n",
    "# for pi, pick in enumerate(picks):\n",
    "#     color = fnirs_colors[raw_h.ch_names[pick][-3:]]\n",
    "#     data = raw_h.get_data(pick)[0] * 1e6\n",
    "#     val = np.ptp(data)\n",
    "#     assert val > 0.01\n",
    "#     ax.plot(raw_h.times, data, color=color, lw=1.)\n",
    "# ax.set(xlabel='Time (s)', ylabel='μM',\n",
    "# #       ylim=ylim, \n",
    "#        xlim=raw_h.times[[0, -1]])\n",
    "# #del raw_h\n",
    "# for ax in axes:\n",
    "#     for key in ('top', 'right'):\n",
    "#         ax.spines[key].set_visible(False)\n",
    "# for ext in ('png', 'svg'):\n",
    "#     fig.savefig(\n",
    "#         op.join(\n",
    "#             results_path, f'figure_1_{exp_name}.{ext}'))\n",
    "\n",
    "print(\"All done!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Run GLM analysis and epoching\n",
    "\n",
    "#### CAN SKIP! ####\n",
    "\n",
    "sfreq = 4.807692050933838\n",
    "\n",
    "df_cha_list = []\n",
    "for day in days:\n",
    "    for subject in subjects:\n",
    "        group = subject_to_group.get(int(subject), \"unknown\")\n",
    "        subj_cha_list = []\n",
    "        t0 = time.time()\n",
    "        for run in runs:\n",
    "            print(f'Running GLM for {group} {subject} day {day} run {run:03d}... ', end='')\n",
    "            fname2 = op.join(proc_path, f'{subject}_{day}_{run:03d}_hbo_raw.fif')\n",
    "            raw_h = mne.io.read_raw_fif(fname2)\n",
    "            _, dm, _ = _make_design(raw_h, design, subject, run, day, group)\n",
    "            glm_est = mne_nirs.statistics.run_glm(\n",
    "                raw_h, dm, noise_model='ols', n_jobs=n_jobs)\n",
    "            cha = glm_est.to_dataframe()\n",
    "            cha['subject'] = subject\n",
    "            cha['run'] = run\n",
    "            cha['day'] = day\n",
    "            cha['group'] = group\n",
    "            subj_cha_list.append(cha)\n",
    "            del raw_h\n",
    "        subj_cha = pd.concat(subj_cha_list, ignore_index=True)\n",
    "        df_cha_list.append(subj_cha) # test\n",
    "        print(f'{time.time() - t0:0.1f} sec') # test\n",
    "        \n",
    "#            print(f'Writing HDF5 for subject {subject}, day {day}... ', end='')\n",
    "#            subj_cha.to_hdf(fname, key='subj_cha', mode='w')\n",
    "#            print(f'{time.time() - t0:0.1f} sec')\n",
    "#        df_cha_list.append(pd.read_hdf(fname))\n",
    "\n",
    "# Test\n",
    "df_cha = pd.concat(df_cha_list, ignore_index=True)\n",
    "df_cha.reset_index(drop=True, inplace=True)\n",
    "df_cha.to_csv(op.join(results_path, 'df_cha.csv'), index=False)\n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To skip above block, just run this:\n",
    "\n",
    "df_cha = pd.read_csv(op.join(results_path, 'df_cha.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block averages\n",
    "event_id = {condition: ci for ci, condition in enumerate(conditions, 1)}\n",
    "evokeds = {condition: dict() for condition in conditions}\n",
    "for day in days:\n",
    "    for subject in subjects:\n",
    "        fname = op.join(proc_path, f'{subject}_{day}_{exp_name}-ave.fif')\n",
    "        if not op.isfile(fname):\n",
    "            tmin, tmax = -2, 38\n",
    "            baseline = (None, 0)\n",
    "            t0 = time.time()\n",
    "            print(f'Creating block average for {subject} day {day}... ', end='')\n",
    "            raws = list()\n",
    "            events = list()\n",
    "            for run in runs:\n",
    "                fname2 = op.join(proc_path, f'{subject}_{day}_{run:03d}_hbo_raw.fif')\n",
    "                raw_h = mne.io.read_raw_fif(fname2)\n",
    "                events.append(_make_design(raw_h, 'block', subject, run)[2])\n",
    "                raws.append(raw_h)\n",
    "            bads = sorted(set(sum((r.info['bads'] for r in raws), [])))\n",
    "            for r in raws:\n",
    "                r.info['bads'] = bads\n",
    "            raw_h, events = mne.concatenate_raws(raws, events_list=events)\n",
    "            epochs = mne.Epochs(raw_h, events, event_id, tmin=tmin, tmax=tmax,\n",
    "                                baseline=baseline)\n",
    "            this_ev = [epochs[condition].average() for condition in conditions]\n",
    "            assert all(ev.nave > 0 for ev in this_ev)\n",
    "            mne.write_evokeds(fname, this_ev, overwrite=True)\n",
    "            print(f'{time.time() - t0:0.1f} sec')\n",
    "            for condition in conditions:\n",
    "                evokeds[condition][subject] = mne.read_evokeds(fname, condition)\n",
    "            print(f'Done for {group} {subject} day {day} run {run:03d}... ', end='')\n",
    "\n",
    "print(\"All done!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make life easier by combining across runs\n",
    "\n",
    "# Mark bad channels \n",
    "bad = dict()\n",
    "bb = dict()\n",
    "\n",
    "for day in days:\n",
    "    for subject in subjects:\n",
    "        for run in runs:\n",
    "            fname2 = op.join(proc_path, f'{subject}_{day}_{run:03d}_hbo_raw.fif')\n",
    "            this_info = mne.io.read_info(fname2)\n",
    "            bad_channels = [idx - 1 for idx in sorted(\n",
    "                this_info['ch_names'].index(bad) + 1 for bad in this_info['bads'])]\n",
    "            valid_indices = np.arange(len(use.ch_names))\n",
    "            bb = [b for b in bad_channels if b in valid_indices]\n",
    "            bad[(subject, run, day)] = bb\n",
    "        assert np.in1d(bad[(subject, run, day)], np.arange(len(use.ch_names))).all()  # noqa: E501\n",
    "\n",
    "bad_combo = dict()\n",
    "for day in days:\n",
    "    for (subject, run, day), bb in bad_channels:\n",
    "        bad_combo[subject] = sorted(set(bad_combo.get(subject, [])) | set(bb))\n",
    "bad = bad_combo\n",
    "#assert set(bad) == set(subjects)\n",
    "\n",
    "start = len(df_cha)\n",
    "n_drop = 0\n",
    "for day in days:\n",
    "    for (subject, run, day), bb in bad_channels:\n",
    "        if not len(bb):\n",
    "            continue\n",
    "        drop_names = [use.ch_names[b] for b in bb]\n",
    "        is_subject = (df_cha['subject'] == subject)\n",
    "        is_day = (df_cha['day'] == day)\n",
    "        assert len(is_subject) == len(df_cha)\n",
    "        is_day = (df_cha['day'] == day)\n",
    "        drop = df_cha.index[\n",
    "            is_subject &\n",
    "            is_day &\n",
    "            np.in1d(df_cha['ch_name'], drop_names)]\n",
    "        n_drop += len(drop)\n",
    "        if len(drop):\n",
    "            print(f'Dropping {len(drop)} for {subject} day {day}')  # {run}')\n",
    "            df_cha.drop(drop, inplace=True)\n",
    "end = len(df_cha)\n",
    "assert n_drop == start - end, (n_drop, start - end)\n",
    "\n",
    "# combine runs by averaging \n",
    "sorts = ['subject', 'ch_name', 'Chroma', 'Condition', 'group', 'day', 'run']\n",
    "df_cha.sort_values(\n",
    "    sorts, inplace=True)\n",
    "#assert (np.array(df_cha['run']).reshape(-1, 2) == runs).all()\n",
    "theta = np.array(df_cha['theta']).reshape(-1, len(runs)).mean(-1)\n",
    "df_cha.drop(\n",
    "    [col for col in df_cha.columns if col not in sorts[:-1]], axis='columns',\n",
    "    inplace=True)\n",
    "df_cha.reset_index(drop=True, inplace=True)\n",
    "df_cha = df_cha[::len(runs)]\n",
    "df_cha.reset_index(drop=True, inplace=True)\n",
    "df_cha['theta'] = theta\n",
    "df_cha.to_csv(op.join(results_path, 'df_cha_theta.csv'), index=False)\n",
    "\n",
    "print(\"All done!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed linear model\n",
    "def _mixed_df(ch_summary):\n",
    "    formula = \"theta ~ -1 + ch_name:Condition\" \n",
    "    ch_model = smf.mixedlm(\n",
    "        formula, ch_summary, groups=ch_summary[\"subject\"]).fit(method='powell')\n",
    "    ch_model_df = mne_nirs.statistics.statsmodels_to_results(ch_model)\n",
    "    ch_model_df['P>|z|'] = ch_model.pvalues\n",
    "    ch_model_df.drop([idx for idx in ch_model_df.index if '[constant]' in idx],\n",
    "                    inplace=True)\n",
    "    return ch_model_df\n",
    "\n",
    "# Make separate subject lists for trained and untrained (TEST)\n",
    "#trained_subjects = {'201 203 207 212 213 214 216 219 221 223'}\n",
    "#control_subjects = {'202 204 205 206 208 209 215 217 218 224'}\n",
    "\n",
    "# Run group level model and convert to dataframe\n",
    "ch_summary = df_cha.query(\"Chroma in ['hbo']\").copy()\n",
    "#ch_summary = df_cha.query(\"Chroma in ['hbo'] and group in ['trained'] and day in ['3']\").copy()\n",
    "ch_model_df = _mixed_df(ch_summary) \n",
    "ch_model_df.reset_index(inplace=True)\n",
    "\n",
    "# Correct for multiple comparisons\n",
    "print(f'Correcting for {len(ch_model_df[\"P>|z|\"])} comparisons using FDR')\n",
    "_, ch_model_df['P_fdr'] = mne.stats.fdr_correction(\n",
    "    ch_model_df['P>|z|'], method='indep')\n",
    "ch_model_df['SIG'] = ch_model_df['P_fdr'] < 0.05\n",
    "ch_model_df.to_csv(op.join(results_path, 'ch_model_corrected_hbo.csv'), index=False)\n",
    "ch_model_df.loc[ch_model_df.SIG == True]\n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP!!!\n",
    "# # Analyze HbR Data Separately\n",
    "\n",
    "# ch_summary_hbr = df_cha.query(\"Chroma in ['hbr']\").copy()\n",
    "# ch_summary_use_hbr = ch_summary_hbr.query(\n",
    "#     f\"subject in {use_subjects}\").copy()\n",
    "# ch_model_df_hbr = _mixed_df(ch_summary_use_hbr) \n",
    "# ch_model_df_hbr.reset_index(inplace=True)\n",
    "\n",
    "# # Correct for multiple comparisons\n",
    "# print(f'Correcting for {len(ch_model_df_hbr[\"P>|z|\"])} comparisons using FDR')\n",
    "# _, ch_model_df_hbr['P_fdr'] = mne.stats.fdr_correction(\n",
    "#     ch_model_df_hbr['P>|z|'], method='indep')\n",
    "# ch_model_df_hbr['SIG'] = ch_model_df_hbr['P_fdr'] < 0.05\n",
    "# ch_model_df_hbr.to_csv(op.join('ch_model_corrected_hbr.csv'), index=False)\n",
    "# ch_model_df_hbr.to_csv(op.join(results_path, 'ch_model_corrected_hbr.csv'), index=False)\n",
    "# ch_model_df_hbr.loc[ch_model_df_hbr.SIG == True]\n",
    "\n",
    "# print(\"All done!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Plot significant channels\n",
    "sig_chs = dict()\n",
    "zs = dict()\n",
    "for condition in conditions:\n",
    "    sig_df = ch_model_df[\n",
    "        (ch_model_df['P_fdr'] < 0.05) &\n",
    "        (ch_model_df['Condition'] == condition) &\n",
    "        (ch_model_df['ch_name'].isin(use.ch_names))\n",
    "        ]\n",
    "    sig_chs[(condition)] = sorted(\n",
    "        (use.ch_names.index(row[1]['ch_name']), row[1]['P_fdr'])\n",
    "        for row in sig_df.iterrows())\n",
    "    ch_model_df[ch_model_df['Condition'] == condition]\n",
    "    zs[condition] = np.array([\n",
    "        ch_model_df.loc[(ch_model_df['Condition'] == condition) & \n",
    "                        (ch_model_df['ch_name'] == ch_name), 'z'].iloc[0]\n",
    "        for ch_name in info['ch_names'][::2]], float)\n",
    "    assert zs[condition].shape == (84,)\n",
    "    assert np.isfinite(zs[condition]).all()\n",
    "\n",
    "def _plot_sig_chs(sigs, ax):\n",
    "    if sigs and isinstance(sigs[0], tuple):\n",
    "        sigs = [s[0] for s in sigs]\n",
    "    ch_groups = [sigs, np.setdiff1d(np.arange(info['nchan']), sigs)]\n",
    "    mne.viz.plot_sensors(\n",
    "        info, 'topomap', 'hbo', title='', axes=ax,\n",
    "        show_names=True, ch_groups=ch_groups)\n",
    "    ax.collections[0].set(lw=0)\n",
    "    c = ax.collections[0].get_facecolor()\n",
    "    c[(c[:, :3] == (0.5, 0, 0)).all(-1)] = (0., 0., 0., 0.1)\n",
    "    c[(c[:, :3] == (0, 0, 0.5)).all(-1)] = (0., 1., 0., 0.5)\n",
    "    ax.collections[0].set_facecolor(c)\n",
    "    ch_names = [info['ch_names'][idx] for idx in sigs]\n",
    "    texts = list(ax.texts)\n",
    "    got = []\n",
    "    for text in list(texts):\n",
    "        try:\n",
    "            idx = ch_names.index(text.get_text())\n",
    "        except ValueError:\n",
    "            text.remove()\n",
    "        else:\n",
    "            got.append(idx)\n",
    "            text.set_text(f'{sigs[idx] // 2 + 1}')\n",
    "            text.set(fontsize='xx-small', zorder=5, ha='center')\n",
    "    assert len(got) == len(sigs), (got, list(sigs))\n",
    "\n",
    "def _plot_sigs(sig_chs, all_corrs=()):\n",
    "    n_col = max(len(x) for x in sig_chs.values()) + 1\n",
    "    n_row = len(conditions)\n",
    "    figsize = (n_col * 1.0, n_row * 1.0)\n",
    "    fig, axes = plt.subplots(\n",
    "        n_row, n_col, figsize=figsize, constrained_layout=True, squeeze=False)\n",
    "    h_colors = {0: 'r', 1: 'b'}\n",
    "    xticks = [0, 10, 20, 30]\n",
    "    ylim = [-0.2, 0.3]\n",
    "    yticks = [-0.2, -0.1, 0, 0.1, 0.2, 0.3]\n",
    "    xlim = [times[0], 35]\n",
    "    ylim = np.array(ylim)\n",
    "    yticks = np.array(yticks)\n",
    "    for ci, condition in enumerate(conditions):\n",
    "        ii = 0\n",
    "        sigs = sig_chs[condition]\n",
    "        if len(sigs) == 0:\n",
    "            sigs = [(None, None)]\n",
    "        for ii, (ch_idx, ch_p) in enumerate(sigs):\n",
    "            ax = axes[ci, ii]\n",
    "            if ch_idx is not None:\n",
    "                for jj in range(2):  # HbO, HbR\n",
    "                    color = h_colors[jj]\n",
    "                    a = 1e6 * np.array(\n",
    "                        [evokeds[condition][subject].data[ch_idx + jj]\n",
    "                         for subject in use_subjects\n",
    "                         if ch_idx + jj not in bad.get(subject, [])], float)\n",
    "                    m = np.mean(a, axis=0)\n",
    "                    lower, upper = stats.t.interval(\n",
    "                        0.95, len(a) - 1, loc=m, scale=stats.sem(a, axis=0))\n",
    "                    ax.fill_between(\n",
    "                        times, lower, upper, facecolor=color,\n",
    "                        edgecolor='none', lw=0, alpha=0.25, zorder=3,\n",
    "                        clip_on=False)\n",
    "                    ax.plot(times, m, color=color, lw=1, zorder=4,\n",
    "                            clip_on=False)\n",
    "                # Correlations\n",
    "                this_df = ch_summary_use.query(\n",
    "                    f'ch_name == {repr(use.ch_names[ch_idx])} and '\n",
    "                    f'Chroma == \"hbo\" and '\n",
    "                    f'Condition == {repr(condition)}')\n",
    "                #assert 8 <= len(this_df) <= len(subjects), len(this_df)\n",
    "                a = np.array(this_df['theta'])\n",
    "                cs = list()\n",
    "                if len(cs):\n",
    "                    cs = [''] + cs\n",
    "                c = '\\n'.join(cs)\n",
    "                ax.text(times[-1], ylim[1],\n",
    "                        f'ch{ch_idx // 2 + 1}\\np={ch_p:0.5f}{c}',\n",
    "                        ha='right', va='top', fontsize='x-small')\n",
    "            ax.axvline(20, ls=':', color='0.5', zorder=2, lw=1)\n",
    "            ax.axhline(0, ls='-', color='k', zorder=2, lw=0.5)\n",
    "            ax.set(xticks=xticks, yticks=yticks)\n",
    "            ax.set(xlim=xlim, ylim=ylim)\n",
    "            for key in ('top', 'right'):\n",
    "                ax.spines[key].set_visible(False)\n",
    "            if ax.get_subplotspec().is_last_row():\n",
    "                ax.set(xlabel='Time (sec)')\n",
    "            else:\n",
    "                ax.set_xticklabels([''] * len(xticks))\n",
    "            if ax.get_subplotspec().is_first_col():\n",
    "                ax.set_ylabel(condition)\n",
    "            else:\n",
    "                ax.set_yticklabels([''] * len(yticks))\n",
    "            for key in ('top', 'right'):\n",
    "                ax.spines[key].set_visible(False)\n",
    "        for ii in range(ii + 1, n_col - 1):\n",
    "            fig.delaxes(axes[ci, ii])\n",
    "        # montage\n",
    "        ax = axes[ci, -1]\n",
    "        if sigs[0][0] is None:\n",
    "            fig.delaxes(ax)\n",
    "        else:\n",
    "            # plot montage\n",
    "            _plot_sig_chs(sigs, ax)\n",
    "    return fig\n",
    "\n",
    "times = evokeds[conditions[0]][subjects[0]].times\n",
    "info = evokeds[conditions[0]][subjects[0]].info\n",
    "fig = _plot_sigs(sig_chs)\n",
    "#for ext in ('png', 'svg'):\n",
    "#    fig.savefig(op.join(results_path, f'stats_{exp_name}.{ext}'))\n",
    "\n",
    "print(\"All done!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY PLOT SENSORS\n",
    "def _plot_sigs(sig_chs, all_corrs=()):\n",
    "    n_col = 1  # Only need one column for sensor locations\n",
    "    n_row = len(conditions)\n",
    "    figsize = (n_col * 2, n_row * 2)  # Increase figure size for better label visibility\n",
    "    fig, axes = plt.subplots(\n",
    "        n_row, n_col, figsize=figsize, constrained_layout=True)\n",
    "\n",
    "    # Handle the case of a single subplot\n",
    "    if n_row == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ci, condition in enumerate(conditions):\n",
    "        sigs = sig_chs[condition]\n",
    "        ax = axes[ci]  # Direct reference to each subplot's axis\n",
    "        \n",
    "        # Ensure labels are set even if there are no significant sensors\n",
    "        ax.set_ylabel(condition, labelpad=100)  # Increase labelpad if necessary\n",
    "\n",
    "        # Only attempt to plot sensor locations if there are significant sensors\n",
    "        if sigs[0][0] is not None:\n",
    "            _plot_sig_chs(sigs, ax)\n",
    "        else:\n",
    "            # Optionally, clear the axes but keep them to display the label\n",
    "            ax.clear()  # Clear the axes of any plotted data\n",
    "            ax.set_xticks([])  # Remove x-ticks\n",
    "            ax.set_yticks([])  # Remove y-ticks\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "    return fig\n",
    "\n",
    "times = evokeds[conditions[0]][subjects[0]].times\n",
    "info = evokeds[conditions[0]][subjects[0]].info\n",
    "fig = _plot_sigs(sig_chs)\n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Source space projection\n",
    "\n",
    "info = use.copy().pick_types(fnirs='hbo', exclude=()).info\n",
    "info['bads'] = []\n",
    "assert tuple(zs) == conditions\n",
    "\n",
    "evoked = mne.EvokedArray(np.array(list(zs.values())).T, info)\n",
    "picks = np.arange(len(evoked.ch_names))\n",
    "\n",
    "for ch in evoked.info['chs']:\n",
    "    assert ch['coord_frame'] == mne.io.constants.FIFF.FIFFV_COORD_HEAD\n",
    "stc = mne.stc_near_sensors(\n",
    "    evoked, trans='fsaverage', subject='fsaverage', mode='weighted',\n",
    "    distance=0.02, project=True, picks=picks, subjects_dir=subjects_dir)\n",
    "# Split channel indices by left lat, posterior, right lat:\n",
    "num_map = {name: str(ii) for ii, name in enumerate(evoked.ch_names)}\n",
    "evoked.copy().rename_channels(num_map) #.plot_sensors(show_names=True)\n",
    "view_map = [np.arange(19), np.arange(19, 33), np.arange(33, 52)]\n",
    "surf = mne.read_bem_surfaces(  # brain surface\n",
    "    f'{subjects_dir}/fsaverage/bem/fsaverage-5120-5120-5120-bem.fif', s_id=1)\n",
    "\n",
    "for ci, condition in enumerate(conditions):\n",
    "    this_sig = [v[0] // 2 for v in sig_chs[condition]]\n",
    "    #assert np.in1d(this_sig, np.arange(52)).all()\n",
    "    pos = np.array([info['chs'][idx]['loc'][:3] for idx in this_sig])\n",
    "    pos.shape = (-1, 3)  # can be empty\n",
    "    trans = mne.transforms._get_trans('fsaverage', 'head', 'mri')[0]\n",
    "    pos = mne.transforms.apply_trans(trans, pos)  # now in MRI coords\n",
    "    pos = mne.surface._project_onto_surface(pos, surf, project_rrs=True)[2]\n",
    "    # plot\n",
    "    brain = stc.plot(hemi='both', views=['lat', 'frontal', 'lat'],\n",
    "                    initial_time=evoked.times[ci], cortex='low_contrast',\n",
    "                    time_viewer=False, show_traces=False,\n",
    "                    surface='pial', smoothing_steps=0, size=(1200, 400),\n",
    "                    clim=dict(kind='value', pos_lims=[0., 1.25, 2.5]),\n",
    "                    colormap='RdBu_r', view_layout='horizontal',\n",
    "                    colorbar=(0, 1), time_label='', background='w',\n",
    "                    brain_kwargs=dict(units='m'),\n",
    "                    add_data_kwargs=dict(colorbar_kwargs=dict(\n",
    "                        title_font_size=24, label_font_size=24, n_labels=5,\n",
    "                        title='z score')), subjects_dir=subjects_dir)\n",
    "    brain.show_view('lat', hemi='lh', row=0, col=0)\n",
    "    brain.show_view(azimuth=270, elevation=90, row=0, col=1)\n",
    "    pl = brain.plotter\n",
    "    used = np.zeros(len(this_sig))\n",
    "    brain.show_view('lat', hemi='rh', row=0, col=2)\n",
    "    plt.imsave(\n",
    "        op.join(results_path, f'all_brain_{exp_name}_{condition}.png'), pl.image)\n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fOLD specificity\n",
    "fold_files = ['10-10.xls', '10-5.xls']\n",
    "for fname in fold_files:\n",
    "    if not op.isfile(fname):\n",
    "        pooch.retrieve(f'https://github.com/nirx/fOLD-public/raw/master/Supplementary/{fname}', None, fname, path=os.getcwd())  # noqa\n",
    "raw_spec = use['h'].copy()\n",
    "raw_spec.pick_channels(raw_spec.ch_names[::2])\n",
    "specs = mne_nirs.io.fold_channel_specificity(raw_spec, fold_files, 'Brodmann')\n",
    "for si, spec in enumerate(specs, 1):\n",
    "    spec['Channel'] = si\n",
    "    spec['negspec'] = -spec['Specificity']\n",
    "specs = pd.concat(specs, ignore_index=True)\n",
    "specs.drop(['Source', 'Detector', 'Distance (mm)', 'brainSens',\n",
    "            'X (mm)', 'Y (mm)', 'Z (mm)'], axis=1, inplace=True)\n",
    "specs.sort_values(['Channel', 'negspec'], inplace=True)\n",
    "specs.drop('negspec', axis=1, inplace=True)\n",
    "specs.reset_index(inplace=True, drop=True)\n",
    "specs.to_csv(op.join(results_path, 'specificity.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HbDiff Analysis\n",
    "\n",
    "df_cha = pd.read_csv(op.join(results_path, 'df_cha_theta.csv'))\n",
    "df_cha['ch_name'] = df_cha['ch_name'].str[:-4]\n",
    "\n",
    "# run twice for trained vs. control\n",
    "df_hbo = df_cha[df_cha['Chroma'].str.endswith('hbo')].set_index(['subject', 'Condition', 'group', 'day', 'ch_name']).sort_index()\n",
    "df_hbr = df_cha[df_cha['Chroma'].str.endswith('hbr')].set_index(['subject', 'Condition', 'group', 'day', 'ch_name']).sort_index()\n",
    "\n",
    "# Compute the difference\n",
    "df_cha_diff_list = []\n",
    "for ch_name in df_cha['ch_name'].unique():\n",
    "    df_cha_ch = df_cha[df_cha['ch_name'] == ch_name].copy()\n",
    "    if df_cha_ch.duplicated(subset=['subject', 'Condition', 'group', 'day']).any():\n",
    "        df_cha_ch.drop_duplicates(subset=['subject', 'Condition', 'group', 'day'], inplace=True)\n",
    "    df_diff = df_hbo[df_hbo.index.get_level_values('ch_name') == ch_name][['theta']].reset_index(drop=True).sub(\n",
    "        df_hbr[df_hbr.index.get_level_values('ch_name') == ch_name][['theta']].reset_index(drop=True)\n",
    "    )\n",
    "    df_cha_diff = df_cha_ch.reset_index(drop=True).copy()\n",
    "    df_cha_diff['theta'] = df_diff.values\n",
    "    if not df_cha_diff.empty:\n",
    "        df_cha_diff_list.append(df_cha_diff)\n",
    "\n",
    "df_cha_diff_concat = pd.concat(df_cha_diff_list, ignore_index=True)\n",
    "df_cha_diff_concat.to_csv(op.join(results_path, 'df_cha_HbDiff.csv'), index=False)  ###\n",
    "#print(df_cha_diff_concat)\n",
    "#print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ansle/opt/anaconda3/lib/python3.9/site-packages/statsmodels/regression/mixed_linear_model.py:2238: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting for 368 comparisons using FDR\n",
      "                                      Coef. Std.Err.         z     P>|z|  \\\n",
      "ch_name[S10_D18]:Condition[A]  1.789201e-08      0.0  0.127024  0.898921   \n",
      "ch_name[S10_D20]:Condition[A]  1.385316e-07      0.0  0.983506  0.325359   \n",
      "ch_name[S10_D22]:Condition[A]  1.574374e-07      0.0  1.117728  0.263683   \n",
      "ch_name[S10_D30]:Condition[A] -8.827109e-08      0.0 -0.407850  0.683384   \n",
      "ch_name[S11_D19]:Condition[A]  2.442712e-07      0.0  1.734205  0.082882   \n",
      "...                                     ...      ...       ...       ...   \n",
      "ch_name[S8_D20]:Condition[W]   1.252584e-07      0.0  0.889273  0.373856   \n",
      "ch_name[S9_D17]:Condition[W]  -7.341379e-09      0.0 -0.052120  0.958433   \n",
      "ch_name[S9_D19]:Condition[W]  -3.433277e-08      0.0 -0.243746  0.807428   \n",
      "ch_name[S9_D20]:Condition[W]  -5.735793e-08      0.0 -0.407213  0.683852   \n",
      "ch_name[S9_D21]:Condition[W]  -8.960140e-08      0.0 -0.636126  0.524694   \n",
      "\n",
      "                                 [0.025    0.975]  ch_name Condition  \\\n",
      "ch_name[S10_D18]:Condition[A]      -0.0       0.0  S10_D18         A   \n",
      "ch_name[S10_D20]:Condition[A]      -0.0       0.0  S10_D20         A   \n",
      "ch_name[S10_D22]:Condition[A]      -0.0       0.0  S10_D22         A   \n",
      "ch_name[S10_D30]:Condition[A] -0.000001       0.0  S10_D30         A   \n",
      "ch_name[S11_D19]:Condition[A]      -0.0  0.000001  S11_D19         A   \n",
      "...                                 ...       ...      ...       ...   \n",
      "ch_name[S8_D20]:Condition[W]       -0.0       0.0   S8_D20         W   \n",
      "ch_name[S9_D17]:Condition[W]       -0.0       0.0   S9_D17         W   \n",
      "ch_name[S9_D19]:Condition[W]       -0.0       0.0   S9_D19         W   \n",
      "ch_name[S9_D20]:Condition[W]       -0.0       0.0   S9_D20         W   \n",
      "ch_name[S9_D21]:Condition[W]       -0.0       0.0   S9_D21         W   \n",
      "\n",
      "                               Significant     P_fdr    SIG  \n",
      "ch_name[S10_D18]:Condition[A]        False  0.994473  False  \n",
      "ch_name[S10_D20]:Condition[A]        False  0.994473  False  \n",
      "ch_name[S10_D22]:Condition[A]        False  0.994473  False  \n",
      "ch_name[S10_D30]:Condition[A]        False  0.994473  False  \n",
      "ch_name[S11_D19]:Condition[A]        False  0.828205  False  \n",
      "...                                    ...       ...    ...  \n",
      "ch_name[S8_D20]:Condition[W]         False  0.994473  False  \n",
      "ch_name[S9_D17]:Condition[W]         False  0.994473  False  \n",
      "ch_name[S9_D19]:Condition[W]         False  0.994473  False  \n",
      "ch_name[S9_D20]:Condition[W]         False  0.994473  False  \n",
      "ch_name[S9_D21]:Condition[W]         False  0.994473  False  \n",
      "\n",
      "[368 rows x 11 columns]\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_cha = pd.read_csv(op.join(results_path, 'df_cha_HbDiff.csv'))\n",
    "\n",
    "# run twice for trained vs. control\n",
    "ch_model_df_diff = pd.DataFrame()\n",
    "#df_day1 = df_cha.query(\"Chroma in ['hbo'] and group == ['control'] and day in [1]\").copy()  ###\n",
    "#df_day3 = df_cha.query(\"Chroma in ['hbo'] and group == ['control'] and day in [3]\").copy()  ###\n",
    "df_day1 = df_cha.query(\"group == ['trained'] and day in [1]\").copy()  ###\n",
    "df_day3 = df_cha.query(\"group == ['trained'] and day in [3]\").copy()  ###\n",
    "\n",
    "df_day1 = df_day1.set_index(['subject', 'ch_name', 'Condition', 'Chroma']).sort_index()\n",
    "df_day3 = df_day3.set_index(['subject', 'ch_name', 'Condition', 'Chroma']).sort_index()\n",
    "\n",
    "# Compute the difference\n",
    "df_diff = df_day3[['theta']] - df_day1[['theta']]\n",
    "df_diff = df_diff.reset_index()\n",
    "ch_model_df_diff = df_diff.copy()\n",
    "\n",
    "def _mixed_df(ch_summary):\n",
    "    formula = \"theta ~ -1 + ch_name:Condition\" \n",
    "    ch_model = smf.mixedlm(\n",
    "        formula, ch_summary, groups=ch_summary[\"subject\"]).fit(method='powell')\n",
    "    ch_model_df = mne_nirs.statistics.statsmodels_to_results(ch_model)\n",
    "    ch_model_df['P>|z|'] = ch_model.pvalues\n",
    "    ch_model_df.drop([idx for idx in ch_model_df.index if '[constant]' in idx],\n",
    "                    inplace=True)\n",
    "    return ch_model_df\n",
    "\n",
    "# Correct for multiple comparisons\n",
    "ch_model_df_diff = _mixed_df(ch_model_df_diff) \n",
    "print(f'Correcting for {len(ch_model_df_diff[\"P>|z|\"])} comparisons using FDR')\n",
    "_, ch_model_df_diff['P_fdr'] = mne.stats.fdr_correction(\n",
    "    ch_model_df_diff['P>|z|'], method='indep')\n",
    "ch_model_df_diff['SIG'] = ch_model_df_diff['P_fdr'] < 0.05\n",
    "ch_model_df_diff.to_csv(op.join(results_path, 'ch_model_trained_diff_hbo.csv'), index=False)  ###\n",
    "ch_model_df_diff.loc[ch_model_df_diff.SIG == True]\n",
    "print(ch_model_df_diff)\n",
    "\n",
    "# Use the ch_model_df_diff for significant channels and z-scores\n",
    "ch_model_df = ch_model_df_diff.copy()\n",
    "ch_model_df = ch_model_df.loc[ch_model_df.SIG == True]\n",
    "#print(ch_model_df)\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../processed/201_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3994 =      0.000 ...   830.752 secs\n",
      "Ready.\n",
      "Projecting data from 84 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 6.1 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "    3 BEM surfaces found\n",
      "Using pyvistaqt 3d backend.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ansle/opt/anaconda3/lib/python3.9/site-packages/pyvista/plotting/plotting/__init__.py:23: PyVistaDeprecationWarning: The `pyvista.plotting.plotting` module has been deprecated. `_ALL_PLOTTERS` is now imported as: `from pyvista.plotting import _ALL_PLOTTERS`.\n",
      "  warnings.warn(\n",
      "2024-07-24 18:56:56.358 python[93290:5416551] NSKeyBindingManager: Bad key binding selector for '^z' = 'undo:'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# Source space projection - differences\n",
    "ch_model_df = pd.read_csv(op.join(results_path, 'ch_model_trained_diff_hbo.csv'))\n",
    "\n",
    "fname = op.join(proc_path, f'{plot_subject}_{plot_day}_001_hbo_raw.fif')\n",
    "use = mne.io.read_raw_fif(fname)\n",
    "info = use.info\n",
    "\n",
    "ch_of_interest = use.pick_channels([ch_name for ch_name in info['ch_names'] if ch_name.endswith('hbo')])\n",
    "ch_of_interest.rename_channels(lambda x: x[:-4] if x.endswith(' hbo') else x) ####\n",
    "info_of_interest = ch_of_interest.info\n",
    "\n",
    "zs = {}\n",
    "for condition in conditions:\n",
    "    condition_data = ch_model_df[ch_model_df['Condition'] == condition]\n",
    "#    zs[condition] = np.array([condition_data.loc[condition_data['ch_name'] == ch_name, 'z'].values[0] for ch_name in info_of_interest['ch_names']]) ###\n",
    "    zs[condition] = np.array([condition_data.loc[(condition_data['Condition'] == condition) & (condition_data['ch_name'] == ch_name), 'z'].iloc[0] for ch_name in info_of_interest['ch_names']], float)\n",
    "\n",
    "# Create an EvokedArray\n",
    "evoked = mne.EvokedArray(np.array(list(zs.values())).T, info_of_interest)\n",
    "picks = np.arange(len(info_of_interest['ch_names']))\n",
    "\n",
    "#info = use.copy().pick_types(fnirs='hbo', exclude=()).info ###\n",
    "#info['bads'] = []\n",
    "assert tuple(zs) == conditions\n",
    "\n",
    "stc = mne.stc_near_sensors(\n",
    "    evoked, trans='fsaverage', subject='fsaverage', mode='weighted',\n",
    "    distance=0.02, project=True, picks=picks, subjects_dir=subjects_dir)\n",
    "\n",
    "# Split channel indices by left lat, posterior, right lat:\n",
    "num_map = {name: str(ii) for ii, name in enumerate(info_of_interest['ch_names'])}\n",
    "evoked.copy().rename_channels(num_map) #.plot_sensors(show_names=True)\n",
    "view_map = [np.arange(19), np.arange(19, 33), np.arange(33, 52)]\n",
    "surf = mne.read_bem_surfaces(  # brain surface\n",
    "    f'{subjects_dir}/fsaverage/bem/fsaverage-5120-5120-5120-bem.fif', s_id=1)\n",
    "\n",
    "for ci, condition in enumerate(conditions):\n",
    "    trans = mne.transforms._get_trans('fsaverage', 'head', 'mri')[0]\n",
    "    brain = stc.plot(hemi='both', views=['lat', 'frontal', 'lat'],\n",
    "                    initial_time=evoked.times[ci], cortex='low_contrast',\n",
    "                    time_viewer=False, show_traces=False,\n",
    "                    surface='pial', smoothing_steps=0, size=(1200, 400),\n",
    "                    clim=dict(kind='value', pos_lims=[0, 1.5, 3]),\n",
    "                    colormap='RdBu_r', view_layout='horizontal',\n",
    "                    colorbar=(0, 1), time_label='', background='w',\n",
    "                    brain_kwargs=dict(units='m'),\n",
    "                    add_data_kwargs=dict(colorbar_kwargs=dict(\n",
    "                        title_font_size=14, label_font_size=12, n_labels=5,\n",
    "                        title='z score')), subjects_dir=subjects_dir)\n",
    "    brain.show_view('lat', hemi='lh', row=0, col=0)\n",
    "    brain.show_view(azimuth=270, elevation=90, row=0, col=1)\n",
    "    pl = brain.plotter\n",
    "    brain.show_view('lat', hemi='rh', row=0, col=2)\n",
    "    plt.imsave(\n",
    "        op.join(results_path, f'trained_HbDiff_diff_{condition}.png'), pl.image)  ###\n",
    "\n",
    "print(\"All done!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
