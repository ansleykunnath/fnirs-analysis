{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SJ Training fNIRS Data Analysis</h1>\n",
    "Written by Ansley Kunnath\n",
    "Updated July 29, 2024\n",
    "Python v3.9.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import PyQt5\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import pooch\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "#import pyvistaqt\n",
    "#import pyvista\n",
    "from itertools import compress\n",
    "from collections import defaultdict\n",
    "from scipy import signal, stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_rel, zscore\n",
    "import os.path as op\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import mne_nirs.preprocessing\n",
    "import mne_nirs.statistics\n",
    "import mne_nirs.utils\n",
    "import mne_nirs.statistics\n",
    "import mne\n",
    "from mne.viz import plot_compare_evokeds\n",
    "from mne.preprocessing.nirs import tddr\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix, compute_regressor  \n",
    "\n",
    "# Open plots in new window\n",
    "%matplotlib qt \n",
    "\n",
    "#warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n",
      "N=29\n"
     ]
    }
   ],
   "source": [
    "# Locate files\n",
    "\n",
    "subjects = ('201 202 203 204 205 206 207 208 209 212 213 214 215 216 217 218 219 221 223 224 225 226 228 229 230 231 232 233 234').split() #\n",
    "\n",
    "# Mapping of subjects to groups\n",
    "subject_to_group = {\n",
    "    201: \"trained\",\n",
    "    202: \"control\",\n",
    "    203: \"trained\",\n",
    "    204: \"control\",\n",
    "    205: \"control\",\n",
    "    206: \"control\",\n",
    "    207: \"trained\",\n",
    "    208: \"control\",\n",
    "    209: \"control\",\n",
    "    212: \"trained\",\n",
    "    213: \"trained\",\n",
    "    214: \"trained\",\n",
    "    215: \"control\",\n",
    "    216: \"trained\",\n",
    "    217: \"control\",\n",
    "    218: \"control\",\n",
    "    219: \"trained\",\n",
    "#    220: \"control\", #sampling rate = 2.40 Hz instead of 4.8\n",
    "    221: \"trained\",\n",
    "    223: \"trained\",\n",
    "    224: \"control\",\n",
    "    225: \"trained\",\n",
    "    226: \"control\",\n",
    "    228: \"trained\",\n",
    "    229: \"control\",\n",
    "    230: \"trained\",\n",
    "    231: \"trained\",\n",
    "    232: \"control\",\n",
    "    233: \"trained\",\n",
    "    234: \"control\",\n",
    "}\n",
    "\n",
    "sfreq = 4.807692\n",
    "conditions = ('A', 'V', 'AV', 'W')\n",
    "groups = ('trained','control')\n",
    "days = ('1', '3')\n",
    "runs = (1, 2)\n",
    "\n",
    "condition_colors = dict(  # https://personal.sron.nl/~pault/data/colourschemes.pdf\n",
    "    A='#4477AA',  # sblue\n",
    "    AV='#CCBB44',  # yellow\n",
    "    V='#EE7733',  # orange\n",
    "    W='#AA3377',  # purple\n",
    ")\n",
    "exp_name = 'av'\n",
    "duration = 1.8\n",
    "design = 'event'\n",
    "plot_subject = '205'\n",
    "plot_day = 1\n",
    "plot_run = 1\n",
    "beh_title, beh_idx = 'AV', 0\n",
    "filt_kwargs = dict(\n",
    "    l_freq=0.02, l_trans_bandwidth=0.02,\n",
    "    h_freq=0.2, h_trans_bandwidth=0.02) \n",
    "run_h = True  # regenerate HbO/HbR\n",
    "n_jobs = 4  # for GLM\n",
    "\n",
    "# SET FOLDER LOCATIONS\n",
    "# I save the output files outside of the folder that's uploaded to Github\n",
    "current_directory = %pwd\n",
    "os.chdir(current_directory)\n",
    "\n",
    "raw_path = '../../data'\n",
    "proc_path = '../../processed'\n",
    "results_path = '../../results'\n",
    "subjects_dir = '../../subjects'\n",
    "os.makedirs(proc_path, exist_ok=True)\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "os.makedirs(subjects_dir, exist_ok=True)\n",
    "#mne.datasets.fetch_fsaverage(subjects_dir=subjects_dir, verbose=True)\n",
    "\n",
    "use = None\n",
    "all_sci = list()\n",
    "plt.rcParams['axes.titlesize'] = 8\n",
    "plt.rcParams['axes.labelsize'] = 8\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "got_bad = 0\n",
    "got_total = 0\n",
    "\n",
    "# Prep making bad channels report\n",
    "bad_channels_filename = op.join(results_path, 'bad_channels_report.csv')\n",
    "if not op.isfile(bad_channels_filename):\n",
    "    with open(bad_channels_filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Subject', 'Day', 'Run', 'Percent Bad'])\n",
    "\n",
    "def normalize_channel_names(channels_set):\n",
    "    return {name.split()[0] for name in channels_set}\n",
    "\n",
    "def add_bad_channel_entry(subject, day, run, percentage_bad):\n",
    "    with open(bad_channels_filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([subject, day, run, f'{percentage_bad:.2f}%'])\n",
    "\n",
    "def piecewise_linear_detrend(raw, sfreq, segment_length):\n",
    "    data = raw.get_data()\n",
    "    n_channels, n_samples = data.shape\n",
    "    segment_samples = int(segment_length * sfreq)\n",
    "    \n",
    "    for ch in range(n_channels):\n",
    "        for start in range(0, n_samples, segment_samples):\n",
    "            end = min(start + segment_samples, n_samples)\n",
    "            segment = data[ch, start:end]\n",
    "            time = np.arange(start, end)\n",
    "            p = np.polyfit(time, segment, 1)\n",
    "            trend = np.polyval(p, time)\n",
    "            data[ch, start:end] -= trend    \n",
    "    raw._data = data\n",
    "    return raw\n",
    "\n",
    "# Sanity check for subjects\n",
    "subjects_check = {int(subject) for subject in subjects}\n",
    "subject_to_group_check = set(subject_to_group.keys())\n",
    "if subjects_check == subject_to_group_check:\n",
    "    print(\"All done!\") \n",
    "    print(\"N=\" + str(len(subjects)))\n",
    "    del subjects_check\n",
    "    del subject_to_group_check\n",
    "else:\n",
    "    print(\"Error loading subject info\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../data/Day1/201_1/2023-09-21_001\n",
      "    Run 201_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/201_1/2023-09-21_002\n",
      "    Run 201_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/201_3/2023-09-23_001\n",
      "    Run 201_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/201_3/2023-09-23_002\n",
      "    Run 201_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/201_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/202_1/2023-10-03_001\n",
      "    Run 202_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/202_1/2023-10-03_002\n",
      "    Run 202_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/202_3/2023-10-05_001\n",
      "    Run 202_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/202_3/2023-10-05_002\n",
      "    Run 202_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/202_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/203_1/2023-10-06_001\n",
      "    Run 203_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/203_1/2023-10-06_002\n",
      "    Run 203_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/203_3/2023-10-08_001\n",
      "    Run 203_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/203_3/2023-10-08_002\n",
      "    Run 203_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/203_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/204_1/2023-10-07_001\n",
      "    Run 204_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/204_1/2023-10-07_002\n",
      "    Run 204_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/204_3/2023-10-09_001\n",
      "    Run 204_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/204_3/2023-10-09_002\n",
      "    Run 204_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/204_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Saved plot file!\n",
      "Loading ../../data/Day1/205_1/2023-10-13_001\n",
      "    Run 205_1_001           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/205_1/2023-10-13_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 205_1_002           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Opening raw data file ../../processed/205_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3864 =      0.000 ...   803.712 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Saved plot file!\n",
      "Loading ../../data/Day3/205_3/2023-10-15_001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 205_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/205_3/2023-10-15_002\n",
      "    Run 205_3_002           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/205_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/206_1/2023-10-14_001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 206_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/206_1/2023-10-14_002\n",
      "    Run 206_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/206_3/2023-10-16_001\n",
      "    Run 206_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/206_3/2023-10-16_002\n",
      "    Run 206_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/206_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/207_1/2023-10-17_001\n",
      "    Run 207_1_001           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/207_1/2023-10-17_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 207_1_002           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/207_3/2023-10-19_001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 207_3_001           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/207_3/2023-10-19_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 207_3_002           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/207_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/208_1/2023-10-23_001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 208_1_001           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/208_1/2023-10-23_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 208_1_002           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/208_3/2023-10-25_001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 208_3_001           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/208_3/2023-10-25_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 208_3_002           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/208_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/209_1/2023-10-26_001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 209_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/209_1/2023-10-26_002\n",
      "    Run 209_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/209_3/2023-10-28_001\n",
      "    Run 209_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/209_3/2023-10-28_002\n",
      "    Run 209_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/209_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/212_1/2023-11-07_001\n",
      "    Run 212_1_001           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/212_1/2023-11-07_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 212_1_002           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_1_002_hbo_raw.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/212_3/2023-11-09_001\n",
      "    Run 212_3_001           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/212_3/2023-11-09_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 212_3_002           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/212_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/213_1/2023-12-04_001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 213_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/213_1/2023-12-04_002\n",
      "    Run 213_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/213_3/2023-12-06_001\n",
      "    Run 213_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/213_3/2023-12-06_002\n",
      "    Run 213_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/213_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/214_1/2023-11-04_001\n",
      "    Run 214_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/214_1/2023-11-04_002\n",
      "    Run 214_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/214_3/2023-11-06_001\n",
      "    Run 214_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/214_3/2023-11-06_002\n",
      "    Run 214_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/214_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/215_1/2023-12-11_001\n",
      "    Run 215_1_001           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/215_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/215_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/215_1/2023-12-11_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 215_1_002           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/215_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/215_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/215_3/2023-12-13_001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 215_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/215_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/215_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/215_3/2023-12-13_002\n",
      "    Run 215_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/215_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/215_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/216_1/2024-01-13_001\n",
      "    Run 216_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/216_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/216_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/216_1/2024-01-13_002\n",
      "    Run 216_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/216_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/216_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/216_3/2024-01-16_001\n",
      "    Run 216_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/216_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/216_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/216_3/2024-01-16_002\n",
      "    Run 216_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/216_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/216_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/217_1/2024-01-10_001\n",
      "    Run 217_1_001           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/217_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/217_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/217_1/2024-01-10_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 217_1_002           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/217_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/217_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/217_3/2024-01-12_001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 217_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/217_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/217_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/217_3/2024-01-12_002\n",
      "    Run 217_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/217_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/217_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/218_1/2024-01-26_001\n",
      "    Run 218_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/218_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/218_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/218_1/2024-01-26_002\n",
      "    Run 218_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/218_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/218_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/218_3/2024-01-28_001\n",
      "    Run 218_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/218_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/218_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/218_3/2024-01-28_002\n",
      "    Run 218_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/218_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/218_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/219_1/2024-01-29_001\n",
      "    Run 219_1_001           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/219_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/219_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/219_1/2024-01-29_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 219_1_002           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/219_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/219_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/219_3/2024-01-31_001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 219_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/219_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/219_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/219_3/2024-01-31_002\n",
      "    Run 219_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/219_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/219_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/221_1/2024-02-03_001\n",
      "    Run 221_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/221_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/221_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/221_1/2024-02-03_002\n",
      "    Run 221_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/221_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/221_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/221_3/2024-02-05_001\n",
      "    Run 221_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/221_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/221_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/221_3/2024-02-05_002\n",
      "    Run 221_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/221_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/221_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/223_1/2024-02-10_001\n",
      "    Run 223_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/223_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/223_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/223_1/2024-02-10_002\n",
      "    Run 223_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/223_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/223_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/223_3/2024-02-12_001\n",
      "    Run 223_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/223_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/223_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/223_3/2024-02-12_002\n",
      "    Run 223_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.6 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/223_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/223_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/224_1/2024-02-10_001\n",
      "    Run 224_1_001           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/224_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/224_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/224_1/2024-02-10_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 224_1_002           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/224_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/224_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/224_3/2024-02-12_001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 224_3_001           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/224_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/224_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/224_3/2024-02-12_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 224_3_002           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/224_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/224_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/225_1/2024-04-29_001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 225_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/225_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/225_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/225_1/2024-04-29_002\n",
      "    Run 225_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/225_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/225_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/225_3/2024-05-01_001\n",
      "    Run 225_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/225_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/225_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/225_3/2024-05-01_002\n",
      "    Run 225_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/225_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/225_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/226_1/2024-04-30_001\n",
      "    Run 226_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/226_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/226_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/226_1/2024-04-30_002\n",
      "    Run 226_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/226_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/226_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/226_3/2024-05-02_001\n",
      "    Run 226_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/226_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/226_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/226_3/2024-05-02_002\n",
      "    Run 226_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/226_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/226_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/228_1/2024-06-10_001\n",
      "    Run 228_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/228_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/228_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/228_1/2024-06-10_002\n",
      "    Run 228_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/228_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/228_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/228_3/2024-06-12_001\n",
      "    Run 228_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/228_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/228_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/228_3/2024-06-12_002\n",
      "    Run 228_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/228_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/228_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/229_1/2024-06-26_001\n",
      "    Run 229_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/229_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/229_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/229_1/2024-06-26_002\n",
      "    Run 229_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/229_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/229_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/229_3/2024-06-28_001\n",
      "    Run 229_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/229_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/229_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/229_3/2024-06-28_002\n",
      "    Run 229_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/229_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/229_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/230_1/2024-06-26_001\n",
      "    Run 230_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/230_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/230_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/230_1/2024-06-26_002\n",
      "    Run 230_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/230_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/230_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/230_3/2024-06-28_001\n",
      "    Run 230_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/230_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/230_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/230_3/2024-06-28_002\n",
      "    Run 230_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/230_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/230_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/231_1/2024-07-01_001\n",
      "    Run 231_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/231_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/231_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/231_1/2024-07-01_002\n",
      "    Run 231_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/231_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/231_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/231_3/2024-07-03_001\n",
      "    Run 231_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/231_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/231_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/231_3/2024-07-03_002\n",
      "    Run 231_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/231_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/231_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/232_1/2024-07-27_001\n",
      "    Run 232_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/232_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/232_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/232_1/2024-07-27_002\n",
      "    Run 232_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/232_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/232_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/232_3/2024-07-29_001\n",
      "    Run 232_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/232_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/232_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/232_3/2024-07-29_002\n",
      "    Run 232_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/232_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/232_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/233_1/2024-07-14_001\n",
      "    Run 233_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/233_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/233_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/233_1/2024-07-14_002\n",
      "    Run 233_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/233_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/233_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/233_3/2024-07-16_001\n",
      "    Run 233_3_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/233_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/233_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/233_3/2024-07-16_002\n",
      "    Run 233_3_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/233_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/233_3_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/234_1/2024-07-01_001\n",
      "    Run 234_1_001           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/234_1_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/234_1_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day1/234_1/2024-07-01_002\n",
      "    Run 234_1_002           \n",
      "Interpolating bad channels\n",
      "    Automatic origin fit: head of radius 93.7 mm\n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/234_1_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/234_1_002_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/234_3/2024-07-04_001\n",
      "    Run 234_3_001           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/234_3_001_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/234_3_001_hbo_raw.fif\n",
      "[done]\n",
      "Loading ../../data/Day3/234_3/2024-07-04_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 234_3_002           \n",
      "Overwriting existing file.\n",
      "Writing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/234_3_002_hbo_raw.fif\n",
      "Closing /Users/ansle/Documents/GitHub/fnirs-analysis/GithubCode/fnirs-analysis/../../processed/234_3_002_hbo_raw.fif\n",
      "[done]\n",
      "All done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/s4k5_3r51ml3q36bgbcdsgbr0000gn/T/ipykernel_6376/1050722460.py:72: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Load participant data\n",
    "\n",
    "#subjects = ('201 202').split() #for testing\n",
    "\n",
    "for subject in subjects:\n",
    "    for day in days:\n",
    "        for run in runs:\n",
    "            group = subject_to_group.get(int(subject), \"unknown\")\n",
    "            root1 = f'Day{day}'\n",
    "            root2 = f'{subject}_{day}'\n",
    "            root3 = f'*-*-*_{run:03d}'\n",
    "            fname_base = op.join(raw_path, root1, root2, root3)\n",
    "            fname = glob.glob(fname_base)\n",
    "            base = f'{subject}_{day}_{run:03d}'\n",
    "            base_pr = base.ljust(20)\n",
    "            # Save the plot subject\n",
    "            if (op.isfile(op.join(proc_path, f'{base}_hbo_raw.fif')) and subject == plot_subject and run == plot_run):\n",
    "                fname2 = op.join(proc_path, f'{subject}_{day}_{run:03d}_hbo_raw.fif')\n",
    "                use = mne.io.read_raw_fif(fname2)\n",
    "                events, _ = mne.events_from_annotations(use)\n",
    "                ch_names = [ch_name.rstrip(' hbo') for ch_name in use.ch_names]\n",
    "                info = use.info\n",
    "                print(\"Saved plot file!\")\n",
    "            # Don't resave old subjects... Uncomment the next line and indent everything below!\n",
    "            #if not op.isfile(op.join(proc_path, f'{base}_hbo_raw.fif')):\n",
    "            raw_intensity = mne.io.read_raw_nirx(fname[0])\n",
    "            raw_detrend = piecewise_linear_detrend(raw_intensity, sfreq, segment_length=60)\n",
    "            raw_od = mne.preprocessing.nirs.optical_density(\n",
    "                raw_detrend, verbose='error')\n",
    "            # identify bad channels\n",
    "            peaks = np.ptp(raw_od.get_data('fnirs'), axis=-1)\n",
    "            flat_names = [\n",
    "                raw_od.ch_names[f].split(' ')[0]\n",
    "                for f in np.where(peaks < 0.001)[0]]\n",
    "            sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "            all_sci.extend(sci)\n",
    "            sci_mask = (sci < 0.25)\n",
    "            got = np.where(sci_mask)[0]\n",
    "            percentage_bad = (len(got) / len(raw_od.ch_names)) * 100\n",
    "            print(f'    Run {base_pr}')\n",
    "            # assign bads\n",
    "            assert raw_od.info['bads'] == []\n",
    "            bads = set(raw_od.ch_names[pick] for pick in got)\n",
    "            bads = bads | set(ch_name for ch_name in raw_od.ch_names\n",
    "                            if ch_name.split(' ')[0] in flat_names)\n",
    "            bads = sorted(bads)\n",
    "            #raw_tddr = tddr(raw_od) # DON'T TDDR?\n",
    "            #raw_tddr_bp = raw_tddr.copy().filter(**filt_kwargs) # DON'T BANDPASS FILTER?\n",
    "            raw_tddr_bp = raw_od.copy() # Alternative Code\n",
    "            raw_tddr_bp.info['bads'] = bads\n",
    "            picks = mne.pick_types(raw_tddr_bp.info, fnirs=True)\n",
    "            peaks = np.ptp(raw_tddr_bp.get_data(picks), axis=-1)\n",
    "            assert (peaks > 1e-5).all()\n",
    "            raw_tddr_bp.info['bads'] = [] \n",
    "            raw_h = mne.preprocessing.nirs.beer_lambert_law(raw_tddr_bp, 6.)\n",
    "            h_bads = [\n",
    "                ch_name for ch_name in raw_h.ch_names\n",
    "                if ch_name.split(' ')[0] in set(bad.split(' ')[0] for bad in bads)]\n",
    "            set_bads = set(bads) \n",
    "            set_h_bads = set(h_bads)\n",
    "            normalized_bads = normalize_channel_names(set_bads)\n",
    "            normalized_h_bads = normalize_channel_names(set_h_bads)\n",
    "            assert normalized_bads == normalized_h_bads\n",
    "#            assert len(bads) == len(h_bads)\n",
    "            raw_h.info['bads'] = h_bads\n",
    "            raw_h.info._check_consistency()\n",
    "            picks = mne.pick_types(raw_h.info, fnirs=True)\n",
    "            peaks = np.ptp(raw_h.get_data(picks), axis=-1)\n",
    "            assert (peaks > 1e-9).all()  \n",
    "            # Interpolate bad channels (flat or low SCI)\n",
    "            raw_h_interp = raw_h.copy().interpolate_bads(reset_bads=True, method = dict(fnirs = 'nearest')) # TEST\n",
    "            raw_h_interp.save(op.join(proc_path, f'{base}_hbo_raw.fif'), \n",
    "                    overwrite=True)  \n",
    "            assert len(raw_h.ch_names) == len(raw_h_interp.ch_names)\n",
    "            # Save the bad channel data\n",
    "            add_bad_channel_entry(subject, day, run, percentage_bad)\n",
    "            del raw_intensity, raw_od, raw_tddr_bp, raw_h #, raw_tddr\n",
    "            del normalized_h_bads, normalized_bads, set_bads, set_h_bads\n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Plot bad channels\n",
    " \n",
    "bad_channels_df = pd.read_csv(bad_channels_filename)\n",
    "bad_channels_df['Percent Bad'] = bad_channels_df['Percent Bad'].str.rstrip('%').astype(float)\n",
    "bad_channels_df['Subject'] = bad_channels_df['Subject'].astype(str)\n",
    "\n",
    "# Sort the DataFrame by percentage_bad in ascending order\n",
    "bad_channels_df = bad_channels_df.groupby('Subject')['Percent Bad'].mean().reset_index()\n",
    "bad_channels_df = bad_channels_df.sort_values(by='Percent Bad')\n",
    "bad_channels_df['Group'] = bad_channels_df['Subject'].map(lambda x: subject_to_group[int(x)])\n",
    "\n",
    "# Sort the DataFrame by Percent Bad in ascending order\n",
    "colors = bad_channels_df['Group'].map({'control': 'gray', 'trained': 'lightblue'})\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(bad_channels_df['Subject'], bad_channels_df['Percent Bad'], color=colors)\n",
    "plt.axhline(30, color='red', linestyle='--')  # Horizontal line at 30%\n",
    "plt.xlabel('Subject', fontsize=14)\n",
    "plt.ylabel('Percent Bad', fontsize=14)\n",
    "plt.title('Percentage of Bad Channels per Participant', fontsize=14)\n",
    "plt.ylim(0, 100)  # Set y-axis from 0 to 100%\n",
    "plt.yticks(np.arange(0, 101, 10), [f'{i}%' for i in np.arange(0, 101, 10)])  # Set y-axis ticks from 0 to 100 with a step of 10 and add percentage signs\n",
    "\n",
    "output_filename = op.join(results_path, 'bad_channels_report_detrend.png')\n",
    "plt.savefig(output_filename, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 5 trained subjects.\n",
      "Removed 4 control subjects.\n",
      "Remaining trained subjects: 10\n",
      "Remaining control subjects: 10\n"
     ]
    }
   ],
   "source": [
    "# Remove patients with over 30% bad channels on average across days and runs\n",
    "\n",
    "subjects_to_remove = ['202', '203', '204', '206', '214', '221', '223', '226', '233'] #201, 209?\n",
    "\n",
    "# Initialize counters for each group\n",
    "removed_trained = 0\n",
    "removed_control = 0\n",
    "\n",
    "remaining_trained = 0\n",
    "remaining_control = 0\n",
    "\n",
    "# Count and remove the subjects\n",
    "for subject in subjects_to_remove:\n",
    "    subject_int = int(subject)  # Convert to integer for dictionary key comparison\n",
    "    if subject_int in subject_to_group:\n",
    "        # Increment the appropriate counter based on the group of the subject\n",
    "        if subject_to_group[subject_int] == \"trained\":\n",
    "            removed_trained += 1\n",
    "        elif subject_to_group[subject_int] == \"control\":\n",
    "            removed_control += 1\n",
    "        # Remove the subject from the dictionary\n",
    "        subject_to_group.pop(subject_int, None)\n",
    "\n",
    "# Update the subjects list after counting the removed subjects\n",
    "subjects = [subject for subject in subjects if subject not in subjects_to_remove]\n",
    "\n",
    "for group in subject_to_group.values():\n",
    "    if group == \"trained\":\n",
    "        remaining_trained += 1\n",
    "    elif group == \"control\":\n",
    "        remaining_control += 1\n",
    "\n",
    "# Output the results\n",
    "print(f'Removed {removed_trained} trained subjects.')\n",
    "print(f'Removed {removed_control} control subjects.')\n",
    "print(f'Remaining trained subjects: {remaining_trained}')\n",
    "print(f'Remaining control subjects: {remaining_control}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../processed/201_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3994 =      0.000 ...   830.752 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/201_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3838 =      0.000 ...   798.304 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/201_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/201_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/205_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 4933 =      0.000 ...  1026.064 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/205_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3864 =      0.000 ...   803.712 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/205_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3943 =      0.000 ...   820.144 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/207_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3929 =      0.000 ...   817.232 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '255.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/207_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3958 =      0.000 ...   823.264 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/207_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3927 =      0.000 ...   816.816 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/207_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/208_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/208_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3931 =      0.000 ...   817.648 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/208_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3971 =      0.000 ...   825.968 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/208_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3726 =      0.000 ...   775.008 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/209_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3966 =      0.000 ...   824.928 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/209_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3965 =      0.000 ...   824.720 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/209_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3965 =      0.000 ...   824.720 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/209_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/212_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3946 =      0.000 ...   820.768 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/212_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/212_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4047 =      0.000 ...   841.776 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/212_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3973 =      0.000 ...   826.384 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/213_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3972 =      0.000 ...   826.176 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/213_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3951 =      0.000 ...   821.808 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/213_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3989 =      0.000 ...   829.712 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/213_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3974 =      0.000 ...   826.592 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/215_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4361 =      0.000 ...   907.088 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/215_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3923 =      0.000 ...   815.984 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/215_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3934 =      0.000 ...   818.272 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/215_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3975 =      0.000 ...   826.800 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/216_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3936 =      0.000 ...   818.688 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/216_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3931 =      0.000 ...   817.648 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/216_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3940 =      0.000 ...   819.520 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/216_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3927 =      0.000 ...   816.816 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/217_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3930 =      0.000 ...   817.440 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/217_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3943 =      0.000 ...   820.144 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/217_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3922 =      0.000 ...   815.776 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/217_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3946 =      0.000 ...   820.768 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/218_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3968 =      0.000 ...   825.344 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/218_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3953 =      0.000 ...   822.224 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/218_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3951 =      0.000 ...   821.808 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/218_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3931 =      0.000 ...   817.648 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/219_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3969 =      0.000 ...   825.552 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/219_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3924 =      0.000 ...   816.192 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/219_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3934 =      0.000 ...   818.272 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/219_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3924 =      0.000 ...   816.192 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/224_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3924 =      0.000 ...   816.192 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/224_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3935 =      0.000 ...   818.480 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/224_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3952 =      0.000 ...   822.016 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/224_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/225_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3942 =      0.000 ...   819.936 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/225_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3939 =      0.000 ...   819.312 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/225_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/225_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3943 =      0.000 ...   820.144 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/228_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3978 =      0.000 ...   827.424 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/228_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3978 =      0.000 ...   827.424 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/228_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3992 =      0.000 ...   830.336 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/228_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3954 =      0.000 ...   822.432 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/229_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3956 =      0.000 ...   822.848 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/229_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3968 =      0.000 ...   825.344 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/229_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/229_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3934 =      0.000 ...   818.272 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/230_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/230_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3925 =      0.000 ...   816.400 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/230_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3986 =      0.000 ...   829.088 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/230_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 4007 =      0.000 ...   833.456 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/231_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3927 =      0.000 ...   816.816 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/231_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3937 =      0.000 ...   818.896 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/231_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3930 =      0.000 ...   817.440 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/231_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3949 =      0.000 ...   821.392 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/232_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3944 =      0.000 ...   820.352 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/232_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3935 =      0.000 ...   818.480 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/232_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3942 =      0.000 ...   819.936 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/232_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3946 =      0.000 ...   820.768 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/234_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3964 =      0.000 ...   824.512 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/234_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3938 =      0.000 ...   819.104 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/234_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3883 =      0.000 ...   807.664 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Opening raw data file ../../processed/234_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3956 =      0.000 ...   822.848 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Define make_design\n",
    "\n",
    "sfreq = 4.807692\n",
    "\n",
    "for subject in subjects:\n",
    "    for day in days:\n",
    "        for run in runs:\n",
    "            fname = op.join(proc_path, f'{subject}_{day}_{run:03d}_hbo_raw.fif')\n",
    "            raw_h = mne.io.read_raw_fif(fname)\n",
    "            events, _ = mne.events_from_annotations(raw_h)\n",
    "            #print(len(events))\n",
    "\n",
    "def _make_design(raw_h, design, subject=None, run=None, day=None, group=None):\n",
    "    annotations_to_remove = raw_h.annotations.description == '255.0'\n",
    "    raw_h.annotations.delete(annotations_to_remove)\n",
    "    events, _ = mne.events_from_annotations(raw_h)\n",
    "    rows_to_remove = events[:, -1] == 1\n",
    "    events = events[~rows_to_remove]\n",
    "    # mis-codings\n",
    "    if len(events)==101:\n",
    "        events = events[1:]\n",
    "    n_times = len(raw_h.times)\n",
    "    stim = np.zeros((n_times, 4))\n",
    "    events[:, 2] -= 1\n",
    "    assert len(events) == 100, len(events)\n",
    "    want = [0] + [25] * 4\n",
    "    count = np.bincount(events[:, 2])\n",
    "    assert np.array_equal(count, want), count\n",
    "    assert events.shape == (100, 3), events.shape\n",
    "    if design == 'block':\n",
    "        events = events[0::5]\n",
    "        duration = 20.\n",
    "        assert np.array_equal(np.bincount(events[:, 2]), [0] + [5] * 4)\n",
    "    else:\n",
    "        assert design == 'event'\n",
    "        assert len(events) == 100\n",
    "        duration = 1.8\n",
    "        assert events.shape == (100, 3)\n",
    "        events_r = events[:, 2].reshape(20, 5)\n",
    "        assert (events_r == events_r[:, :1]).all()\n",
    "        del events_r\n",
    "    idx = (events[:, [0, 2]] - [0, 1]).T\n",
    "    assert np.in1d(idx[1], np.arange(len(conditions))).all()\n",
    "    stim[tuple(idx)] = 1\n",
    "#    assert raw_h.info['sfreq'] == sfreq  # necessary for below logic to work\n",
    "    n_block = int(np.ceil(duration * sfreq))\n",
    "    stim = signal.fftconvolve(stim, np.ones((n_block, 1)), axes=0)[:n_times]\n",
    "    dm_events = pd.DataFrame({\n",
    "        'trial_type': [conditions[ii] for ii in idx[1]],\n",
    "        'onset': idx[0] / raw_h.info['sfreq'],\n",
    "        'duration': n_block / raw_h.info['sfreq']})\n",
    "    dm = make_first_level_design_matrix(\n",
    "        raw_h.times, dm_events, hrf_model='glover',\n",
    "        drift_model='polynomial', drift_order=0)\n",
    "    return stim, dm, events\n",
    "\n",
    "#mne.viz.plot_events(events)\n",
    "\n",
    "###############################################################################\n",
    "# Plot the design matrix and some raw traces\n",
    "\n",
    "#fig, axes = plt.subplots(2, 1, figsize=(6., 3), constrained_layout=True)\n",
    "# Design\n",
    "#ax = axes[0]\n",
    "#raw_h = use\n",
    "#stim, dm, _ = _make_design(raw_h, design)\n",
    "\n",
    "# for ci, condition in enumerate(conditions):\n",
    "#     color = condition_colors[condition]\n",
    "#     ax.fill_between(\n",
    "#         raw_h.times, stim[:, ci], 0, edgecolor='none', facecolor='k',\n",
    "#         alpha=0.5)\n",
    "#     model = dm[conditions[ci]].to_numpy()\n",
    "#     ax.plot(raw_h.times, model, ls='-', lw=1, color=color)\n",
    "#     x = raw_h.times[np.where(model > 0)[0][0]]\n",
    "#     ax.text(\n",
    "#         x + 10, 1.1, condition, color=color, fontweight='bold', ha='center')\n",
    "# ax.set(ylabel='Modeled\\noxyHb', xlabel='', xlim=raw_h.times[[0, -1]])\n",
    "\n",
    "# # HbO/HbR\n",
    "# ax = axes[1]\n",
    "# picks = [pi for pi, ch_name in enumerate(raw_h.ch_names)\n",
    "#          if 'S1_D2' in ch_name]\n",
    "# assert len(picks) == 2\n",
    "# fnirs_colors = dict(hbo='r', hbr='b')\n",
    "# #ylim = np.array([-1, 1])\n",
    "# for pi, pick in enumerate(picks):\n",
    "#     color = fnirs_colors[raw_h.ch_names[pick][-3:]]\n",
    "#     data = raw_h.get_data(pick)[0] * 1e6\n",
    "#     val = np.ptp(data)\n",
    "#     assert val > 0.01\n",
    "#     ax.plot(raw_h.times, data, color=color, lw=1.)\n",
    "# ax.set(xlabel='Time (s)', ylabel='μM',\n",
    "# #       ylim=ylim, \n",
    "#        xlim=raw_h.times[[0, -1]])\n",
    "# #del raw_h\n",
    "# for ax in axes:\n",
    "#     for key in ('top', 'right'):\n",
    "#         ax.spines[key].set_visible(False)\n",
    "# for ext in ('png', 'svg'):\n",
    "#     fig.savefig(\n",
    "#         op.join(\n",
    "#             results_path, f'figure_1_{exp_name}.{ext}'))\n",
    "\n",
    "print(\"All done!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GLM for trained 201 day 1 run 001... Opening raw data file ../../processed/201_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3994 =      0.000 ...   830.752 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 201 day 1 run 002... Opening raw data file ../../processed/201_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3838 =      0.000 ...   798.304 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 201 day 1.\n",
      "Running GLM for control 205 day 1 run 001... Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 205 day 1 run 002... Opening raw data file ../../processed/205_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 4933 =      0.000 ...  1026.064 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 205 day 1.\n",
      "Running GLM for trained 207 day 1 run 001... Opening raw data file ../../processed/207_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3929 =      0.000 ...   817.232 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 207 day 1 run 002... Opening raw data file ../../processed/207_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3958 =      0.000 ...   823.264 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 207 day 1.\n",
      "Running GLM for control 208 day 1 run 001... Opening raw data file ../../processed/208_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 208 day 1 run 002... Opening raw data file ../../processed/208_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3931 =      0.000 ...   817.648 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 208 day 1.\n",
      "Running GLM for control 209 day 1 run 001... Opening raw data file ../../processed/209_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3966 =      0.000 ...   824.928 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 209 day 1 run 002... Opening raw data file ../../processed/209_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3965 =      0.000 ...   824.720 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 209 day 1.\n",
      "Running GLM for trained 212 day 1 run 001... Opening raw data file ../../processed/212_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3946 =      0.000 ...   820.768 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 212 day 1 run 002... Opening raw data file ../../processed/212_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 212 day 1.\n",
      "Running GLM for trained 213 day 1 run 001... Opening raw data file ../../processed/213_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3972 =      0.000 ...   826.176 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 213 day 1 run 002... Opening raw data file ../../processed/213_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3951 =      0.000 ...   821.808 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 213 day 1.\n",
      "Running GLM for control 215 day 1 run 001... Opening raw data file ../../processed/215_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4361 =      0.000 ...   907.088 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 215 day 1 run 002... Opening raw data file ../../processed/215_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3923 =      0.000 ...   815.984 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 215 day 1.\n",
      "Running GLM for trained 216 day 1 run 001... Opening raw data file ../../processed/216_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3936 =      0.000 ...   818.688 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 216 day 1 run 002... Opening raw data file ../../processed/216_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3931 =      0.000 ...   817.648 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 216 day 1.\n",
      "Running GLM for control 217 day 1 run 001... Opening raw data file ../../processed/217_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3930 =      0.000 ...   817.440 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 217 day 1 run 002... Opening raw data file ../../processed/217_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3943 =      0.000 ...   820.144 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 217 day 1.\n",
      "Running GLM for control 218 day 1 run 001... Opening raw data file ../../processed/218_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3968 =      0.000 ...   825.344 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 218 day 1 run 002... Opening raw data file ../../processed/218_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3953 =      0.000 ...   822.224 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 218 day 1.\n",
      "Running GLM for trained 219 day 1 run 001... Opening raw data file ../../processed/219_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3969 =      0.000 ...   825.552 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 219 day 1 run 002... Opening raw data file ../../processed/219_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3924 =      0.000 ...   816.192 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 219 day 1.\n",
      "Running GLM for control 224 day 1 run 001... Opening raw data file ../../processed/224_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3924 =      0.000 ...   816.192 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 224 day 1 run 002... Opening raw data file ../../processed/224_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3935 =      0.000 ...   818.480 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 224 day 1.\n",
      "Running GLM for trained 225 day 1 run 001... Opening raw data file ../../processed/225_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3942 =      0.000 ...   819.936 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 225 day 1 run 002... Opening raw data file ../../processed/225_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3939 =      0.000 ...   819.312 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 225 day 1.\n",
      "Running GLM for trained 228 day 1 run 001... Opening raw data file ../../processed/228_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3978 =      0.000 ...   827.424 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 228 day 1 run 002... Opening raw data file ../../processed/228_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3978 =      0.000 ...   827.424 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 228 day 1.\n",
      "Running GLM for control 229 day 1 run 001... Opening raw data file ../../processed/229_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3956 =      0.000 ...   822.848 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 229 day 1 run 002... Opening raw data file ../../processed/229_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3968 =      0.000 ...   825.344 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 229 day 1.\n",
      "Running GLM for trained 230 day 1 run 001... Opening raw data file ../../processed/230_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 230 day 1 run 002... Opening raw data file ../../processed/230_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3925 =      0.000 ...   816.400 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 230 day 1.\n",
      "Running GLM for trained 231 day 1 run 001... Opening raw data file ../../processed/231_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3927 =      0.000 ...   816.816 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 231 day 1 run 002... Opening raw data file ../../processed/231_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3937 =      0.000 ...   818.896 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 231 day 1.\n",
      "Running GLM for control 232 day 1 run 001... Opening raw data file ../../processed/232_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3944 =      0.000 ...   820.352 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 232 day 1 run 002... Opening raw data file ../../processed/232_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3935 =      0.000 ...   818.480 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 232 day 1.\n",
      "Running GLM for control 234 day 1 run 001... Opening raw data file ../../processed/234_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3964 =      0.000 ...   824.512 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 234 day 1 run 002... Opening raw data file ../../processed/234_1_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3938 =      0.000 ...   819.104 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 234 day 1.\n",
      "Running GLM for trained 201 day 3 run 001... Opening raw data file ../../processed/201_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 201 day 3 run 002... Opening raw data file ../../processed/201_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 201 day 3.\n",
      "Running GLM for control 205 day 3 run 001... Opening raw data file ../../processed/205_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3864 =      0.000 ...   803.712 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 205 day 3 run 002... Opening raw data file ../../processed/205_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3943 =      0.000 ...   820.144 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 205 day 3.\n",
      "Running GLM for trained 207 day 3 run 001... Opening raw data file ../../processed/207_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3927 =      0.000 ...   816.816 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 207 day 3 run 002... Opening raw data file ../../processed/207_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 207 day 3.\n",
      "Running GLM for control 208 day 3 run 001... Opening raw data file ../../processed/208_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3971 =      0.000 ...   825.968 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 208 day 3 run 002... Opening raw data file ../../processed/208_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3726 =      0.000 ...   775.008 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 208 day 3.\n",
      "Running GLM for control 209 day 3 run 001... Opening raw data file ../../processed/209_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3965 =      0.000 ...   824.720 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 209 day 3 run 002... Opening raw data file ../../processed/209_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3941 =      0.000 ...   819.728 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 209 day 3.\n",
      "Running GLM for trained 212 day 3 run 001... Opening raw data file ../../processed/212_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4047 =      0.000 ...   841.776 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 212 day 3 run 002... Opening raw data file ../../processed/212_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3973 =      0.000 ...   826.384 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 212 day 3.\n",
      "Running GLM for trained 213 day 3 run 001... Opening raw data file ../../processed/213_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3989 =      0.000 ...   829.712 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 213 day 3 run 002... Opening raw data file ../../processed/213_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3974 =      0.000 ...   826.592 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 213 day 3.\n",
      "Running GLM for control 215 day 3 run 001... Opening raw data file ../../processed/215_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3934 =      0.000 ...   818.272 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 215 day 3 run 002... Opening raw data file ../../processed/215_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3975 =      0.000 ...   826.800 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 215 day 3.\n",
      "Running GLM for trained 216 day 3 run 001... Opening raw data file ../../processed/216_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3940 =      0.000 ...   819.520 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 216 day 3 run 002... Opening raw data file ../../processed/216_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3927 =      0.000 ...   816.816 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 216 day 3.\n",
      "Running GLM for control 217 day 3 run 001... Opening raw data file ../../processed/217_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3922 =      0.000 ...   815.776 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 217 day 3 run 002... Opening raw data file ../../processed/217_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3946 =      0.000 ...   820.768 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 217 day 3.\n",
      "Running GLM for control 218 day 3 run 001... Opening raw data file ../../processed/218_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3951 =      0.000 ...   821.808 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 218 day 3 run 002... Opening raw data file ../../processed/218_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3931 =      0.000 ...   817.648 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 218 day 3.\n",
      "Running GLM for trained 219 day 3 run 001... Opening raw data file ../../processed/219_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3934 =      0.000 ...   818.272 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 219 day 3 run 002... Opening raw data file ../../processed/219_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3924 =      0.000 ...   816.192 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 219 day 3.\n",
      "Running GLM for control 224 day 3 run 001... Opening raw data file ../../processed/224_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3952 =      0.000 ...   822.016 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 224 day 3 run 002... Opening raw data file ../../processed/224_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 224 day 3.\n",
      "Running GLM for trained 225 day 3 run 001... Opening raw data file ../../processed/225_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 225 day 3 run 002... Opening raw data file ../../processed/225_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3943 =      0.000 ...   820.144 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 225 day 3.\n",
      "Running GLM for trained 228 day 3 run 001... Opening raw data file ../../processed/228_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3992 =      0.000 ...   830.336 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 228 day 3 run 002... Opening raw data file ../../processed/228_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3954 =      0.000 ...   822.432 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 228 day 3.\n",
      "Running GLM for control 229 day 3 run 001... Opening raw data file ../../processed/229_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3945 =      0.000 ...   820.560 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 229 day 3 run 002... Opening raw data file ../../processed/229_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3934 =      0.000 ...   818.272 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 229 day 3.\n",
      "Running GLM for trained 230 day 3 run 001... Opening raw data file ../../processed/230_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3986 =      0.000 ...   829.088 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 230 day 3 run 002... Opening raw data file ../../processed/230_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 4007 =      0.000 ...   833.456 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 230 day 3.\n",
      "Running GLM for trained 231 day 3 run 001... Opening raw data file ../../processed/231_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3930 =      0.000 ...   817.440 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for trained 231 day 3 run 002... Opening raw data file ../../processed/231_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3949 =      0.000 ...   821.392 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 231 day 3.\n",
      "Running GLM for control 232 day 3 run 001... Opening raw data file ../../processed/232_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3942 =      0.000 ...   819.936 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 232 day 3 run 002... Opening raw data file ../../processed/232_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3946 =      0.000 ...   820.768 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 232 day 3.\n",
      "Running GLM for control 234 day 3 run 001... Opening raw data file ../../processed/234_3_001_hbo_raw.fif...\n",
      "    Range : 0 ... 3883 =      0.000 ...   807.664 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "Running GLM for control 234 day 3 run 002... Opening raw data file ../../processed/234_3_002_hbo_raw.fif...\n",
      "    Range : 0 ... 3956 =      0.000 ...   822.848 secs\n",
      "Ready.\n",
      "Used Annotations descriptions: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "***Finished processing subject 234 day 3.\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Run GLM analysis and epoching\n",
    "\n",
    "#### CAN SKIP! ####\n",
    "\n",
    "sfreq = 4.807692050933838\n",
    "\n",
    "df_cha_list = []\n",
    "for day in days:\n",
    "    for subject in subjects:\n",
    "        group = subject_to_group.get(int(subject), \"unknown\")\n",
    "        subj_cha_list = []\n",
    "        t0 = time.time()\n",
    "        fname = op.join(proc_path, f'{subject}_{day}_{exp_name}.h5')\n",
    "#        if not op.isfile(fname):\n",
    "        for run in runs:\n",
    "            fname2 = op.join(proc_path, f'{subject}_{day}_{run:03d}_hbo_raw.fif')\n",
    "            print(f'Running GLM for {group} {subject} day {day} run {run:03d}... ', end='')\n",
    "            raw_h = mne.io.read_raw_fif(fname2)\n",
    "            _, dm, _ = _make_design(raw_h, design, subject, run, day, group)\n",
    "            glm_est = mne_nirs.statistics.run_glm(\n",
    "                raw_h, dm, noise_model='ols', n_jobs=n_jobs)\n",
    "            cha = glm_est.to_dataframe()\n",
    "            cha['subject'] = subject\n",
    "            cha['run'] = run\n",
    "            cha['day'] = day\n",
    "            cha['group'] = group\n",
    "            subj_cha_list.append(cha) # test\n",
    "            del raw_h\n",
    "        df_subj_cha = pd.concat(subj_cha_list, ignore_index=True)\n",
    "        df_cha_list.append(df_subj_cha)\n",
    "        print(f'***Finished processing subject {subject} day {day}.')\n",
    "\n",
    "df_cha = pd.concat(df_cha_list, ignore_index=True)\n",
    "df_cha.reset_index(drop=True, inplace=True)\n",
    "df_cha.to_csv(op.join(results_path, 'df_cha_detrend.csv'), index=False)\n",
    "\n",
    "print(\"All done!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To skip above block, just run this:\n",
    "\n",
    "df_cha = pd.read_csv(op.join(results_path, 'df_cha.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make life easier by combining across runs\n",
    "\n",
    "# block averages\n",
    "event_id = {condition: ci for ci, condition in enumerate(conditions, 1)}\n",
    "evokeds = {condition: dict() for condition in conditions}\n",
    "for day in days:\n",
    "    for subject in subjects:\n",
    "        fname = op.join(proc_path, f'{subject}_{day}_{exp_name}-ave.fif')\n",
    "        if not op.isfile(fname):\n",
    "            tmin, tmax = -2, 38\n",
    "            baseline = (None, 0)\n",
    "            t0 = time.time()\n",
    "            print(f'Creating block average for {subject} day {day}... ', end='')\n",
    "            raws = list()\n",
    "            events = list()\n",
    "            for run in runs:\n",
    "                fname2 = op.join(proc_path, f'{subject}_{day}_{run:03d}_hbo_raw.fif')\n",
    "                raw_h = mne.io.read_raw_fif(fname2)\n",
    "                events.append(_make_design(raw_h, 'block', subject, run)[2])\n",
    "                raws.append(raw_h)\n",
    "            bads = sorted(set(sum((r.info['bads'] for r in raws), [])))\n",
    "            for r in raws:\n",
    "                r.info['bads'] = bads\n",
    "            raw_h, events = mne.concatenate_raws(raws, events_list=events)\n",
    "            epochs = mne.Epochs(raw_h, events, event_id, tmin=tmin, tmax=tmax,\n",
    "                                baseline=baseline)\n",
    "            this_ev = [epochs[condition].average() for condition in conditions]\n",
    "            assert all(ev.nave > 0 for ev in this_ev)\n",
    "            mne.write_evokeds(fname, this_ev, overwrite=True)\n",
    "            print(f'{time.time() - t0:0.1f} sec')\n",
    "            for condition in conditions:\n",
    "                evokeds[condition][subject] = mne.read_evokeds(fname, condition)\n",
    "            print(f'Done for {group} {subject} day {day} run {run:03d}... ', end='')\n",
    "\n",
    "# Mark bad channels \n",
    "bad = dict()\n",
    "bb = dict()\n",
    "\n",
    "for day in days:\n",
    "    for subject in subjects:\n",
    "        for run in runs:\n",
    "            fname2 = op.join(proc_path, f'{subject}_{day}_{run:03d}_hbo_raw.fif')\n",
    "            this_info = mne.io.read_info(fname2)\n",
    "            bad_channels = [idx - 1 for idx in sorted(\n",
    "                this_info['ch_names'].index(bad) + 1 for bad in this_info['bads'])]\n",
    "            valid_indices = np.arange(len(use.ch_names))\n",
    "            bb = [b for b in bad_channels if b in valid_indices]\n",
    "            bad[(subject, run, day)] = bb\n",
    "        assert np.in1d(bad[(subject, run, day)], np.arange(len(use.ch_names))).all()  # noqa: E501\n",
    "\n",
    "bad_combo = dict()\n",
    "for day in days:\n",
    "    for (subject, run, day), bb in bad_channels:\n",
    "        bad_combo[subject] = sorted(set(bad_combo.get(subject, [])) | set(bb))\n",
    "bad = bad_combo\n",
    "#assert set(bad) == set(subjects)\n",
    "\n",
    "start = len(df_cha)\n",
    "n_drop = 0\n",
    "for day in days:\n",
    "    for (subject, run, day), bb in bad_channels:\n",
    "        if not len(bb):\n",
    "            continue\n",
    "        drop_names = [use.ch_names[b] for b in bb]\n",
    "        is_subject = (df_cha['subject'] == subject)\n",
    "        is_day = (df_cha['day'] == day)\n",
    "        assert len(is_subject) == len(df_cha)\n",
    "        is_day = (df_cha['day'] == day)\n",
    "        drop = df_cha.index[\n",
    "            is_subject &\n",
    "            is_day &\n",
    "            np.in1d(df_cha['ch_name'], drop_names)]\n",
    "        n_drop += len(drop)\n",
    "        if len(drop):\n",
    "            print(f'Dropping {len(drop)} for {subject} day {day}')  # {run}')\n",
    "            df_cha.drop(drop, inplace=True)\n",
    "end = len(df_cha)\n",
    "assert n_drop == start - end, (n_drop, start - end)\n",
    "\n",
    "# combine runs by averaging \n",
    "sorts = ['subject', 'ch_name', 'Chroma', 'Condition', 'group', 'day', 'run']\n",
    "df_cha.sort_values(\n",
    "    sorts, inplace=True)\n",
    "theta = np.array(df_cha['theta']).reshape(-1, len(runs)).mean(-1)\n",
    "df_cha.drop(\n",
    "    [col for col in df_cha.columns if col not in sorts[:-1]], axis='columns',\n",
    "    inplace=True)\n",
    "df_cha.reset_index(drop=True, inplace=True)\n",
    "df_cha = df_cha[::len(runs)]\n",
    "df_cha.reset_index(drop=True, inplace=True)\n",
    "df_cha['theta'] = theta\n",
    "df_cha.to_csv(op.join(results_path, 'df_cha_theta_detrend.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS FOR CHANNEL x CONDITION ANALYSIS... OPTIONAL\n",
    "\n",
    "# Mixed linear model\n",
    "def _mixed_df(ch_summary):\n",
    "    formula = \"theta ~ -1 + ch_name:Condition\" \n",
    "    ch_model = smf.mixedlm(\n",
    "        formula, ch_summary, groups=ch_summary[\"subject\"]).fit(method='powell')\n",
    "    ch_model_df = mne_nirs.statistics.statsmodels_to_results(ch_model)\n",
    "    ch_model_df['P>|z|'] = ch_model.pvalues\n",
    "    ch_model_df.drop([idx for idx in ch_model_df.index if '[constant]' in idx],\n",
    "                    inplace=True)\n",
    "    return ch_model_df\n",
    "\n",
    "print(\"All done!\") \n",
    "\n",
    "# Make separate subject lists for trained and untrained (TEST)\n",
    "#trained_subjects = {'201 203 207 212 213 214 216 219 221 223'}\n",
    "#control_subjects = {'202 204 205 206 208 209 215 217 218 224'}\n",
    "\n",
    "# Run group level model and convert to dataframe\n",
    "ch_summary = df_cha.query(\"Chroma in ['hbo']\").copy()   ### ALSO RUN HBR ANALYSIS\n",
    "#ch_summary = df_cha.query(\"Chroma in ['hbo'] and group in ['trained'] and day in ['3']\").copy()\n",
    "ch_model_df = _mixed_df(ch_summary) \n",
    "ch_model_df.reset_index(inplace=True)\n",
    "\n",
    "# Correct for multiple comparisons\n",
    "print(f'Correcting for {len(ch_model_df[\"P>|z|\"])} comparisons using FDR')\n",
    "_, ch_model_df['P_fdr'] = mne.stats.fdr_correction(\n",
    "    ch_model_df['P>|z|'], method='indep')\n",
    "ch_model_df['SIG'] = ch_model_df['P_fdr'] < 0.05\n",
    "ch_model_df.to_csv(op.join(results_path, 'ch_model_corrected_hbo.csv'), index=False)\n",
    "ch_model_df.loc[ch_model_df.SIG == True]\n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Plot significant channels\n",
    "\n",
    "sig_chs = dict()\n",
    "zs = dict()\n",
    "for condition in conditions:\n",
    "    sig_df = ch_model_df[\n",
    "        (ch_model_df['P_fdr'] < 0.05) &\n",
    "        (ch_model_df['Condition'] == condition) &\n",
    "        (ch_model_df['ch_name'].isin(use.ch_names))\n",
    "        ]\n",
    "    sig_chs[(condition)] = sorted(\n",
    "        (use.ch_names.index(row[1]['ch_name']), row[1]['P_fdr'])\n",
    "        for row in sig_df.iterrows())\n",
    "    ch_model_df[ch_model_df['Condition'] == condition]\n",
    "    zs[condition] = np.array([\n",
    "        ch_model_df.loc[(ch_model_df['Condition'] == condition) & \n",
    "                        (ch_model_df['ch_name'] == ch_name), 'z'].iloc[0]\n",
    "        for ch_name in info['ch_names'][::2]], float)\n",
    "    assert zs[condition].shape == (84,)\n",
    "    assert np.isfinite(zs[condition]).all()\n",
    "\n",
    "def _plot_sig_chs(sigs, ax):\n",
    "    if sigs and isinstance(sigs[0], tuple):\n",
    "        sigs = [s[0] for s in sigs]\n",
    "    ch_groups = [sigs, np.setdiff1d(np.arange(info['nchan']), sigs)]\n",
    "    mne.viz.plot_sensors(\n",
    "        info, 'topomap', 'hbo', title='', axes=ax,\n",
    "        show_names=True, ch_groups=ch_groups)\n",
    "    ax.collections[0].set(lw=0)\n",
    "    c = ax.collections[0].get_facecolor()\n",
    "    c[(c[:, :3] == (0.5, 0, 0)).all(-1)] = (0., 0., 0., 0.1)\n",
    "    c[(c[:, :3] == (0, 0, 0.5)).all(-1)] = (0., 1., 0., 0.5)\n",
    "    ax.collections[0].set_facecolor(c)\n",
    "    ch_names = [info['ch_names'][idx] for idx in sigs]\n",
    "    texts = list(ax.texts)\n",
    "    got = []\n",
    "    for text in list(texts):\n",
    "        try:\n",
    "            idx = ch_names.index(text.get_text())\n",
    "        except ValueError:\n",
    "            text.remove()\n",
    "        else:\n",
    "            got.append(idx)\n",
    "            text.set_text(f'{sigs[idx] // 2 + 1}')\n",
    "            text.set(fontsize='xx-small', zorder=5, ha='center')\n",
    "    assert len(got) == len(sigs), (got, list(sigs))\n",
    "\n",
    "def _plot_sigs(sig_chs, all_corrs=()):\n",
    "    n_col = max(len(x) for x in sig_chs.values()) + 1\n",
    "    n_row = len(conditions)\n",
    "    figsize = (n_col * 1.0, n_row * 1.0)\n",
    "    fig, axes = plt.subplots(\n",
    "        n_row, n_col, figsize=figsize, constrained_layout=True, squeeze=False)\n",
    "    h_colors = {0: 'r', 1: 'b'}\n",
    "    xticks = [0, 10, 20, 30]\n",
    "    ylim = [-0.2, 0.3]\n",
    "    yticks = [-0.2, -0.1, 0, 0.1, 0.2, 0.3]\n",
    "    xlim = [times[0], 35]\n",
    "    ylim = np.array(ylim)\n",
    "    yticks = np.array(yticks)\n",
    "    for ci, condition in enumerate(conditions):\n",
    "        ii = 0\n",
    "        sigs = sig_chs[condition]\n",
    "        if len(sigs) == 0:\n",
    "            sigs = [(None, None)]\n",
    "        for ii, (ch_idx, ch_p) in enumerate(sigs):\n",
    "            ax = axes[ci, ii]\n",
    "            if ch_idx is not None:\n",
    "                for jj in range(2):  # HbO, HbR\n",
    "                    color = h_colors[jj]\n",
    "                    a = 1e6 * np.array(\n",
    "                        [evokeds[condition][subject].data[ch_idx + jj]\n",
    "                         for subject in use_subjects\n",
    "                         if ch_idx + jj not in bad.get(subject, [])], float)\n",
    "                    m = np.mean(a, axis=0)\n",
    "                    lower, upper = stats.t.interval(\n",
    "                        0.95, len(a) - 1, loc=m, scale=stats.sem(a, axis=0))\n",
    "                    ax.fill_between(\n",
    "                        times, lower, upper, facecolor=color,\n",
    "                        edgecolor='none', lw=0, alpha=0.25, zorder=3,\n",
    "                        clip_on=False)\n",
    "                    ax.plot(times, m, color=color, lw=1, zorder=4,\n",
    "                            clip_on=False)\n",
    "                # Correlations\n",
    "                this_df = ch_summary_use.query(\n",
    "                    f'ch_name == {repr(use.ch_names[ch_idx])} and '\n",
    "                    f'Chroma == \"hbo\" and '\n",
    "                    f'Condition == {repr(condition)}')\n",
    "                #assert 8 <= len(this_df) <= len(subjects), len(this_df)\n",
    "                a = np.array(this_df['theta'])\n",
    "                cs = list()\n",
    "                if len(cs):\n",
    "                    cs = [''] + cs\n",
    "                c = '\\n'.join(cs)\n",
    "                ax.text(times[-1], ylim[1],\n",
    "                        f'ch{ch_idx // 2 + 1}\\np={ch_p:0.5f}{c}',\n",
    "                        ha='right', va='top', fontsize='x-small')\n",
    "            ax.axvline(20, ls=':', color='0.5', zorder=2, lw=1)\n",
    "            ax.axhline(0, ls='-', color='k', zorder=2, lw=0.5)\n",
    "            ax.set(xticks=xticks, yticks=yticks)\n",
    "            ax.set(xlim=xlim, ylim=ylim)\n",
    "            for key in ('top', 'right'):\n",
    "                ax.spines[key].set_visible(False)\n",
    "            if ax.get_subplotspec().is_last_row():\n",
    "                ax.set(xlabel='Time (sec)')\n",
    "            else:\n",
    "                ax.set_xticklabels([''] * len(xticks))\n",
    "            if ax.get_subplotspec().is_first_col():\n",
    "                ax.set_ylabel(condition)\n",
    "            else:\n",
    "                ax.set_yticklabels([''] * len(yticks))\n",
    "            for key in ('top', 'right'):\n",
    "                ax.spines[key].set_visible(False)\n",
    "        for ii in range(ii + 1, n_col - 1):\n",
    "            fig.delaxes(axes[ci, ii])\n",
    "        # montage\n",
    "        ax = axes[ci, -1]\n",
    "        if sigs[0][0] is None:\n",
    "            fig.delaxes(ax)\n",
    "        else:\n",
    "            # plot montage\n",
    "            _plot_sig_chs(sigs, ax)\n",
    "    return fig\n",
    "\n",
    "times = evokeds[conditions[0]][subjects[0]].times\n",
    "info = evokeds[conditions[0]][subjects[0]].info\n",
    "fig = _plot_sigs(sig_chs)\n",
    "for ext in ('png', 'svg'):\n",
    "    fig.savefig(op.join(results_path, f'stats_{exp_name}.{ext}'))\n",
    "\n",
    "print(\"All done!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'216'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             ax\u001b[38;5;241m.\u001b[39mspines[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mset_visible(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fig\n\u001b[0;32m---> 35\u001b[0m times \u001b[38;5;241m=\u001b[39m \u001b[43mevokeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msubjects\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtimes\n\u001b[1;32m     36\u001b[0m info \u001b[38;5;241m=\u001b[39m evokeds[conditions[\u001b[38;5;241m0\u001b[39m]][subjects[\u001b[38;5;241m8\u001b[39m]]\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m     37\u001b[0m fig \u001b[38;5;241m=\u001b[39m _plot_sigs(sig_chs)\n",
      "\u001b[0;31mKeyError\u001b[0m: '216'"
     ]
    }
   ],
   "source": [
    "# ONLY PLOT SENSORS\n",
    "\n",
    "def _plot_sigs(sig_chs, all_corrs=()):\n",
    "    n_col = 1  # Only need one column for sensor locations\n",
    "    n_row = len(conditions)\n",
    "    figsize = (n_col * 2, n_row * 2)  # Increase figure size for better label visibility\n",
    "    fig, axes = plt.subplots(\n",
    "        n_row, n_col, figsize=figsize, constrained_layout=True)\n",
    "\n",
    "    # Handle the case of a single subplot\n",
    "    if n_row == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ci, condition in enumerate(conditions):\n",
    "        sigs = sig_chs[condition]\n",
    "        ax = axes[ci]  # Direct reference to each subplot's axis\n",
    "        \n",
    "        # Ensure labels are set even if there are no significant sensors\n",
    "        ax.set_ylabel(condition, labelpad=100)  # Increase labelpad if necessary\n",
    "\n",
    "        # Only attempt to plot sensor locations if there are significant sensors\n",
    "        if sigs[0][0] is not None:\n",
    "            _plot_sig_chs(sigs, ax)\n",
    "        else:\n",
    "            # Optionally, clear the axes but keep them to display the label\n",
    "            ax.clear()  # Clear the axes of any plotted data\n",
    "            ax.set_xticks([])  # Remove x-ticks\n",
    "            ax.set_yticks([])  # Remove y-ticks\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "    return fig\n",
    "\n",
    "times = evokeds[conditions[0]][subjects[8]].times\n",
    "info = evokeds[conditions[0]][subjects[8]].info\n",
    "fig = _plot_sigs(sig_chs)\n",
    "\n",
    "for ext in ('png', 'svg'):\n",
    "    fig.savefig(op.join(results_path, f'sensors_{exp_name}.{ext}'))\n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Source space projection\n",
    "import pyvista\n",
    "\n",
    "info = use.copy().pick_types(fnirs='hbo', exclude=()).info\n",
    "info['bads'] = []\n",
    "assert tuple(zs) == conditions\n",
    "\n",
    "evoked = mne.EvokedArray(np.array(list(zs.values())).T, info)\n",
    "picks = np.arange(len(evoked.ch_names))\n",
    "\n",
    "for ch in evoked.info['chs']:\n",
    "    assert ch['coord_frame'] == mne.io.constants.FIFF.FIFFV_COORD_HEAD\n",
    "stc = mne.stc_near_sensors(\n",
    "    evoked, trans='fsaverage', subject='fsaverage', mode='weighted',\n",
    "    distance=0.02, project=True, picks=picks, subjects_dir=subjects_dir)\n",
    "# Split channel indices by left lat, posterior, right lat:\n",
    "num_map = {name: str(ii) for ii, name in enumerate(evoked.ch_names)}\n",
    "evoked.copy().rename_channels(num_map) #.plot_sensors(show_names=True)\n",
    "view_map = [np.arange(19), np.arange(19, 33), np.arange(33, 52)]\n",
    "surf = mne.read_bem_surfaces(  # brain surface\n",
    "    f'{subjects_dir}/fsaverage/bem/fsaverage-5120-5120-5120-bem.fif', s_id=1)\n",
    "\n",
    "for ci, condition in enumerate(conditions):\n",
    "    this_sig = [v[0] // 2 for v in sig_chs[condition]]\n",
    "    #assert np.in1d(this_sig, np.arange(52)).all()\n",
    "    pos = np.array([info['chs'][idx]['loc'][:3] for idx in this_sig])\n",
    "    pos.shape = (-1, 3)  # can be empty\n",
    "    trans = mne.transforms._get_trans('fsaverage', 'head', 'mri')[0]\n",
    "    pos = mne.transforms.apply_trans(trans, pos)  # now in MRI coords\n",
    "    pos = mne.surface._project_onto_surface(pos, surf, project_rrs=True)[2]\n",
    "    # plot\n",
    "    brain = stc.plot(hemi='both', views=['lat', 'frontal', 'lat'],\n",
    "                    initial_time=evoked.times[ci], cortex='low_contrast',\n",
    "                    time_viewer=False, show_traces=False,\n",
    "                    surface='pial', smoothing_steps=0, size=(1200, 400),\n",
    "                    clim=dict(kind='value', pos_lims=[0., 1.25, 2.5]),\n",
    "                    colormap='RdBu_r', view_layout='horizontal',\n",
    "                    colorbar=(0, 1), time_label='', background='w',\n",
    "                    brain_kwargs=dict(units='m'),\n",
    "                    add_data_kwargs=dict(colorbar_kwargs=dict(\n",
    "                        title_font_size=24, label_font_size=24, n_labels=5,\n",
    "                        title='z score')), subjects_dir=subjects_dir)\n",
    "    brain.show_view('lat', hemi='lh', row=0, col=0)\n",
    "    brain.show_view(azimuth=270, elevation=90, row=0, col=1)\n",
    "    pl = brain.plotter\n",
    "    used = np.zeros(len(this_sig))\n",
    "    brain.show_view('lat', hemi='rh', row=0, col=2)\n",
    "    plt.imsave(\n",
    "        op.join(results_path, f'all_brain_{exp_name}_{condition}.png'), pl.image)\n",
    "\n",
    "print(\"All done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vtk==9.1.0 --> redid with conda --> 9.0.3 --> 9.0.3 ??\n",
    "# pyvistaqt==0.2.0 --> upgrade --> 0.11.1\n",
    "# pyvista==0.32.0 --> redid with pip --> 0.44.1\n",
    "# numpy 1.26.4 --> 1.19.5 --> 1.23.0\n",
    "# pip install vtk==9.1.0 --no-cache-dir\n",
    "# \"pip show X\" to see version // conda list vtk\n",
    "# conda install -c conda-forge vtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fOLD specificity\n",
    "import xlrd\n",
    "\n",
    "fold_files = ['10-10.xls', '10-5.xls']\n",
    "for fname in fold_files:\n",
    "    if not op.isfile(fname):\n",
    "        pooch.retrieve(f'https://github.com/nirx/fOLD-public/raw/master/Supplementary/{fname}', None, fname, path=os.getcwd())  # noqa\n",
    "raw_spec = use.copy()\n",
    "raw_spec.pick_channels(raw_spec.ch_names[::2])\n",
    "specs = mne_nirs.io.fold_channel_specificity(raw_spec, fold_files, 'Brodmann')\n",
    "for si, spec in enumerate(specs, 1):\n",
    "    spec['Channel'] = si\n",
    "    spec['negspec'] = -spec['Specificity']\n",
    "specs = pd.concat(specs, ignore_index=True)\n",
    "specs.drop(['Source', 'Detector', 'Distance (mm)', 'brainSens',\n",
    "            'X (mm)', 'Y (mm)', 'Z (mm)'], axis=1, inplace=True)\n",
    "specs.sort_values(['Channel', 'negspec'], inplace=True)\n",
    "specs.drop('negspec', axis=1, inplace=True)\n",
    "specs.reset_index(inplace=True, drop=True)\n",
    "specs.to_csv(op.join(results_path, 'specificity.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Condition  Chroma        ch_name  subject  day    group         theta\n",
      "0             A     hbo    S10_D18 hbo      201    1  trained -2.686171e-07\n",
      "1             A     hbo    S10_D18 hbo      201    3  trained -4.071789e-07\n",
      "2            AV     hbo    S10_D18 hbo      201    1  trained  7.349320e-09\n",
      "3            AV     hbo    S10_D18 hbo      201    3  trained  2.241841e-08\n",
      "4             V     hbo    S10_D18 hbo      201    1  trained  1.001392e-06\n",
      "...         ...     ...            ...      ...  ...      ...           ...\n",
      "52075         V  hbdiff  S5_D29 hbdiff      234    3  control -3.522506e-07\n",
      "52076         W  hbdiff  S5_D29 hbdiff      234    1  control -3.437235e-07\n",
      "52077         W  hbdiff  S5_D29 hbdiff      234    3  control -3.145766e-07\n",
      "52078  constant  hbdiff  S5_D29 hbdiff      234    1  control  1.688264e-08\n",
      "52079  constant  hbdiff  S5_D29 hbdiff      234    3  control  5.331788e-08\n",
      "\n",
      "[52080 rows x 7 columns]\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# CALCULATE HbDIFF\n",
    "\n",
    "# Load the data\n",
    "df_cha = pd.read_csv(op.join(results_path, 'df_cha_theta_detrend.csv'))\n",
    "df_cha_nolabels = df_cha.copy()\n",
    "df_cha_nolabels['ch_name'] = df_cha_nolabels['ch_name'].str[:-4]\n",
    "\n",
    "# Separate HbO and HbR\n",
    "df_hbo = df_cha_nolabels[df_cha_nolabels['Chroma'].str.endswith('hbo')].set_index(['subject', 'Condition', 'group', 'day', 'ch_name']).sort_index()\n",
    "df_hbr = df_cha_nolabels[df_cha_nolabels['Chroma'].str.endswith('hbr')].set_index(['subject', 'Condition', 'group', 'day', 'ch_name']).sort_index()\n",
    "\n",
    "# Compute the difference\n",
    "df_cha_diff_list = []\n",
    "for ch_name in df_hbo.index.get_level_values('ch_name').unique():\n",
    "    # Get aligned indices\n",
    "    df_hbo_ch = df_hbo.loc[(slice(None), slice(None), slice(None), slice(None), ch_name), :].sort_index()\n",
    "    df_hbr_ch = df_hbr.loc[(slice(None), slice(None), slice(None), slice(None), ch_name), :].sort_index()\n",
    "    \n",
    "    # Ensure df_hbo_ch and df_hbr_ch have the same length\n",
    "    common_index = df_hbo_ch.index.intersection(df_hbr_ch.index)\n",
    "    df_hbo_ch = df_hbo_ch.loc[common_index]\n",
    "    df_hbr_ch = df_hbr_ch.loc[common_index]\n",
    "    \n",
    "    # Calculate the difference\n",
    "    df_diff = df_hbo_ch[['theta']].sub(df_hbr_ch[['theta']])\n",
    "    \n",
    "    # Align df_cha_ch with df_diff\n",
    "    df_cha_ch = df_hbo_ch.reset_index()\n",
    "    df_cha_ch['theta'] = df_diff.values\n",
    "    df_cha_ch['Chroma'] = 'hbdiff'\n",
    "    df_cha_ch['ch_name'] = df_cha_ch['ch_name'] + ' hbdiff'\n",
    "    \n",
    "    if not df_cha_ch.empty:\n",
    "        df_cha_diff_list.append(df_cha_ch)\n",
    "\n",
    "df_cha_diff_concat = pd.concat(df_cha_diff_list, ignore_index=True)\n",
    "\n",
    "# Concatenate original df_cha with df_cha_diff_concat\n",
    "df_final = pd.concat([df_cha, df_cha_diff_concat], ignore_index=True)\n",
    "df_final.to_csv(op.join(results_path, 'df_combined_final_cha_detrend.csv'), index=False)\n",
    "\n",
    "# Print the head of the resulting dataframe\n",
    "print(df_final)\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting for 460 comparisons using FDR\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "12   S10_D22 hbo         V  2.446799  0.036950 -0.573293  0.993305  True\n",
      "46   S12_D22 hbo        AV  2.932725  0.016683 -0.321339  0.993305  True\n",
      "81   S14_D22 hbo        AV  3.222281  0.010451 -0.411065  0.993305  True\n",
      "91   S14_D26 hbo        AV  2.901366  0.017556 -0.428864  0.993305  True\n",
      "106  S15_D25 hbo        AV  2.612209  0.028169 -0.458097  0.993305  True\n",
      "117  S16_D25 hbo         V  2.869439  0.018494 -0.341219  0.993305  True\n",
      "303  S25_D14 hbo         W  2.530275  0.032221 -0.743672  0.993305  True\n",
      "365    S4_D3 hbo         A -2.270985  0.049284  0.424488  0.993305  True\n",
      "415   S7_D19 hbo         A -2.266226  0.049669  0.354150  0.993305  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: A\n",
      "Group: trained\n",
      "Chroma: hbo\n",
      "Condition Data:\n",
      "        ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "365   S4_D3 hbo         A -2.270985  0.049284  0.424488  0.993305  True\n",
      "415  S7_D19 hbo         A -2.266226  0.049669  0.354150  0.993305  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "12   S10_D22 hbo         V  2.446799  0.036950 -0.573293  0.993305  True\n",
      "46   S12_D22 hbo        AV  2.932725  0.016683 -0.321339  0.993305  True\n",
      "81   S14_D22 hbo        AV  3.222281  0.010451 -0.411065  0.993305  True\n",
      "91   S14_D26 hbo        AV  2.901366  0.017556 -0.428864  0.993305  True\n",
      "106  S15_D25 hbo        AV  2.612209  0.028169 -0.458097  0.993305  True\n",
      "117  S16_D25 hbo         V  2.869439  0.018494 -0.341219  0.993305  True\n",
      "303  S25_D14 hbo         W  2.530275  0.032221 -0.743672  0.993305  True\n",
      "365    S4_D3 hbo         A -2.270985  0.049284  0.424488  0.993305  True\n",
      "415   S7_D19 hbo         A -2.266226  0.049669  0.354150  0.993305  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: V\n",
      "Group: trained\n",
      "Chroma: hbo\n",
      "Condition Data:\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "12   S10_D22 hbo         V  2.446799  0.036950 -0.573293  0.993305  True\n",
      "117  S16_D25 hbo         V  2.869439  0.018494 -0.341219  0.993305  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "12   S10_D22 hbo         V  2.446799  0.036950 -0.573293  0.993305  True\n",
      "46   S12_D22 hbo        AV  2.932725  0.016683 -0.321339  0.993305  True\n",
      "81   S14_D22 hbo        AV  3.222281  0.010451 -0.411065  0.993305  True\n",
      "91   S14_D26 hbo        AV  2.901366  0.017556 -0.428864  0.993305  True\n",
      "106  S15_D25 hbo        AV  2.612209  0.028169 -0.458097  0.993305  True\n",
      "117  S16_D25 hbo         V  2.869439  0.018494 -0.341219  0.993305  True\n",
      "303  S25_D14 hbo         W  2.530275  0.032221 -0.743672  0.993305  True\n",
      "365    S4_D3 hbo         A -2.270985  0.049284  0.424488  0.993305  True\n",
      "415   S7_D19 hbo         A -2.266226  0.049669  0.354150  0.993305  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: AV\n",
      "Group: trained\n",
      "Chroma: hbo\n",
      "Condition Data:\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "46   S12_D22 hbo        AV  2.932725  0.016683 -0.321339  0.993305  True\n",
      "81   S14_D22 hbo        AV  3.222281  0.010451 -0.411065  0.993305  True\n",
      "91   S14_D26 hbo        AV  2.901366  0.017556 -0.428864  0.993305  True\n",
      "106  S15_D25 hbo        AV  2.612209  0.028169 -0.458097  0.993305  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "3    S10_D18 hbr         W  2.683820  0.025049 -0.664215  0.878438  True\n",
      "28   S11_D21 hbr         W -2.280199  0.048547  0.069387  0.878438  True\n",
      "43   S12_D21 hbr         W -2.415471  0.038898  0.098527  0.878438  True\n",
      "61   S13_D23 hbr        AV -2.342595  0.043834  0.343919  0.878438  True\n",
      "92   S14_D26 hbr         V -2.571584  0.030109  0.491214  0.878438  True\n",
      "104  S15_D14 hbr  constant  2.426684  0.038190 -0.214247  0.878438  True\n",
      "119  S16_D25 hbr  constant  2.487621  0.034557 -0.131946  0.878438  True\n",
      "123  S16_D26 hbr         W -2.538005  0.031815  0.307793  0.878438  True\n",
      "166   S19_D4 hbr        AV -2.330730  0.044694  0.518059  0.878438  True\n",
      "218   S21_D6 hbr         W -3.342072  0.008632  0.221430  0.878438  True\n",
      "275   S24_D7 hbr         A  2.555874  0.030896 -0.196999  0.878438  True\n",
      "403   S6_D18 hbr         W  2.786313  0.021181 -0.870050  0.878438  True\n",
      "418   S7_D19 hbr         W -2.431580  0.037884  0.288859  0.878438  True\n",
      "432   S8_D18 hbr         V -2.569489  0.030213  0.576449  0.878438  True\n",
      "443   S9_D17 hbr         W -2.311685  0.046109  0.277803  0.878438  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: A\n",
      "Group: trained\n",
      "Chroma: hbr\n",
      "Condition Data:\n",
      "        ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "275  S24_D7 hbr         A  2.555874  0.030896 -0.196999  0.878438  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "3    S10_D18 hbr         W  2.683820  0.025049 -0.664215  0.878438  True\n",
      "28   S11_D21 hbr         W -2.280199  0.048547  0.069387  0.878438  True\n",
      "43   S12_D21 hbr         W -2.415471  0.038898  0.098527  0.878438  True\n",
      "61   S13_D23 hbr        AV -2.342595  0.043834  0.343919  0.878438  True\n",
      "92   S14_D26 hbr         V -2.571584  0.030109  0.491214  0.878438  True\n",
      "104  S15_D14 hbr  constant  2.426684  0.038190 -0.214247  0.878438  True\n",
      "119  S16_D25 hbr  constant  2.487621  0.034557 -0.131946  0.878438  True\n",
      "123  S16_D26 hbr         W -2.538005  0.031815  0.307793  0.878438  True\n",
      "166   S19_D4 hbr        AV -2.330730  0.044694  0.518059  0.878438  True\n",
      "218   S21_D6 hbr         W -3.342072  0.008632  0.221430  0.878438  True\n",
      "275   S24_D7 hbr         A  2.555874  0.030896 -0.196999  0.878438  True\n",
      "403   S6_D18 hbr         W  2.786313  0.021181 -0.870050  0.878438  True\n",
      "418   S7_D19 hbr         W -2.431580  0.037884  0.288859  0.878438  True\n",
      "432   S8_D18 hbr         V -2.569489  0.030213  0.576449  0.878438  True\n",
      "443   S9_D17 hbr         W -2.311685  0.046109  0.277803  0.878438  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: V\n",
      "Group: trained\n",
      "Chroma: hbr\n",
      "Condition Data:\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "92   S14_D26 hbr         V -2.571584  0.030109  0.491214  0.878438  True\n",
      "432   S8_D18 hbr         V -2.569489  0.030213  0.576449  0.878438  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "3    S10_D18 hbr         W  2.683820  0.025049 -0.664215  0.878438  True\n",
      "28   S11_D21 hbr         W -2.280199  0.048547  0.069387  0.878438  True\n",
      "43   S12_D21 hbr         W -2.415471  0.038898  0.098527  0.878438  True\n",
      "61   S13_D23 hbr        AV -2.342595  0.043834  0.343919  0.878438  True\n",
      "92   S14_D26 hbr         V -2.571584  0.030109  0.491214  0.878438  True\n",
      "104  S15_D14 hbr  constant  2.426684  0.038190 -0.214247  0.878438  True\n",
      "119  S16_D25 hbr  constant  2.487621  0.034557 -0.131946  0.878438  True\n",
      "123  S16_D26 hbr         W -2.538005  0.031815  0.307793  0.878438  True\n",
      "166   S19_D4 hbr        AV -2.330730  0.044694  0.518059  0.878438  True\n",
      "218   S21_D6 hbr         W -3.342072  0.008632  0.221430  0.878438  True\n",
      "275   S24_D7 hbr         A  2.555874  0.030896 -0.196999  0.878438  True\n",
      "403   S6_D18 hbr         W  2.786313  0.021181 -0.870050  0.878438  True\n",
      "418   S7_D19 hbr         W -2.431580  0.037884  0.288859  0.878438  True\n",
      "432   S8_D18 hbr         V -2.569489  0.030213  0.576449  0.878438  True\n",
      "443   S9_D17 hbr         W -2.311685  0.046109  0.277803  0.878438  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: AV\n",
      "Group: trained\n",
      "Chroma: hbr\n",
      "Condition Data:\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "61   S13_D23 hbr        AV -2.342595  0.043834  0.343919  0.878438  True\n",
      "166   S19_D4 hbr        AV -2.330730  0.044694  0.518059  0.878438  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "            ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "22   S11_D19 hbdiff         V  2.758012  0.022184 -0.671189  0.995478  True\n",
      "46   S12_D22 hbdiff        AV  2.921746  0.016984 -0.300506  0.995478  True\n",
      "91   S14_D26 hbdiff        AV  3.232971  0.010274 -0.483272  0.995478  True\n",
      "303  S25_D14 hbdiff         W  2.866251  0.018590 -0.546667  0.995478  True\n",
      "370    S4_D5 hbdiff         A -2.620879  0.027771  0.434624  0.995478  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: A\n",
      "Group: trained\n",
      "Chroma: hbdiff\n",
      "Condition Data:\n",
      "          ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "370  S4_D5 hbdiff         A -2.620879  0.027771  0.434624  0.995478  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "            ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "22   S11_D19 hbdiff         V  2.758012  0.022184 -0.671189  0.995478  True\n",
      "46   S12_D22 hbdiff        AV  2.921746  0.016984 -0.300506  0.995478  True\n",
      "91   S14_D26 hbdiff        AV  3.232971  0.010274 -0.483272  0.995478  True\n",
      "303  S25_D14 hbdiff         W  2.866251  0.018590 -0.546667  0.995478  True\n",
      "370    S4_D5 hbdiff         A -2.620879  0.027771  0.434624  0.995478  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: V\n",
      "Group: trained\n",
      "Chroma: hbdiff\n",
      "Condition Data:\n",
      "           ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "22  S11_D19 hbdiff         V  2.758012  0.022184 -0.671189  0.995478  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "            ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "22   S11_D19 hbdiff         V  2.758012  0.022184 -0.671189  0.995478  True\n",
      "46   S12_D22 hbdiff        AV  2.921746  0.016984 -0.300506  0.995478  True\n",
      "91   S14_D26 hbdiff        AV  3.232971  0.010274 -0.483272  0.995478  True\n",
      "303  S25_D14 hbdiff         W  2.866251  0.018590 -0.546667  0.995478  True\n",
      "370    S4_D5 hbdiff         A -2.620879  0.027771  0.434624  0.995478  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: AV\n",
      "Group: trained\n",
      "Chroma: hbdiff\n",
      "Condition Data:\n",
      "           ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "46  S12_D22 hbdiff        AV  2.921746  0.016984 -0.300506  0.995478  True\n",
      "91  S14_D26 hbdiff        AV  3.232971  0.010274 -0.483272  0.995478  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "42   S12_D21 hbo         V  2.550055  0.031192 -0.147463  0.957422  True\n",
      "47   S12_D22 hbo         V  2.391241  0.040475 -0.226243  0.957422  True\n",
      "48   S12_D22 hbo         W  2.591596  0.029137 -0.379761  0.957422  True\n",
      "82   S14_D22 hbo         V  2.772695  0.021658 -0.163002  0.957422  True\n",
      "91   S14_D26 hbo        AV  2.492744  0.034267 -0.365558  0.957422  True\n",
      "130  S16_D32 hbo         A -9.668562  0.010529  0.739619  0.957422  True\n",
      "272  S24_D34 hbo         V  8.813754  0.012630 -0.116922  0.957422  True\n",
      "297  S25_D13 hbo         V  2.301354  0.046895 -0.345052  0.957422  True\n",
      "327    S2_D2 hbo         V  2.653262  0.026335 -0.112383  0.957422  True\n",
      "377   S5_D15 hbo         V  2.273483  0.049083 -0.196419  0.957422  True\n",
      "387   S5_D17 hbo         V  2.961655  0.015917 -0.257778  0.957422  True\n",
      "388   S5_D17 hbo         W  2.408289  0.039359 -0.207828  0.957422  True\n",
      "423   S8_D16 hbo         W  2.273244  0.049102 -0.528893  0.957422  True\n",
      "427   S8_D17 hbo         V  2.335850  0.044321 -0.220801  0.957422  True\n",
      "429   S8_D17 hbo  constant -3.327967  0.008828  0.123580  0.957422  True\n",
      "437   S8_D20 hbo         V  2.580153  0.029689 -0.262517  0.957422  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: A\n",
      "Group: control\n",
      "Chroma: hbo\n",
      "Condition Data:\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "130  S16_D32 hbo         A -9.668562  0.010529  0.739619  0.957422  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "42   S12_D21 hbo         V  2.550055  0.031192 -0.147463  0.957422  True\n",
      "47   S12_D22 hbo         V  2.391241  0.040475 -0.226243  0.957422  True\n",
      "48   S12_D22 hbo         W  2.591596  0.029137 -0.379761  0.957422  True\n",
      "82   S14_D22 hbo         V  2.772695  0.021658 -0.163002  0.957422  True\n",
      "91   S14_D26 hbo        AV  2.492744  0.034267 -0.365558  0.957422  True\n",
      "130  S16_D32 hbo         A -9.668562  0.010529  0.739619  0.957422  True\n",
      "272  S24_D34 hbo         V  8.813754  0.012630 -0.116922  0.957422  True\n",
      "297  S25_D13 hbo         V  2.301354  0.046895 -0.345052  0.957422  True\n",
      "327    S2_D2 hbo         V  2.653262  0.026335 -0.112383  0.957422  True\n",
      "377   S5_D15 hbo         V  2.273483  0.049083 -0.196419  0.957422  True\n",
      "387   S5_D17 hbo         V  2.961655  0.015917 -0.257778  0.957422  True\n",
      "388   S5_D17 hbo         W  2.408289  0.039359 -0.207828  0.957422  True\n",
      "423   S8_D16 hbo         W  2.273244  0.049102 -0.528893  0.957422  True\n",
      "427   S8_D17 hbo         V  2.335850  0.044321 -0.220801  0.957422  True\n",
      "429   S8_D17 hbo  constant -3.327967  0.008828  0.123580  0.957422  True\n",
      "437   S8_D20 hbo         V  2.580153  0.029689 -0.262517  0.957422  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: V\n",
      "Group: control\n",
      "Chroma: hbo\n",
      "Condition Data:\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "42   S12_D21 hbo         V  2.550055  0.031192 -0.147463  0.957422  True\n",
      "47   S12_D22 hbo         V  2.391241  0.040475 -0.226243  0.957422  True\n",
      "82   S14_D22 hbo         V  2.772695  0.021658 -0.163002  0.957422  True\n",
      "272  S24_D34 hbo         V  8.813754  0.012630 -0.116922  0.957422  True\n",
      "297  S25_D13 hbo         V  2.301354  0.046895 -0.345052  0.957422  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "         ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "42   S12_D21 hbo         V  2.550055  0.031192 -0.147463  0.957422  True\n",
      "47   S12_D22 hbo         V  2.391241  0.040475 -0.226243  0.957422  True\n",
      "48   S12_D22 hbo         W  2.591596  0.029137 -0.379761  0.957422  True\n",
      "82   S14_D22 hbo         V  2.772695  0.021658 -0.163002  0.957422  True\n",
      "91   S14_D26 hbo        AV  2.492744  0.034267 -0.365558  0.957422  True\n",
      "130  S16_D32 hbo         A -9.668562  0.010529  0.739619  0.957422  True\n",
      "272  S24_D34 hbo         V  8.813754  0.012630 -0.116922  0.957422  True\n",
      "297  S25_D13 hbo         V  2.301354  0.046895 -0.345052  0.957422  True\n",
      "327    S2_D2 hbo         V  2.653262  0.026335 -0.112383  0.957422  True\n",
      "377   S5_D15 hbo         V  2.273483  0.049083 -0.196419  0.957422  True\n",
      "387   S5_D17 hbo         V  2.961655  0.015917 -0.257778  0.957422  True\n",
      "388   S5_D17 hbo         W  2.408289  0.039359 -0.207828  0.957422  True\n",
      "423   S8_D16 hbo         W  2.273244  0.049102 -0.528893  0.957422  True\n",
      "427   S8_D17 hbo         V  2.335850  0.044321 -0.220801  0.957422  True\n",
      "429   S8_D17 hbo  constant -3.327967  0.008828  0.123580  0.957422  True\n",
      "437   S8_D20 hbo         V  2.580153  0.029689 -0.262517  0.957422  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: AV\n",
      "Group: control\n",
      "Chroma: hbo\n",
      "Condition Data:\n",
      "        ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "91  S14_D26 hbo        AV  2.492744  0.034267 -0.365558  0.957422  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "         ch_name Condition    t_stat   p_value         z    P_fdr   SIG\n",
      "55   S13_D21 hbr         A -2.451810  0.036648  0.262079  0.93116  True\n",
      "59   S13_D21 hbr  constant  2.420208  0.038597 -0.054285  0.93116  True\n",
      "130  S16_D32 hbr         A -4.393031  0.048108  0.928895  0.93116  True\n",
      "177   S19_D8 hbr         V  2.448371  0.036855 -0.326245  0.93116  True\n",
      "227   S21_D8 hbr         V  3.245897  0.010063 -0.225969  0.93116  True\n",
      "233   S22_D5 hbr         W  3.042019  0.013972 -0.226372  0.93116  True\n",
      "279   S24_D7 hbr  constant  2.366490  0.042151 -0.018257  0.93116  True\n",
      "312  S26_D12 hbr         V -2.329665  0.044772  0.321252  0.93116  True\n",
      "321    S2_D1 hbr        AV -2.643941  0.026740  0.100626  0.93116  True\n",
      "342    S3_D2 hbr         V  2.422828  0.038432 -0.757076  0.93116  True\n",
      "352    S3_D4 hbr         V  2.591991  0.029118 -0.681722  0.93116  True\n",
      "370    S4_D5 hbr         A  2.271316  0.049257 -0.193458  0.93116  True\n",
      "411   S7_D17 hbr        AV -2.933155  0.016671  0.187009  0.93116  True\n",
      "414   S7_D17 hbr  constant  3.029408  0.014260 -0.017983  0.93116  True\n",
      "418   S7_D19 hbr         W  2.515821  0.032994 -0.155917  0.93116  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: A\n",
      "Group: control\n",
      "Chroma: hbr\n",
      "Condition Data:\n",
      "         ch_name Condition    t_stat   p_value         z    P_fdr   SIG\n",
      "55   S13_D21 hbr         A -2.451810  0.036648  0.262079  0.93116  True\n",
      "130  S16_D32 hbr         A -4.393031  0.048108  0.928895  0.93116  True\n",
      "370    S4_D5 hbr         A  2.271316  0.049257 -0.193458  0.93116  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "         ch_name Condition    t_stat   p_value         z    P_fdr   SIG\n",
      "55   S13_D21 hbr         A -2.451810  0.036648  0.262079  0.93116  True\n",
      "59   S13_D21 hbr  constant  2.420208  0.038597 -0.054285  0.93116  True\n",
      "130  S16_D32 hbr         A -4.393031  0.048108  0.928895  0.93116  True\n",
      "177   S19_D8 hbr         V  2.448371  0.036855 -0.326245  0.93116  True\n",
      "227   S21_D8 hbr         V  3.245897  0.010063 -0.225969  0.93116  True\n",
      "233   S22_D5 hbr         W  3.042019  0.013972 -0.226372  0.93116  True\n",
      "279   S24_D7 hbr  constant  2.366490  0.042151 -0.018257  0.93116  True\n",
      "312  S26_D12 hbr         V -2.329665  0.044772  0.321252  0.93116  True\n",
      "321    S2_D1 hbr        AV -2.643941  0.026740  0.100626  0.93116  True\n",
      "342    S3_D2 hbr         V  2.422828  0.038432 -0.757076  0.93116  True\n",
      "352    S3_D4 hbr         V  2.591991  0.029118 -0.681722  0.93116  True\n",
      "370    S4_D5 hbr         A  2.271316  0.049257 -0.193458  0.93116  True\n",
      "411   S7_D17 hbr        AV -2.933155  0.016671  0.187009  0.93116  True\n",
      "414   S7_D17 hbr  constant  3.029408  0.014260 -0.017983  0.93116  True\n",
      "418   S7_D19 hbr         W  2.515821  0.032994 -0.155917  0.93116  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: V\n",
      "Group: control\n",
      "Chroma: hbr\n",
      "Condition Data:\n",
      "         ch_name Condition    t_stat   p_value         z    P_fdr   SIG\n",
      "177   S19_D8 hbr         V  2.448371  0.036855 -0.326245  0.93116  True\n",
      "227   S21_D8 hbr         V  3.245897  0.010063 -0.225969  0.93116  True\n",
      "312  S26_D12 hbr         V -2.329665  0.044772  0.321252  0.93116  True\n",
      "342    S3_D2 hbr         V  2.422828  0.038432 -0.757076  0.93116  True\n",
      "352    S3_D4 hbr         V  2.591991  0.029118 -0.681722  0.93116  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "         ch_name Condition    t_stat   p_value         z    P_fdr   SIG\n",
      "55   S13_D21 hbr         A -2.451810  0.036648  0.262079  0.93116  True\n",
      "59   S13_D21 hbr  constant  2.420208  0.038597 -0.054285  0.93116  True\n",
      "130  S16_D32 hbr         A -4.393031  0.048108  0.928895  0.93116  True\n",
      "177   S19_D8 hbr         V  2.448371  0.036855 -0.326245  0.93116  True\n",
      "227   S21_D8 hbr         V  3.245897  0.010063 -0.225969  0.93116  True\n",
      "233   S22_D5 hbr         W  3.042019  0.013972 -0.226372  0.93116  True\n",
      "279   S24_D7 hbr  constant  2.366490  0.042151 -0.018257  0.93116  True\n",
      "312  S26_D12 hbr         V -2.329665  0.044772  0.321252  0.93116  True\n",
      "321    S2_D1 hbr        AV -2.643941  0.026740  0.100626  0.93116  True\n",
      "342    S3_D2 hbr         V  2.422828  0.038432 -0.757076  0.93116  True\n",
      "352    S3_D4 hbr         V  2.591991  0.029118 -0.681722  0.93116  True\n",
      "370    S4_D5 hbr         A  2.271316  0.049257 -0.193458  0.93116  True\n",
      "411   S7_D17 hbr        AV -2.933155  0.016671  0.187009  0.93116  True\n",
      "414   S7_D17 hbr  constant  3.029408  0.014260 -0.017983  0.93116  True\n",
      "418   S7_D19 hbr         W  2.515821  0.032994 -0.155917  0.93116  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: AV\n",
      "Group: control\n",
      "Chroma: hbr\n",
      "Condition Data:\n",
      "        ch_name Condition    t_stat   p_value         z    P_fdr   SIG\n",
      "321   S2_D1 hbr        AV -2.643941  0.026740  0.100626  0.93116  True\n",
      "411  S7_D17 hbr        AV -2.933155  0.016671  0.187009  0.93116  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "            ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "32   S11_D23 hbdiff         V  2.487581  0.034559 -0.161151  0.932953  True\n",
      "42   S12_D21 hbdiff         V  3.206975  0.010711 -0.233615  0.932953  True\n",
      "48   S12_D22 hbdiff         W  3.200548  0.010822 -0.280262  0.932953  True\n",
      "53   S12_D24 hbdiff         W  2.796196  0.020842 -0.331553  0.932953  True\n",
      "57   S13_D21 hbdiff         V  2.530698  0.032198 -0.429329  0.932953  True\n",
      "77   S13_D31 hbdiff         V  4.918260  0.038942 -0.084450  0.932953  True\n",
      "103  S15_D14 hbdiff         W  2.476241  0.035208 -0.403379  0.932953  True\n",
      "207   S20_D7 hbdiff         V  2.582659  0.029567 -0.191882  0.932953  True\n",
      "251  S23_D12 hbdiff        AV  2.324034  0.045186 -0.653249  0.932953  True\n",
      "327    S2_D2 hbdiff         V  2.902205  0.017532 -0.111952  0.932953  True\n",
      "382   S5_D16 hbdiff         V  2.566488  0.030362 -0.226528  0.932953  True\n",
      "387   S5_D17 hbdiff         V  2.364411  0.042295 -0.269456  0.932953  True\n",
      "412   S7_D17 hbdiff         V  2.787991  0.021123 -0.194081  0.932953  True\n",
      "429   S8_D17 hbdiff  constant -2.266182  0.049672  0.131175  0.932953  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: A\n",
      "Group: control\n",
      "Chroma: hbdiff\n",
      "Condition Data:\n",
      "Empty DataFrame\n",
      "Columns: [ch_name, Condition, t_stat, p_value, z, P_fdr, SIG]\n",
      "Index: []\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "            ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "32   S11_D23 hbdiff         V  2.487581  0.034559 -0.161151  0.932953  True\n",
      "42   S12_D21 hbdiff         V  3.206975  0.010711 -0.233615  0.932953  True\n",
      "48   S12_D22 hbdiff         W  3.200548  0.010822 -0.280262  0.932953  True\n",
      "53   S12_D24 hbdiff         W  2.796196  0.020842 -0.331553  0.932953  True\n",
      "57   S13_D21 hbdiff         V  2.530698  0.032198 -0.429329  0.932953  True\n",
      "77   S13_D31 hbdiff         V  4.918260  0.038942 -0.084450  0.932953  True\n",
      "103  S15_D14 hbdiff         W  2.476241  0.035208 -0.403379  0.932953  True\n",
      "207   S20_D7 hbdiff         V  2.582659  0.029567 -0.191882  0.932953  True\n",
      "251  S23_D12 hbdiff        AV  2.324034  0.045186 -0.653249  0.932953  True\n",
      "327    S2_D2 hbdiff         V  2.902205  0.017532 -0.111952  0.932953  True\n",
      "382   S5_D16 hbdiff         V  2.566488  0.030362 -0.226528  0.932953  True\n",
      "387   S5_D17 hbdiff         V  2.364411  0.042295 -0.269456  0.932953  True\n",
      "412   S7_D17 hbdiff         V  2.787991  0.021123 -0.194081  0.932953  True\n",
      "429   S8_D17 hbdiff  constant -2.266182  0.049672  0.131175  0.932953  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: V\n",
      "Group: control\n",
      "Chroma: hbdiff\n",
      "Condition Data:\n",
      "            ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "32   S11_D23 hbdiff         V  2.487581  0.034559 -0.161151  0.932953  True\n",
      "42   S12_D21 hbdiff         V  3.206975  0.010711 -0.233615  0.932953  True\n",
      "57   S13_D21 hbdiff         V  2.530698  0.032198 -0.429329  0.932953  True\n",
      "77   S13_D31 hbdiff         V  4.918260  0.038942 -0.084450  0.932953  True\n",
      "207   S20_D7 hbdiff         V  2.582659  0.029567 -0.191882  0.932953  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "Correcting for 460 comparisons using FDR\n",
      "            ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "32   S11_D23 hbdiff         V  2.487581  0.034559 -0.161151  0.932953  True\n",
      "42   S12_D21 hbdiff         V  3.206975  0.010711 -0.233615  0.932953  True\n",
      "48   S12_D22 hbdiff         W  3.200548  0.010822 -0.280262  0.932953  True\n",
      "53   S12_D24 hbdiff         W  2.796196  0.020842 -0.331553  0.932953  True\n",
      "57   S13_D21 hbdiff         V  2.530698  0.032198 -0.429329  0.932953  True\n",
      "77   S13_D31 hbdiff         V  4.918260  0.038942 -0.084450  0.932953  True\n",
      "103  S15_D14 hbdiff         W  2.476241  0.035208 -0.403379  0.932953  True\n",
      "207   S20_D7 hbdiff         V  2.582659  0.029567 -0.191882  0.932953  True\n",
      "251  S23_D12 hbdiff        AV  2.324034  0.045186 -0.653249  0.932953  True\n",
      "327    S2_D2 hbdiff         V  2.902205  0.017532 -0.111952  0.932953  True\n",
      "382   S5_D16 hbdiff         V  2.566488  0.030362 -0.226528  0.932953  True\n",
      "387   S5_D17 hbdiff         V  2.364411  0.042295 -0.269456  0.932953  True\n",
      "412   S7_D17 hbdiff         V  2.787991  0.021123 -0.194081  0.932953  True\n",
      "429   S8_D17 hbdiff  constant -2.266182  0.049672  0.131175  0.932953  True\n",
      "Opening raw data file ../../processed/205_1_001_hbo_raw.fif...\n",
      "    Range : 0 ... 4171 =      0.000 ...   867.568 secs\n",
      "Ready.\n",
      "\n",
      "Condition: AV\n",
      "Group: control\n",
      "Chroma: hbdiff\n",
      "Condition Data:\n",
      "            ch_name Condition    t_stat   p_value         z     P_fdr   SIG\n",
      "251  S23_D12 hbdiff        AV  2.324034  0.045186 -0.653249  0.932953  True\n",
      "Projecting data from 168 sensors onto 327684 surface vertices: weighted mode\n",
      "    Projecting sensors onto surface\n",
      "    Minimum projected intra-sensor distance: 0.0 mm\n",
      "    160208 / 327684 non-zero vertices\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# RUN T-TEST AND PLOT THE CHANGES OVER TIME\n",
    "\n",
    "# Source space projection - significant changes over time\n",
    "import pandas as pd\n",
    "import mne\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_rel, zscore\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from io import BytesIO\n",
    "\n",
    "raw_path = '../../data'\n",
    "proc_path = '../../processed'\n",
    "results_path = '../../results'\n",
    "subjects_dir = '../../subjects'\n",
    "\n",
    "groups = ['trained', 'control']\n",
    "chromas = ['hbo', 'hbr', 'hbdiff']\n",
    "conditions = ['A', 'V', 'AV']\n",
    "\n",
    "df_final = pd.read_csv(op.join(results_path, 'df_combined_final_cha_detrend.csv'))\n",
    "\n",
    "# Perform analysis for each group and Chroma\n",
    "for group in groups:\n",
    "    for chroma in chromas:\n",
    "        # Prepare figure for composite plots\n",
    "        fig, axes = plt.subplots(1, len(conditions), figsize=(15, 5))\n",
    "        for idx, condition in enumerate(conditions):\n",
    "            # Filter data for day 1 and day 3 for the specific group and Chroma\n",
    "            df_day1 = df_final.query(f\"group == '{group}' and Chroma == '{chroma}' and day == 1\").copy()\n",
    "            df_day3 = df_final.query(f\"group == '{group}' and Chroma == '{chroma}' and day == 3\").copy()\n",
    "\n",
    "            # Set index and sort\n",
    "            df_day1 = df_day1.set_index(['subject', 'ch_name', 'Condition', 'Chroma']).sort_index()\n",
    "            df_day3 = df_day3.set_index(['subject', 'ch_name', 'Condition', 'Chroma']).sort_index()\n",
    "            \n",
    "            # Merge dataframes to align day 1 and day 3 data\n",
    "            df_merged = df_day1[['theta']].rename(columns={'theta': 'theta_day1'}).merge(\n",
    "                df_day3[['theta']].rename(columns={'theta': 'theta_day3'}),\n",
    "                left_index=True, right_index=True)\n",
    "\n",
    "            # Calculate the difference and z-score\n",
    "            df_merged['theta_diff'] = df_merged['theta_day3'] - df_merged['theta_day1']\n",
    "            df_merged['z'] = zscore(df_merged['theta_diff'])\n",
    "\n",
    "            # Perform paired t-test for each channel and condition across subjects\n",
    "            t_stats = []\n",
    "            p_values = []\n",
    "            ch_names = []\n",
    "            condition_list = []\n",
    "\n",
    "            for (ch_name, cond), group_df in df_merged.groupby(['ch_name', 'Condition']):\n",
    "                t_stat, p_value = ttest_rel(group_df['theta_day1'], group_df['theta_day3'])\n",
    "                t_stats.append(t_stat)\n",
    "                p_values.append(p_value)\n",
    "                ch_names.append(ch_name)\n",
    "                condition_list.append(cond)\n",
    "\n",
    "            # Create a results DataFrame\n",
    "            results_df = pd.DataFrame({\n",
    "                'ch_name': ch_names,\n",
    "                'Condition': condition_list,\n",
    "                't_stat': t_stats,\n",
    "                'p_value': p_values\n",
    "            })\n",
    "\n",
    "            # Combine with z-score data\n",
    "            z_scores = df_merged.groupby(['ch_name', 'Condition'])['z'].mean().reset_index()\n",
    "            results_df = results_df.merge(z_scores, on=['ch_name', 'Condition'])\n",
    "\n",
    "            # Correct for multiple comparisons\n",
    "            print(f'Correcting for {len(results_df[\"p_value\"])} comparisons using FDR')\n",
    "            _, results_df['P_fdr'] = mne.stats.fdr_correction(results_df['p_value'], method='indep')\n",
    "            results_df['SIG'] = results_df['p_value'] < 0.05\n",
    "            \n",
    "            # Print significant results\n",
    "            significant_results = results_df.loc[results_df.SIG == True]\n",
    "            print(significant_results)\n",
    "\n",
    "            # Prepare data for brain plots\n",
    "            fname = op.join(proc_path, f'205_1_001_hbo_raw.fif')\n",
    "            use = mne.io.read_raw_fif(fname)\n",
    "            info = use.info\n",
    "\n",
    "            ch_of_interest = use.pick_channels([ch_name for ch_name in use.info['ch_names']])\n",
    "            info_of_interest = ch_of_interest.info\n",
    "\n",
    "            zs = {}\n",
    "            condition_data = results_df[(results_df['Condition'] == condition) & (results_df['SIG'] == True)]\n",
    "            \n",
    "            # Debugging prints\n",
    "            print(f\"\\nCondition: {condition}\")\n",
    "            print(f\"Group: {group}\")\n",
    "            print(f\"Chroma: {chroma}\")\n",
    "            print(f\"Condition Data:\\n{condition_data.head()}\")\n",
    "            \n",
    "            zs[condition] = np.array([\n",
    "                condition_data.loc[(condition_data['ch_name'] == ch_name) & (condition_data['SIG'] == True), 'z'].values[0]\n",
    "                if not condition_data.loc[(condition_data['ch_name'] == ch_name) & (condition_data['SIG'] == True), 'z'].empty\n",
    "                else 0\n",
    "                for ch_name in info_of_interest['ch_names']\n",
    "            ])\n",
    "            \n",
    "            # Create an EvokedArray for each condition\n",
    "            evoked = mne.EvokedArray(zs[condition][:, np.newaxis], info_of_interest)\n",
    "            picks = np.arange(len(info_of_interest['ch_names']))\n",
    "\n",
    "            stc = mne.stc_near_sensors(\n",
    "                evoked, trans='fsaverage', subject='fsaverage', mode='weighted',\n",
    "                distance=0.02, project=True, picks=picks, subjects_dir=subjects_dir)\n",
    "\n",
    "            # Plot the brain and capture the image in-memory\n",
    "            brain = stc.plot(hemi='both', views=['lat', 'frontal', 'lat'],\n",
    "                             cortex='low_contrast', time_viewer=False, show_traces=False,\n",
    "                             surface='pial', smoothing_steps=0, size=(1200, 400),\n",
    "                             clim=dict(kind='value', pos_lims=[0, 0.5, 1]),\n",
    "                             colormap='RdBu_r', view_layout='horizontal',\n",
    "                             colorbar=(0, 1), time_label='', background='w',\n",
    "                             brain_kwargs=dict(units='m'),\n",
    "                             add_data_kwargs=dict(colorbar_kwargs=dict(\n",
    "                                 title_font_size=24, label_font_size=20, n_labels=5,\n",
    "                                 title='z score')), subjects_dir=subjects_dir)\n",
    "            brain.show_view('lat', hemi='lh', row=0, col=0)\n",
    "            brain.show_view(azimuth=270, elevation=90, row=0, col=1)\n",
    "            brain.show_view('lat', hemi='rh', row=0, col=2)\n",
    "\n",
    "            # Capture the plot as an image in memory\n",
    "            screenshot = brain.screenshot(time_viewer=False)\n",
    "            brain.close()\n",
    "\n",
    "            # Display the image in the composite figure\n",
    "            ax = axes[idx]\n",
    "            ax.imshow(screenshot)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f'{group.capitalize()} - Condition {condition} ({chroma})', fontsize=18)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.savefig(op.join(results_path, f'{group}_{chroma}_composite_brain_plots_detrend.png'))\n",
    "        plt.show()\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
